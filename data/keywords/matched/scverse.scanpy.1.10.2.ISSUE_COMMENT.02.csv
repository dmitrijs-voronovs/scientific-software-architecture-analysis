id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/189:392,availability,cluster,clustering,392,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:418,availability,cluster,clustering,418,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:787,availability,cluster,clustering,787,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:862,availability,cluster,clusters,862,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:392,deployability,cluster,clustering,392,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:418,deployability,cluster,clustering,418,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:689,deployability,continu,continuous,689,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:787,deployability,cluster,clustering,787,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:862,deployability,cluster,clusters,862,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:378,energy efficiency,reduc,reduction,378,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1190,energy efficiency,current,currently,1190,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:134,integrability,batch,batch,134,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:735,integrability,graph-bas,graph-based,735,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1243,integrability,standardiz,standardization,1243,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:29,interoperability,convers,conversation,29,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:847,interoperability,distribut,distributed,847,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1136,interoperability,convers,conversation,1136,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1243,interoperability,standard,standardization,1243,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:134,performance,batch,batch,134,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:249,security,misus,misuse,249,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:116,usability,visual,visualization,116,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:275,usability,learn,learning,275,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:332,usability,visual,visualization,332,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:747,usability,visual,visualizations,747,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:960,usability,user,users,960,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1167,availability,cluster,clustering,1167," critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1514,availability,avail,available,1514,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1835,availability,avail,available,1835,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1167,deployability,cluster,clustering,1167," critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,deployability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1504,energy efficiency,current,currently,1504,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1593,energy efficiency,current,currently,1593,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1946,integrability,batch,batch,1946,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,integrability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1671,interoperability,specif,specific,1671,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,interoperability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,modifiability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:775,performance,perform,performs,775,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1789,performance,perform,performance,1789,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1946,performance,batch,batch,1946,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:695,reliability,doe,doesn,695,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1009,reliability,pra,practitioner,1009,"ry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1514,reliability,availab,available,1514,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1835,reliability,availab,available,1835,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,reliability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:589,safety,avoid,avoided,589,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:664,safety,valid,validated,664,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:684,safety,valid,validation,684,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1242,safety,valid,valid,1242,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1318,safety,valid,valid,1318,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1442,safety,test,test,1442,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1514,safety,avail,available,1514,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1616,safety,valid,validation,1616,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1835,safety,avail,available,1835,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:664,security,validat,validated,664,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:684,security,validat,validation,684,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1514,security,availab,available,1514,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1616,security,validat,validation,1616,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1835,security,availab,available,1835,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,security,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1432,testability,simul,simulated,1432,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1442,testability,test,test,1442,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1978,testability,integr,integration,1978,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:103,usability,tool,tools,103,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:151,usability,tool,tool,151,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:313,usability,workflow,workflow,313,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:775,usability,perform,performs,775,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1260,usability,tool,tools,1260,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:1789,usability,perform,performance,1789,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:2160,usability,document,documentation,2160,"ritically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided. II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable. III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/pull/191:232,deployability,pipelin,pipeline,232,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:232,integrability,pipelin,pipeline,232,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:134,performance,memor,memory,134,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:122,reliability,doe,doesn,122,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:134,usability,memor,memory,134,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:97,availability,consist,consistent,97,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1019,availability,slo,slower,1019,"r the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2325,availability,redund,redundant,2325,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2418,availability,consist,consistently,2418,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2738,availability,restor,restore,2738,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1246,deployability,pipelin,pipeline,1246," functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tell",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1431,deployability,automat,automatically,1431,"for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1445,deployability,scale,scale,1445," memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1471,deployability,observ,observations,1471,"have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would ca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1572,deployability,pipelin,pipeline,1572,"g omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2295,deployability,pipelin,pipelines,2295,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2325,deployability,redundan,redundant,2325,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:959,energy efficiency,reduc,reduction,959,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1445,energy efficiency,scale,scale,1445," memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1612,energy efficiency,load,load,1612,"things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1246,integrability,pipelin,pipeline,1246," functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tell",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1572,integrability,pipelin,pipeline,1572,"g omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2295,integrability,pipelin,pipelines,2295,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1445,modifiability,scal,scale,1445," memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1957,modifiability,variab,variable,1957,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:449,performance,memor,memory,449,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:787,performance,memor,memory,787,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:856,performance,bottleneck,bottleneck,856,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:948,performance,memor,memory,948,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:973,performance,memor,memory,973,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1419,performance,memor,memory,1419,"course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is cons",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1445,performance,scale,scale,1445," memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1612,performance,load,load,1612,"things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1644,performance,memor,memory,1644,"and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:770,reliability,doe,doesn,770,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1019,reliability,slo,slower,1019,"r the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2305,reliability,doe,does,2305,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2325,reliability,redundan,redundant,2325,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2738,reliability,restor,restore,2738,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2325,safety,redund,redundant,2325,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1731,security,modif,modifying,1731," mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either yo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1431,testability,automat,automatically,1431,"for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1471,testability,observ,observations,1471,"have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would ca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2520,testability,simpl,simple,2520,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:97,usability,consist,consistent,97,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:127,usability,user,user-images,127,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:449,usability,memor,memory,449,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:456,usability,efficien,efficiency,456,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:523,usability,tool,toolkit,523,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:787,usability,memor,memory,787,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:948,usability,memor,memory,948,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:973,usability,memor,memory,973,"Hi James! Thank you for the remark! And you're right... several repetitions of the following are consistent:. ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1115,usability,tool,tools,1115,"(https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1301,usability,user,user,1301,"... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipeline",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1419,usability,memor,memory,1419,"course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:. - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is cons",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1644,usability,memor,memory,1644,"and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2256,usability,user,user,2256,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2418,usability,consist,consistently,2418,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2520,usability,simpl,simple,2520,"ck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions. - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_? Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday! Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:579,deployability,API,API,579,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:634,deployability,api,api-guide,634,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:898,deployability,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:579,integrability,API,API,579,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:634,integrability,api,api-guide,634,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:898,integrability,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:579,interoperability,API,API,579,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:634,interoperability,api,api-guide,634,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:898,modifiability,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:118,safety,test,test,118,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:898,safety,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:118,testability,test,test,118,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:751,testability,simpl,simpler,751,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:898,testability,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:35,usability,prefer,preferred,35,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:467,usability,learn,learn,467,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:583,usability,guid,guide,583,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:638,usability,guid,guide,638,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:751,usability,simpl,simpler,751,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:777,usability,person,personal,777,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:786,usability,prefer,preference,786,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:915,usability,user,user,915,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```. processed_data = raw_data.log1p().normalize(options...).some_other_method(options...). ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:69,safety,test,tested,69,I modified the code to reintroduce `chunked` but I confess I haven't tested it because I've never been able to get it to work. I think there might be other issues with that functionality...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:2,security,modif,modified,2,I modified the code to reintroduce `chunked` but I confess I haven't tested it because I've never been able to get it to work. I think there might be other issues with that functionality...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:69,testability,test,tested,69,I modified the code to reintroduce `chunked` but I confess I haven't tested it because I've never been able to get it to work. I think there might be other issues with that functionality...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:407,availability,operat,operate,407,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:901,availability,operat,operations,901,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1037,availability,operat,operations,1037,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1722,availability,operat,operation,1722,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:954,deployability,contain,container,954,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:985,deployability,contain,contain,985,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1774,deployability,API,API,1774,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1819,deployability,API,APIs,1819,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1598,energy efficiency,model,model,1598,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1774,integrability,API,API,1774,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1819,integrability,API,APIs,1819,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1774,interoperability,API,API,1774,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1819,interoperability,API,APIs,1819,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1582,performance,Time,TimeDistributed,1582,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1778,reliability,doe,doesn,1778,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:283,safety,compl,completely,283,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:535,safety,compl,completely,535,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1140,safety,compl,complicated,1140,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1169,safety,compl,complicated,1169,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:283,security,compl,completely,283,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:422,security,modif,modified,422,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:535,security,compl,completely,535,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1140,security,compl,complicated,1140,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1169,security,compl,complicated,1169,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1598,security,model,model,1598,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1623,security,modif,modified,1623,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:206,usability,learn,learn,206,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1094,usability,clear,clear,1094,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1276,usability,tool,toolkit,1276,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1292,usability,clear,clear,1292,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1318,usability,clear,clear,1318,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place? Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:77,deployability,build,building,77,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:948,modifiability,interm,intermediate,948,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:568,performance,memor,memory,568,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:32,reliability,doe,doesn,32,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:52,security,modif,modifications,52,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:135,security,modif,modify,135,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:478,security,immut,immutable,478,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:611,security,immut,immutability,611,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:159,usability,learn,learn,159,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:568,usability,memor,memory,568,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:691,usability,Person,Personally,691,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:777,usability,prefer,prefer,777,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:927,usability,workflow,workflow,927,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:985,usability,behavi,behavior,985,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:669,availability,avail,available,669,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1486,deployability,pipelin,pipelines,1486,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1486,integrability,pipelin,pipelines,1486,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:804,modifiability,interm,intermediate,804,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1260,performance,time,time,1260,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1421,performance,disk,disk,1421,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:284,reliability,doe,doesn,284,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:669,reliability,availab,available,669,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1523,reliability,doe,doesn,1523,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:483,safety,safe,safeguard,483,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:669,safety,avail,available,669,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:669,security,availab,available,669,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1127,security,modif,modified,1127,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:444,testability,simpl,simple,444,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1567,testability,simpl,simply,1567,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:61,usability,help,helpful,61,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:444,usability,simpl,simple,444,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:783,usability,workflow,workflow,783,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:881,usability,behavi,behavior,881,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:1567,usability,simpl,simply,1567,"Thank you for all your thoughts! That's very interesting and helpful! > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:26,safety,test,test,26,@Koncopd I made an actual test from your notebook :smile:: https://github.com/theislab/scanpy/commit/8d4ec6376c5b338456ced0bc051683052f15aa37,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:26,testability,test,test,26,@Koncopd I made an actual test from your notebook :smile:: https://github.com/theislab/scanpy/commit/8d4ec6376c5b338456ced0bc051683052f15aa37,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/192:2272,availability,cluster,clusters,2272,"anov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** :",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2417,availability,cluster,clusters,2417,": `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3177,availability,state,states,3177,"ter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3230,availability,state,state,3230,"ult: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomS",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3916,availability,error,error,3916,"exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7477,availability,slo,slowly,7477,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7589,availability,redund,redundency,7589,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7677,availability,error,errors,7677,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1191,deployability,instal,install,1191,"iable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_ex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1301,deployability,automat,automatically,1301,"rlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1689,deployability,automat,automatically,1689,"c.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2272,deployability,cluster,clusters,2272,"anov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** :",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2417,deployability,cluster,clusters,2417,": `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3366,deployability,instal,installed,3366,"For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3547,deployability,Depend,Depending,3547,"es during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3579,deployability,updat,updates,3579,". the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4488,deployability,observ,observations,4488,"eturn a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `No",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:5721,deployability,automat,automatically,5721,"thod=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6739,deployability,Depend,Depending,6739,"vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm settin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6760,deployability,updat,updates,6760,"used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7317,deployability,api,api,7317,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7328,deployability,api,api,7328,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7887,deployability,depend,depending,7887,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:70,energy efficiency,cool,cool,70,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2570,energy efficiency,optim,optimization,2570," `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2895,energy efficiency,optim,optimization,2895,"lated to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3192,energy efficiency,optim,optimization,3192,"eration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, ran",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4625,energy efficiency,estimat,estimating,4625,"ne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4809,energy efficiency,adapt,adaption,4809," now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6374,energy efficiency,adapt,adaptive,6374,"nel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `N",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3177,integrability,state,states,3177,"ter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3230,integrability,state,state,3230,"ult: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomS",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3547,integrability,Depend,Depending,3547,"es during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4809,integrability,adapt,adaption,4809," now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6374,integrability,adapt,adaptive,6374,"nel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `N",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6739,integrability,Depend,Depending,6739,"vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm settin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7317,integrability,api,api,7317,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7328,integrability,api,api,7328,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7887,integrability,depend,depending,7887,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:917,interoperability,distribut,distributed,917,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4809,interoperability,adapt,adaption,4809," now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6374,interoperability,adapt,adaptive,6374,"nel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `N",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7317,interoperability,api,api,7317,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7328,interoperability,api,api,7328,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:192,modifiability,variab,variable,192,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:327,modifiability,Paramet,Parameters,327,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1336,modifiability,Paramet,Parameters,1336,"s a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how muc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1819,modifiability,paramet,parameters,1819,"ne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2174,modifiability,paramet,parameter,2174,"ence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2491,modifiability,paramet,parameter,2491,"==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Ret",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2726,modifiability,pac,package,2726,"s used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2801,modifiability,paramet,parameter,2801,"with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3335,modifiability,pac,package,3335,"uch space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3547,modifiability,Depend,Depending,3547,"es during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4809,modifiability,adapt,adaption,4809," now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4839,modifiability,Paramet,Parameters,4839,"ad. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:5851,modifiability,paramet,parameters,5851,"ata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['nei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6374,modifiability,adapt,adaptive,6374,"nel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `N",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6739,modifiability,Depend,Depending,6739,"vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm settin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7887,modifiability,depend,depending,7887,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:8011,modifiability,paramet,parameters,8011,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2570,performance,optimiz,optimization,2570," `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2895,performance,optimiz,optimization,2895,"lated to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3192,performance,optimiz,optimization,3192,"eration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, ran",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3916,performance,error,error,3916,"exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7677,performance,error,errors,7677,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:658,reliability,doe,doesn,658,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7477,reliability,slo,slowly,7477,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1315,safety,detect,detected,1315,"ions? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3547,safety,Depend,Depending,3547,"es during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3579,safety,updat,updates,3579,". the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3916,safety,error,error,3916,"exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3980,safety,except,except,3980,"the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6739,safety,Depend,Depending,6739,"vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm settin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6760,safety,updat,updates,6760,"used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7563,safety,compl,completely,7563,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7589,safety,redund,redundency,7589,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7677,safety,error,errors,7677,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7887,safety,depend,depending,7887,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:684,security,Sign,Signature,684,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1315,security,detect,detected,1315,"ions? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2245,security,Control,Controls,2245,"/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reprod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3579,security,updat,updates,3579,". the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3769,security,sign,signature,3769,"ing rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3797,security,sign,signature,3797,"arameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4037,security,Sign,Signature,4037,"sing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6760,security,updat,updates,6760,"used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7563,security,compl,completely,7563,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7608,security,sign,signature,7608,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7771,security,sign,signature,7771,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1301,testability,automat,automatically,1301,"rlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1689,testability,automat,automatically,1689,"c.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2245,testability,Control,Controls,2245,"/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reprod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3547,testability,Depend,Depending,3547,"es during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4488,testability,observ,observations,4488,"eturn a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `No",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:5721,testability,automat,automatically,5721,"thod=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:6739,testability,Depend,Depending,6739,"vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm settin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7887,testability,depend,depending,7887,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:432,usability,help,help,432,"Hey Phil! Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Large",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1005,usability,visual,visualizating,1005,"ce, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1106,usability,learn,learn,1106,"ic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them. - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart. - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1622,usability,indicat,indicated,1622,"oking it in a Jupyter notebook doesn't look nice... ```. Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False). Docstring:. t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:1974,usability,learn,learning,1974,"8]_ has been. proposed for visualizating single-cell data by [Amir13]_. Here, by default,. we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve. a huge speedup and better convergence if you install `Multicore-tSNE. <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which. will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2622,usability,learn,learning,2622,"cated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2769,usability,learn,learning,2769,"a' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that. is used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the sign",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:2946,usability,learn,learning,2946," used in other manifold learning algorithms. Larger datasets. usually require a larger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3028,usability,minim,minimum,3028,"rger perplexity. Consider selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3051,usability,learn,learning,3051,"er selecting a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3065,usability,help,helps,3065,"a value. between 5 and 50. The choice is not extremely critical since t-SNE. is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the. embedded space and how much space will be between them. For larger. values, the space between natural clusters will be larger in the. embedded space. Again, the choice of this parameter is not very. critical. If the cost function increases during initial optimization,. the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200. The learning rate can be a critical parameter. It should be. between 100 and 1000. If the cost function increases during initial. optimization, the early exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:3916,usability,error,error,3916,"exaggeration factor or the learning rate. might be too high. If the cost function gets stuck in a bad local. minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,. the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:4535,usability,efficien,efficiency,4535,"urns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`). ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature. - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read. - There is an error ` <class 'inspect._empty'>`. - The rest looks good to me, except for the superficial stylistic remarks above. ```. Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]. Docstring:. Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,. which also provides a method for estimating connectivities of data points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:5654,usability,indicat,indicated,5654,"points -. the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,. connectivities are computed according to [Coifman05]_, in the adaption of. [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data. points) used for manifold approximation. Larger values result in more. global views of the manifold, while smaller values result in more local. data being preserved. In general values should be in the range 2 to 100. If `knn` is `True`, number of nearest neighbors to be searched. If `knn`. is `False`, a Gaussian kernel width is set to the distance of the. `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen. automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used. If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to. `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian. Kernel to assign low weights to neighbors more distant than the. `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7677,usability,error,errors,7677,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:7958,usability,Clear,Clearly,7958,"or. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_. with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data. points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of. neighbors. File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py. Type: function. ```. PS: . - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks. - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:250,interoperability,format,format,250,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:51,modifiability,extens,extension,51,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:339,modifiability,paramet,parameters,339,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:221,reliability,doe,doesn,221,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:519,safety,compl,complex,519,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:110,security,modif,modify,110,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:519,security,compl,complex,519,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:418,testability,simpl,simple,418,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:418,usability,simpl,simple,418,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:12,interoperability,format,formatting,12,"OK, got the formatting issues. Let's discuss at the office. Let's stick with `None`, this just requires to rewrite a very small number of strings... will not be a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:395,availability,consist,consist,395,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:154,deployability,contain,contains,154,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:498,integrability,discover,discovered,498,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:354,interoperability,format,format,354,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:498,interoperability,discover,discovered,498,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:182,modifiability,paramet,parameter,182,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:415,modifiability,paramet,parameter,415,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:52,testability,simpl,simpler,52,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:52,usability,simpl,simpler,52,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:395,usability,consist,consist,395,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:498,usability,discov,discovered,498,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:62,deployability,api,api,62,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:73,deployability,api,api,73,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:147,deployability,api,api,147,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:158,deployability,api,api,158,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:229,deployability,api,api,229,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:240,deployability,api,api,240,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:62,integrability,api,api,62,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:73,integrability,api,api,73,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:147,integrability,api,api,147,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:158,integrability,api,api,158,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:229,integrability,api,api,229,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:240,integrability,api,api,240,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:62,interoperability,api,api,62,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:73,interoperability,api,api,73,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:147,interoperability,api,api,147,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:158,interoperability,api,api,158,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:229,interoperability,api,api,229,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:240,interoperability,api,api,240,Rendered:. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.pp.neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.html. - https://scanpy.readthedocs.io/en/type-annotation/api/scanpy.api.Neighbors.compute_neighbors.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/192:140,reliability,doe,does,140,"Looks very good to me! Also the plain text docstrings look good. A few small things remain, for instance that the `AnnData` type annotation does not resolve to `:class:~anndata.AnnData` and is hence no longer linked... It's not a dramatic thing, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/193:51,integrability,sub,subset,51,"One option is to not allow using to request a gene subset with MAGIC from Scanpy, but I don't particularly like that as the returned data is forcibly dense and could be extremely large. Alternatively, I could return a new AnnData object (force copy=True) with var_names corresponding to the genes that were returned.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/pull/193:417,availability,operat,operation,417,"I'd say that the second option is very viable for the task of imputation, which is a bit different than all other tools - instead of ""adding"" annotation, one replaces the whole data matrix. So, I'd say, if you do _not_ want to impute the whole data matrix as is, one should return a new AnnData for the wished gene selection. Now the only problem is that people might get confused because magic doesn't do an inplace operation in this case. So I'd leave `copy=None`, the default behavior being the inplace replacement of the full data matrix. If one passes a gene list, the passed AnnData should stay unchanged but a new imputed ""AnnData"" is returned. People can then directly use this for further tasks... Sounds like a good solution to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/pull/193:395,reliability,doe,doesn,395,"I'd say that the second option is very viable for the task of imputation, which is a bit different than all other tools - instead of ""adding"" annotation, one replaces the whole data matrix. So, I'd say, if you do _not_ want to impute the whole data matrix as is, one should return a new AnnData for the wished gene selection. Now the only problem is that people might get confused because magic doesn't do an inplace operation in this case. So I'd leave `copy=None`, the default behavior being the inplace replacement of the full data matrix. If one passes a gene list, the passed AnnData should stay unchanged but a new imputed ""AnnData"" is returned. People can then directly use this for further tasks... Sounds like a good solution to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/pull/193:114,usability,tool,tools,114,"I'd say that the second option is very viable for the task of imputation, which is a bit different than all other tools - instead of ""adding"" annotation, one replaces the whole data matrix. So, I'd say, if you do _not_ want to impute the whole data matrix as is, one should return a new AnnData for the wished gene selection. Now the only problem is that people might get confused because magic doesn't do an inplace operation in this case. So I'd leave `copy=None`, the default behavior being the inplace replacement of the full data matrix. If one passes a gene list, the passed AnnData should stay unchanged but a new imputed ""AnnData"" is returned. People can then directly use this for further tasks... Sounds like a good solution to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/pull/193:479,usability,behavi,behavior,479,"I'd say that the second option is very viable for the task of imputation, which is a bit different than all other tools - instead of ""adding"" annotation, one replaces the whole data matrix. So, I'd say, if you do _not_ want to impute the whole data matrix as is, one should return a new AnnData for the wished gene selection. Now the only problem is that people might get confused because magic doesn't do an inplace operation in this case. So I'd leave `copy=None`, the default behavior being the inplace replacement of the full data matrix. If one passes a gene list, the passed AnnData should stay unchanged but a new imputed ""AnnData"" is returned. People can then directly use this for further tasks... Sounds like a good solution to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/issues/194:116,performance,time,time,116,"Great, that worked! So, correct me if I'm wrong, by setting n_jobs=1 it forces the computer to handle one task at a time instead of many parallel processes which may have been crashing the program?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:137,performance,parallel,parallel,137,"Great, that worked! So, correct me if I'm wrong, by setting n_jobs=1 it forces the computer to handle one task at a time instead of many parallel processes which may have been crashing the program?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:91,deployability,instal,installation,91,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:204,deployability,releas,release,204,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:59,performance,parallel,parallel,59,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:104,reliability,doe,doesn,104,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/196:141,usability,help,helpful,141,I wrote something that’s able to convert between AnnData and SingleCellExperiment: https://github.com/flying-sheep/anndata2ri/. Maybe that’s helpful?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/196
https://github.com/scverse/scanpy/pull/199:32,availability,error,error,32,Anyone familiar with the travis error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:32,performance,error,error,32,Anyone familiar with the travis error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:32,safety,error,error,32,Anyone familiar with the travis error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:32,usability,error,error,32,Anyone familiar with the travis error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:124,availability,error,error,124,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:182,deployability,version,version,182,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:182,integrability,version,version,182,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:182,modifiability,version,version,182,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:124,performance,error,error,124,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:47,safety,test,tests,47,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:124,safety,error,error,124,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:275,safety,test,tests,275,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:47,testability,test,tests,47,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:275,testability,test,tests,275,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/199:124,usability,error,error,124,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables. Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/issues/200:109,deployability,api,api,109,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:120,deployability,api,api,120,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:144,deployability,api,api,144,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:233,deployability,api,api,233,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:244,deployability,api,api,244,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:278,deployability,api,api,278,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:109,integrability,api,api,109,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:120,integrability,api,api,120,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:144,integrability,api,api,144,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:233,integrability,api,api,233,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:244,integrability,api,api,244,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:278,integrability,api,api,278,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:109,interoperability,api,api,109,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:120,interoperability,api,api,120,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:144,interoperability,api,api,144,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:233,interoperability,api,api,233,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:244,interoperability,api,api,244,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:278,interoperability,api,api,278,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/200:38,modifiability,paramet,parameter,38,"Yes, sure! Use either the `color_map` parameter in any of [`pl.tsne`](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.tsne.html#scanpy.api.pl.tsne) or globally set the color map [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/200
https://github.com/scverse/scanpy/issues/202:138,usability,interact,interactive,138,"Hm, I really don't know what you're talking about... 🙂 Are you using an ordinary Python shell? If yes, use a jupyter notebook if you want interactive plotting. If you want to stick with a Python script, turn off showing figures interacticely (`sc.settings.autoshow = False`) and save them instead (`sc.settings.autosave = True`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202
https://github.com/scverse/scanpy/issues/202:228,usability,interact,interacticely,228,"Hm, I really don't know what you're talking about... 🙂 Are you using an ordinary Python shell? If yes, use a jupyter notebook if you want interactive plotting. If you want to stick with a Python script, turn off showing figures interacticely (`sc.settings.autoshow = False`) and save them instead (`sc.settings.autosave = True`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202
https://github.com/scverse/scanpy/issues/203:47,usability,experien,experienced,47,"Hm, strange. The others in the lab and I never experienced that behaviour... Without further details, I can't do anything...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:64,usability,behavi,behaviour,64,"Hm, strange. The others in the lab and I never experienced that behaviour... Without further details, I can't do anything...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:314,deployability,automat,automatically,314,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:328,deployability,depend,depending,328,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:188,integrability,wrap,wrapper,188,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:328,integrability,depend,depending,328,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:188,interoperability,wrapper,wrapper,188,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:33,modifiability,paramet,parametter,33,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:328,modifiability,depend,depending,328,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:425,performance,time,time,425,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:328,safety,depend,depending,328,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:314,testability,automat,automatically,314,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:328,testability,depend,depending,328,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:83,usability,document,document,83,"@falexwolf . Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem. The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/204:46,availability,error,errors,46,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:46,performance,error,errors,46,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:46,safety,error,errors,46,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:139,safety,test,tests,139,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:148,safety,avoid,avoid,148,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:139,testability,test,tests,139,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:46,usability,error,errors,46,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/206:73,usability,help,help,73,I thought I had fixed it but it turns out that making that change didn't help. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:64,deployability,version,version,64,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:199,deployability,updat,updated,199,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:247,deployability,version,version,247,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:298,deployability,instal,install,298,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:308,deployability,upgrad,upgrade,308,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:64,integrability,version,version,64,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:247,integrability,version,version,247,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:370,integrability,sub,subdirectory,370,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:64,modifiability,version,version,64,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:247,modifiability,version,version,247,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:308,modifiability,upgrad,upgrade,308,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:199,safety,updat,updated,199,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:199,security,updat,updated,199,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:31,usability,confirm,confirm,31,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:210,usability,support,support,210,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:318,usability,user,user,318,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running. ```. import magic. magic.__version__. ```. MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:. ```. pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python. ```. Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:9,deployability,version,version,9,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:24,deployability,Updat,Updating,24,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:47,deployability,version,version,47,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:9,integrability,version,version,9,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:47,integrability,version,version,47,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:9,modifiability,version,version,9,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:47,modifiability,version,version,47,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:24,safety,Updat,Updating,24,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/issues/206:24,security,Updat,Updating,24,I was on version 1.0.0. Updating to the newest version solved the problem. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/pull/207:89,interoperability,conflict,conflict,89,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:111,usability,clear,clear,111,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:160,usability,user,user-images,160,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:391,deployability,api,api,391,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:498,deployability,version,version,498,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:490,energy efficiency,current,current,490,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:391,integrability,api,api,391,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:498,integrability,version,version,498,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:391,interoperability,api,api,391,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:520,interoperability,conflict,conflicts,520,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:498,modifiability,version,version,498,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:621,modifiability,paramet,parameters,621,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:153,safety,test,tests,153,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:271,safety,test,test,271,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:417,safety,test,test,417,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:153,testability,test,tests,153,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:271,testability,test,test,271,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:417,testability,test,test,417,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:50,interoperability,conflict,conflicts,50,Tell me when you're finished and I'll resolve the conflicts. No problem about that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,deployability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,integrability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:192,integrability,repositor,repository,192,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,interoperability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:192,interoperability,repositor,repository,192,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,modifiability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,reliability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:84,safety,test,tests,84,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,security,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:43,testability,integr,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:84,testability,test,tests,84,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:64,usability,visual,visualizations,64,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:98,usability,document,documentation,98,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:201,availability,down,downloaded,201,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:187,deployability,automat,automatically,187,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:48,integrability,repositor,repository,48,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:77,integrability,sub,subsampled,77,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:578,integrability,repositor,repository,578,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:48,interoperability,repositor,repository,48,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:578,interoperability,repositor,repository,578,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:510,performance,cach,cache,510,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:558,safety,avoid,avoids,558,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:187,testability,automat,automatically,187,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:30,availability,down,downloading,30,"@falexwolf I like the idea of downloading the data on the fly! For an idea of a small dataset that is already filtered and normalized, do you know if the pbmc data from 10x Genomics has some restrictions? ... nevermind, they have a Creative Commons licence. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:110,integrability,filter,filtered,110,"@falexwolf I like the idea of downloading the data on the fly! For an idea of a small dataset that is already filtered and normalized, do you know if the pbmc data from 10x Genomics has some restrictions? ... nevermind, they have a Creative Commons licence. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:181,deployability,API,API,181,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:206,deployability,api,api,206,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:262,deployability,api,api,262,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:273,deployability,api,api,273,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:181,integrability,API,API,181,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:206,integrability,api,api,206,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:262,integrability,api,api,262,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:273,integrability,api,api,273,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:181,interoperability,API,API,181,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:206,interoperability,api,api,206,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:262,interoperability,api,api,262,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:273,interoperability,api,api,273,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:373,reliability,doe,does,373,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:52,safety,test,tests,52,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:52,testability,test,tests,52,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:104,testability,understand,understand,104,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:185,usability,document,documentation,185,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:61,safety,test,tests,61,"Sorry, but I think there still is a range-related bug in the tests:. ```. E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:87,safety,test,testing,87,"Sorry, but I think there still is a range-related bug in the tests:. ```. E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:95,safety,except,exceptions,95,"Sorry, but I think there still is a range-related bug in the tests:. ```. E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:61,testability,test,tests,61,"Sorry, but I think there still is a range-related bug in the tests:. ```. E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:87,testability,test,testing,87,"Sorry, but I think there still is a range-related bug in the tests:. ```. E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:63,deployability,version,version,63,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:63,integrability,version,version,63,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:63,modifiability,version,version,63,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:9,safety,test,tests,9,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:9,testability,test,tests,9,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:117,usability,close,closer,117,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:95,deployability,automat,automatic,95,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:105,deployability,API,API,105,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:105,integrability,API,API,105,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:105,interoperability,API,API,105,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:0,safety,Test,Test,0,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:0,testability,Test,Test,0,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:95,testability,automat,automatic,95,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:109,usability,document,documentation,109,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:495,deployability,api,api,495,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:553,deployability,api,api,553,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:618,deployability,api,api,618,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:629,deployability,api,api,629,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:495,integrability,api,api,495,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:553,integrability,api,api,553,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:618,integrability,api,api,618,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:629,integrability,api,api,629,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:755,integrability,repositor,repository-contributors,755,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:495,interoperability,api,api,495,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:553,interoperability,api,api,553,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:618,interoperability,api,api,618,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:629,interoperability,api,api,629,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:755,interoperability,repositor,repository-contributors,755,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:377,testability,simpl,simply,377,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:377,usability,simpl,simply,377,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:687,usability,guid,guidelines,687,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:707,usability,help,help,707,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:740,usability,guid,guidelines-for-repository-contributors,740,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:176,energy efficiency,cool,cool,176,"OK, merged this into master on the command line after fixing `plotting/anndata.py`, where some changes would have otherwise been reversed... Thank you very much, Fidel! Really cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:35,usability,command,command,35,"OK, merged this into master on the command line after fixing `plotting/anndata.py`, where some changes would have otherwise been reversed... Thank you very much, Fidel! Really cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:349,energy efficiency,cool,cool,349,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:81,safety,test,test,81,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:401,security,auth,authored,401,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:81,testability,test,test,81,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:51,usability,document,documentation,51,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:204,usability,command,command,204,". I hope that soon I can add some figures into the documentation and add further test. . . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:. > . > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed... > . > Thank you very much, Fidel! Really cool! > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/issues/208:11,usability,help,helps,11,Maybe this helps https://github.com/theislab/scanpy/issues/206?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:18,deployability,updat,updated,18,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:36,deployability,version,version,36,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:36,integrability,version,version,36,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:36,modifiability,version,version,36,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:18,safety,updat,updated,18,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:18,security,updat,updated,18,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:8,usability,help,helped,8,"Yes, It helped. I updated the magic version and it seems to be working fine now. . Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/209:34,safety,compl,completely,34,"Oh man, sorry that I seem to have completely forgotten about this here. And even worse, it looks very much like a very strange bug. I could reproduce the behavior but need to look into it more closely tomorrow or during the next days...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:34,security,compl,completely,34,"Oh man, sorry that I seem to have completely forgotten about this here. And even worse, it looks very much like a very strange bug. I could reproduce the behavior but need to look into it more closely tomorrow or during the next days...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:154,usability,behavi,behavior,154,"Oh man, sorry that I seem to have completely forgotten about this here. And even worse, it looks very much like a very strange bug. I could reproduce the behavior but need to look into it more closely tomorrow or during the next days...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:193,usability,close,closely,193,"Oh man, sorry that I seem to have completely forgotten about this here. And even worse, it looks very much like a very strange bug. I could reproduce the behavior but need to look into it more closely tomorrow or during the next days...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/210:166,integrability,filter,filtering,166,"My two cents: `normalize_per_cell` shouldn't remove zero-expression cells. That functionality should explicitly be taken care of by either `filter_cells` or specific filtering expressions like. ```{python}. adata = adata[adata.obs['n_genes'] > 200, :]. ```. Modifying `raw`, especially without a warning, shouldn't happen unless it's done directly by the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:157,interoperability,specif,specific,157,"My two cents: `normalize_per_cell` shouldn't remove zero-expression cells. That functionality should explicitly be taken care of by either `filter_cells` or specific filtering expressions like. ```{python}. adata = adata[adata.obs['n_genes'] > 200, :]. ```. Modifying `raw`, especially without a warning, shouldn't happen unless it's done directly by the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:258,security,Modif,Modifying,258,"My two cents: `normalize_per_cell` shouldn't remove zero-expression cells. That functionality should explicitly be taken care of by either `filter_cells` or specific filtering expressions like. ```{python}. adata = adata[adata.obs['n_genes'] > 200, :]. ```. Modifying `raw`, especially without a warning, shouldn't happen unless it's done directly by the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:355,usability,user,user,355,"My two cents: `normalize_per_cell` shouldn't remove zero-expression cells. That functionality should explicitly be taken care of by either `filter_cells` or specific filtering expressions like. ```{python}. adata = adata[adata.obs['n_genes'] > 200, :]. ```. Modifying `raw`, especially without a warning, shouldn't happen unless it's done directly by the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:189,integrability,filter,filtered,189,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:198,integrability,sub,subset,198,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:311,integrability,sub,subsetting,311,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:439,integrability,sub,subset,439,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:622,integrability,sub,subselection,622,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:211,reliability,doe,does,211,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:149,usability,visual,visualization,149,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:668,usability,visual,visualize,668,"This is very related to a discussion I've been having with Alex about e.g. `rank_genes_groups()` which is run on the full .raw dataset, but then the visualization happens on a possibly HVG filtered subset which does not have certain indices. So the problem is bigger than just `normalize_per_cell()`. . I think subsetting raw is not really a viable option as you want especially rank_genes_groups() to be run on all genes rather than on a subset which may be chosen for some reason that is unrelated to the genes' importance as marker genes. Rather you will need a solution that revolves around selecting which processing/subselection level of data you want to use to visualize...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:374,availability,operat,operation,374,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:483,availability,operat,operation,483,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:695,availability,error,error,695,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:278,deployability,version,version,278,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:397,deployability,observ,observations,397,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:278,integrability,version,version,278,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:364,integrability,filter,filtering,364,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:473,integrability,filter,filtering,473,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:343,interoperability,compatib,compatible,343,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:278,modifiability,version,version,278,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:500,modifiability,variab,variables,500,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:695,performance,error,error,695,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:695,safety,error,error,695,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:397,testability,observ,observations,397,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:695,usability,error,error,695,"Thank you for your thoughts! 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible... 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. . 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:444,deployability,fail,fail,444,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:189,integrability,sub,subsetted,189,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:359,integrability,sub,subset,359,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:651,modifiability,paramet,parameter,651,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:444,reliability,fail,fail,444,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:611,safety,test,tested,611,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:611,testability,test,tested,611,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:54,deployability,log,logging,54,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:788,deployability,observ,observe,788,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:195,integrability,sub,subsetting,195,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:54,safety,log,logging,54,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:54,security,log,logging,54,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:54,testability,log,logging,54,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:788,testability,observ,observe,788,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:520,usability,tool,tools,520,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:694,usability,tool,tools,694,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf. 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6. 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:259,performance,time,time,259,"Ah, yes... this was a commit I made a month ago, which I had forgotten about that fixed the issue at least in part (caddf9b5934301f9cf2048e6bb947161fd84a210). I recall that I found it harder to fix for `pl.rank_genes_groups_violin` and so I left that for the time being as I felt it was a longer discussion. Let's do this offline next week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:391,energy efficiency,predict,predicted,391,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:35,integrability,filter,filters,35,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:176,integrability,sub,subset,176,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:354,integrability,filter,filters,354,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:272,safety,valid,validation,272,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:391,safety,predict,predicted,391,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:272,security,validat,validation,272,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:121,usability,document,documentation,121,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:474,usability,behavi,behavior,474,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/210:55,energy efficiency,current,current,55,That's a very straight-forward solution viable for the current way in which this is written. A pull request would be very welcome! Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210
https://github.com/scverse/scanpy/issues/211:77,availability,sli,slight,77,"No problem, and yes I did thanks! That's why I closed the issue. There was a slight discrepancy between my merging indices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/211:77,reliability,sli,slight,77,"No problem, and yes I did thanks! That's why I closed the issue. There was a slight discrepancy between my merging indices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/211:47,usability,close,closed,47,"No problem, and yes I did thanks! That's why I closed the issue. There was a slight discrepancy between my merging indices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/211:98,integrability,filter,filtering,98,"Hi,. I have a similar problem here. I want to know how to merge two different anndata if I do the filtering separately first. Anyone knows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/211:215,integrability,filter,filtering,215,"Check out `AnnData.concatenate()` [here](https://anndata.readthedocs.io/en/latest/anndata.AnnData.concatenate.html). However, you can only concatenate if the anndata objects have the same `adata.var_names`. So gene filtering cannot be done separately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/212:39,availability,error,error,39,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:61,deployability,contain,contains,61,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:108,integrability,filter,filtering,108,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:317,integrability,sub,subscribed,317,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:39,performance,error,error,39,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:39,safety,error,error,39,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:39,usability,error,error,39,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before? Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:. > . > Can you try running with n_jobs=1? > . > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:38,deployability,pipelin,pipeline,38,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:174,deployability,updat,update,174,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:215,deployability,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:226,deployability,modul,module,226,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:262,deployability,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:317,deployability,instal,installing,317,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:368,deployability,updat,updated,368,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:17,integrability,filter,filtering,17,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:38,integrability,pipelin,pipeline,38,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:215,integrability,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:262,integrability,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:215,modifiability,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:226,modifiability,modul,module,226,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:262,modifiability,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:333,modifiability,pac,package,333,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:174,safety,updat,update,174,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:215,safety,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:226,safety,modul,module,226,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:262,safety,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:368,safety,updat,updated,368,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:174,security,updat,update,174,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:368,security,updat,updated,368,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:215,testability,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:262,testability,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————. The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. . After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:105,deployability,modul,module,105,"I could not find an import for `patsy` (I think is patsy and not pasty) in the scanpy repo. I think this module is required by other package, probably sklearn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:105,modifiability,modul,module,105,"I could not find an import for `patsy` (I think is patsy and not pasty) in the scanpy repo. I think this module is required by other package, probably sklearn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:133,modifiability,pac,package,133,"I could not find an import for `patsy` (I think is patsy and not pasty) in the scanpy repo. I think this module is required by other package, probably sklearn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:105,safety,modul,module,105,"I could not find an import for `patsy` (I think is patsy and not pasty) in the scanpy repo. I think this module is required by other package, probably sklearn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/pull/216:220,availability,operat,operating,220,"I'd tend to deprecate the `key_ext` parameter. I think I never really used it in the end and I'm having a hard time imagining a canonical use for it. If you want to rename coordinates, you can always do that manually by operating on the AnnData object after calling the tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:172,interoperability,coordinat,coordinates,172,"I'd tend to deprecate the `key_ext` parameter. I think I never really used it in the end and I'm having a hard time imagining a canonical use for it. If you want to rename coordinates, you can always do that manually by operating on the AnnData object after calling the tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:36,modifiability,paramet,parameter,36,"I'd tend to deprecate the `key_ext` parameter. I think I never really used it in the end and I'm having a hard time imagining a canonical use for it. If you want to rename coordinates, you can always do that manually by operating on the AnnData object after calling the tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:111,performance,time,time,111,"I'd tend to deprecate the `key_ext` parameter. I think I never really used it in the end and I'm having a hard time imagining a canonical use for it. If you want to rename coordinates, you can always do that manually by operating on the AnnData object after calling the tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:270,usability,tool,tool,270,"I'd tend to deprecate the `key_ext` parameter. I think I never really used it in the end and I'm having a hard time imagining a canonical use for it. If you want to rename coordinates, you can always do that manually by operating on the AnnData object after calling the tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:341,availability,operat,operating,341,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:290,interoperability,coordinat,coordinates,290,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:149,modifiability,paramet,parameter,149,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:227,performance,time,time,227,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:442,security,auth,authored,442,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:657,security,auth,auth,657,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/216:391,usability,tool,tool,391,"Ah ok, fair enough, nevermind then. On Thu, Jul 26, 2018 at 4:54 PM Alex Wolf <notifications@github.com> wrote:. > I'd tend to deprecate the key_ext parameter. I think I never really used. > it in the end and I'm having a hard time imagining a canonical use for it. > If you want to rename coordinates, you can always do that manually by. > operating on the AnnData object after calling the tool. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/216#issuecomment-408124653>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQHgg9TKnIrfqtx1R0e6x_1_qvRJks5uKdg3gaJpZM4Vh6jK>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/216
https://github.com/scverse/scanpy/pull/217:79,usability,visual,visualization,79,"Hm, I'm not super happy with that additional flexibility. People can break the visualization function `pl.umap` in that case. Why would you need it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:34,energy efficiency,draw,draw,34,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:17,modifiability,paramet,parameter,17,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:164,modifiability,paramet,parameter,164,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:85,reliability,doe,doesn,85,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:551,security,auth,authored,551,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:766,security,auth,auth,766,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/pull/217:438,usability,visual,visualization,438,"I found the same parameter in the draw graph function and used it to make. sure paga doesn’t overwrite the original coords. Given that you don’t like. the original parameter, I agree, let’s just remove it everywhere. Never. mind! Guess I felt like doing pull requests today... On Thu, Jul 26, 2018 at 4:52 PM Alex Wolf <notifications@github.com> wrote:. > Hm, I'm not super happy with that additional flexibility. People can break. > the visualization function pl.umap in that case. Why would you need it? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/217#issuecomment-408123751>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TRSVGolotm8iHNvB86pLgzi05YJ8ks5uKdevgaJpZM4Vh7eB>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217
https://github.com/scverse/scanpy/issues/218:11,deployability,automat,automatically,11,"Won't this automatically lexicographically sort, among other things, the order of plot legend labels for all scatter plots now? That is probably not desirable as default behavior; the order of the categories should be defined at the data object level, and not altered during plotting. There should instead maybe be a utility function to properly assigns a palette to your labels prior to plotting, then the actual plotting functionality can stay the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:11,testability,automat,automatically,11,"Won't this automatically lexicographically sort, among other things, the order of plot legend labels for all scatter plots now? That is probably not desirable as default behavior; the order of the categories should be defined at the data object level, and not altered during plotting. There should instead maybe be a utility function to properly assigns a palette to your labels prior to plotting, then the actual plotting functionality can stay the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:170,usability,behavi,behavior,170,"Won't this automatically lexicographically sort, among other things, the order of plot legend labels for all scatter plots now? That is probably not desirable as default behavior; the order of the categories should be defined at the data object level, and not altered during plotting. There should instead maybe be a utility function to properly assigns a palette to your labels prior to plotting, then the actual plotting functionality can stay the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:20,integrability,discover,discovered,20,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:66,integrability,sub,subtle,66,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:20,interoperability,discover,discovered,20,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:20,usability,discov,discovered,20,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:40,usability,behavi,behavior,40,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:384,usability,behavi,behavior,384,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:430,usability,close,close,430,"@wflynny actually I discovered that the behavior is a little more subtle: they are ordered based on the categories order. Usually this is lexicographically. However, if you produce categories through [pandas.cut()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html) they are ordered following the original bins - not lexicographically. I think that this original behavior is the more natural, so I think I'll close this issue and the linked PR. Thanks,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:59,integrability,coupl,couple,59,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:59,modifiability,coupl,couple,59,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:59,testability,coupl,couple,59,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:605,usability,clear,clears,605,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:144,integrability,discover,discovered,144,"No worries and thanks for the clarification. I initially reworked part of the code because I thought the order was not reproducible. But then I discovered that it was like that because scanpy was using pandas categories, which is the correct implementation in my opinion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:144,interoperability,discover,discovered,144,"No worries and thanks for the clarification. I initially reworked part of the code because I thought the order was not reproducible. But then I discovered that it was like that because scanpy was using pandas categories, which is the correct implementation in my opinion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/issues/218:144,usability,discov,discovered,144,"No worries and thanks for the clarification. I initially reworked part of the code because I thought the order was not reproducible. But then I discovered that it was like that because scanpy was using pandas categories, which is the correct implementation in my opinion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218
https://github.com/scverse/scanpy/pull/219:31,safety,test,tested,31,"Sorry for the PR, was not prop tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/219
https://github.com/scverse/scanpy/pull/219:31,testability,test,tested,31,"Sorry for the PR, was not prop tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/219
https://github.com/scverse/scanpy/issues/220:51,interoperability,format,format,51,This sounds like it may have to do with the matrix format of `adata.X`. Could you check the type of your adata.X object?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:98,energy efficiency,load,loaded,98,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:312,energy efficiency,load,loading,312,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:258,integrability,interfac,interface,258,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:288,integrability,transform,transformed,288,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:258,interoperability,interfac,interface,258,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:288,interoperability,transform,transformed,288,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:258,modifiability,interfac,interface,258,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:98,performance,load,loaded,98,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:199,performance,cach,cache,199,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:312,performance,load,loading,312,"It's numpy.ndarray:. ```type(adata.X)```. ``` numpy.ndarray```. I guess it should be matrix? It's loaded once like this . ```. path = '../count-genes/datafiles/all_counts.csv'. adata = sc.read(path, cache=True). ```. and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:83,interoperability,format,format,83,From your side everything seems okay. `numpy.ndarray` is the expected dense matrix format. It seems the function was written for sparse matrices. I'll see if I can tweak it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:47,deployability,version,version,47,It should be fixed now. If you pull the latest version from github you can check. Please let me know if there's still an issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:47,integrability,version,version,47,It should be fixed now. If you pull the latest version from github you can check. Please let me know if there's still an issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:47,modifiability,version,version,47,It should be fixed now. If you pull the latest version from github you can check. Please let me know if there's still an issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/pull/221:37,energy efficiency,current,current,37,I decided to close this PR since the current behavior is more correct in my opinion. See #218 for details.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/221
https://github.com/scverse/scanpy/pull/221:13,usability,close,close,13,I decided to close this PR since the current behavior is more correct in my opinion. See #218 for details.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/221
https://github.com/scverse/scanpy/pull/221:45,usability,behavi,behavior,45,I decided to close this PR since the current behavior is more correct in my opinion. See #218 for details.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/221
https://github.com/scverse/scanpy/issues/222:136,availability,cluster,clustering,136,"The biggest problem here is that you take `adata.X` in the `silhouette_score` function to compute distances. However, the basis for the clustering typically is the PCA representation of data in `adata.obsm['X_pca']`. If you adopt this, you should get meaningful results. Alternatively, you should be able to pass `adata.uns['neighbors']['distances']` as the first argument of `silhouette_score` if you pass `metric='precomputed'` at the same time... This would also solve the problem I mentioned above. Maybe it doesn't accept sparse matrices and you need to run `.toarray()` before. You don't need `adata.X` in this case, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:136,deployability,cluster,clustering,136,"The biggest problem here is that you take `adata.X` in the `silhouette_score` function to compute distances. However, the basis for the clustering typically is the PCA representation of data in `adata.obsm['X_pca']`. If you adopt this, you should get meaningful results. Alternatively, you should be able to pass `adata.uns['neighbors']['distances']` as the first argument of `silhouette_score` if you pass `metric='precomputed'` at the same time... This would also solve the problem I mentioned above. Maybe it doesn't accept sparse matrices and you need to run `.toarray()` before. You don't need `adata.X` in this case, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:442,performance,time,time,442,"The biggest problem here is that you take `adata.X` in the `silhouette_score` function to compute distances. However, the basis for the clustering typically is the PCA representation of data in `adata.obsm['X_pca']`. If you adopt this, you should get meaningful results. Alternatively, you should be able to pass `adata.uns['neighbors']['distances']` as the first argument of `silhouette_score` if you pass `metric='precomputed'` at the same time... This would also solve the problem I mentioned above. Maybe it doesn't accept sparse matrices and you need to run `.toarray()` before. You don't need `adata.X` in this case, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:512,reliability,doe,doesn,512,"The biggest problem here is that you take `adata.X` in the `silhouette_score` function to compute distances. However, the basis for the clustering typically is the PCA representation of data in `adata.obsm['X_pca']`. If you adopt this, you should get meaningful results. Alternatively, you should be able to pass `adata.uns['neighbors']['distances']` as the first argument of `silhouette_score` if you pass `metric='precomputed'` at the same time... This would also solve the problem I mentioned above. Maybe it doesn't accept sparse matrices and you need to run `.toarray()` before. You don't need `adata.X` in this case, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:53,availability,cluster,clustering,53,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:136,availability,cluster,clustering,136,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1113,availability,cluster,clustering,1113,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:53,deployability,cluster,clustering,53,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:136,deployability,cluster,clustering,136,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1020,deployability,updat,update,1020,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1113,deployability,cluster,clustering,1113,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:36,energy efficiency,predict,predicted,36,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:918,energy efficiency,Measur,Measure,918,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1098,energy efficiency,predict,predict,1098,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:36,safety,predict,predicted,36,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:706,safety,avoid,avoid,706,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1020,safety,updat,update,1020,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1098,safety,predict,predict,1098,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/222:1020,security,updat,update,1020,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. . (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). . It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/223:577,availability,cluster,cluster,577,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:604,availability,cluster,cluster,604,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:639,availability,cluster,cluster,639,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:667,availability,cluster,cluster,667,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:918,availability,cluster,clustering,918,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1039,availability,cluster,clustering,1039,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1105,availability,cluster,clustering,1105,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1116,availability,cluster,clusters,1116,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:473,deployability,modul,modularity,473,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:523,deployability,version,version,523,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:577,deployability,cluster,cluster,577,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:604,deployability,cluster,cluster,604,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:639,deployability,cluster,cluster,639,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:667,deployability,cluster,cluster,667,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:918,deployability,cluster,clustering,918,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1039,deployability,cluster,clustering,1039,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1094,deployability,modul,modularity,1094,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1105,deployability,cluster,clustering,1105,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1116,deployability,cluster,clusters,1116,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:463,energy efficiency,optim,optimizes,463,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:473,integrability,modular,modularity,473,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:511,integrability,graph-bas,graph-based,511,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:523,integrability,version,version,523,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1094,integrability,modular,modularity,1094,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:473,modifiability,modul,modularity,473,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:523,modifiability,version,version,523,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1094,modifiability,modul,modularity,1094,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:463,performance,optimiz,optimizes,463,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:354,reliability,doe,doesn,354,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1061,reliability,doe,doesn,1061,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:473,safety,modul,modularity,473,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1094,safety,modul,modularity,1094,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:473,testability,modula,modularity,473,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1094,testability,modula,modularity,1094,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:881,usability,workflow,workflow,881,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:264,availability,cluster,clusters,264,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:95,deployability,modul,modularity,95,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:264,deployability,cluster,clusters,264,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:28,energy efficiency,measur,measure,28,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:95,integrability,modular,modularity,95,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:95,modifiability,modul,modularity,95,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:127,modifiability,paramet,parameter,127,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:141,reliability,doe,does,141,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:95,safety,modul,modularity,95,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:95,testability,modula,modularity,95,"But Is there a quantitative measure to get an idea of how the graph is improving or worsening, modularity wise as you change a parameter? Or does it just make more sense to go based of marker genes and a seeming need to increase resolution for particularly coarse clusters?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:7,availability,cluster,clustering,7,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:125,availability,cluster,clustering,125,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:883,availability,cluster,clustering,883,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1352,availability,cluster,clusters,1352,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1381,availability,cluster,clusters,1381,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:7,deployability,cluster,clustering,7,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:44,deployability,modul,modularity,44,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:98,deployability,modul,modularity,98,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:125,deployability,cluster,clustering,125,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:184,deployability,modul,modularity,184,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,deployability,modul,modularity,303,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:411,deployability,modul,modularity,411,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:475,deployability,modul,modular,475,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:747,deployability,modul,modularity,747,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:883,deployability,cluster,clustering,883,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1352,deployability,cluster,clusters,1352,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1381,deployability,cluster,clusters,1381,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1453,deployability,modul,modularity,1453,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1501,deployability,Depend,Depending,1501,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:24,energy efficiency,optim,optimization,24,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:289,energy efficiency,optim,optimize,289,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:758,energy efficiency,optim,optimization,758,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1464,energy efficiency,optim,optimization,1464,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:44,integrability,modular,modularity,44,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:98,integrability,modular,modularity,98,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:184,integrability,modular,modularity,184,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,integrability,modular,modularity,303,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:411,integrability,modular,modularity,411,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:475,integrability,modular,modular,475,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:747,integrability,modular,modularity,747,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1453,integrability,modular,modularity,1453,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1501,integrability,Depend,Depending,1501,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1518,interoperability,specif,specific,1518,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:44,modifiability,modul,modularity,44,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:98,modifiability,modul,modularity,98,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:184,modifiability,modul,modularity,184,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:258,modifiability,paramet,parameter,258,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,modifiability,modul,modularity,303,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:411,modifiability,modul,modularity,411,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:475,modifiability,modul,modular,475,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:747,modifiability,modul,modularity,747,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1453,modifiability,modul,modularity,1453,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1501,modifiability,Depend,Depending,1501,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:24,performance,optimiz,optimization,24,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:289,performance,optimiz,optimize,289,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:758,performance,optimiz,optimization,758,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1464,performance,optimiz,optimization,1464,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:536,reliability,doe,does,536,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:44,safety,modul,modularity,44,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:98,safety,modul,modularity,98,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:184,safety,modul,modularity,184,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,safety,modul,modularity,303,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:411,safety,modul,modularity,411,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:475,safety,modul,modular,475,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:747,safety,modul,modularity,747,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1453,safety,modul,modularity,1453,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1501,safety,Depend,Depending,1501,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:695,security,assess,assessment,695,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:44,testability,modula,modularity,44,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:98,testability,modula,modularity,98,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:184,testability,modula,modularity,184,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,testability,modula,modularity,303,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:411,testability,modula,modularity,411,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:475,testability,modula,modular,475,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:747,testability,modula,modularity,747,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1453,testability,modula,modularity,1453,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1501,testability,Depend,Depending,1501,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:1058,usability,tool,tool,1058,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you? I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:73,availability,cluster,clustered,73,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:96,availability,cluster,cluster,96,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:128,availability,cluster,cluster,128,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,availability,cluster,clusters,303,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:73,deployability,cluster,clustered,73,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:96,deployability,cluster,cluster,96,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:128,deployability,cluster,cluster,128,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:303,deployability,cluster,clusters,303,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:439,deployability,continu,continuing,439,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:279,reliability,pra,practical,279,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:453,usability,learn,learn,453,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/226:0,integrability,Sub,Subset,0,"Subset the AnnData object. ```. adata_subset = adata[adata.obs['louvain'] == '1']. ```. Get the expression matrix from an arbitrary AnnData object. ```. adata.X. ```. Get the raw expression matrix if you set the `.raw` attribute at some point. ```. adata.raw.X. ```. Convert the sparse expression matrix of an AnnData object to a dense array. ```. adata.X.toarray(). ```. on which you can call `np.savetxt`, for instance. Easier: save the whole AnnData object as csvs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html. ```. adata.write_csvs(). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:57,availability,cluster,clusters,57,"As we just had this in another issue, subset to multiple clusters:. ```. adata_subset = adata[adata.obs['louvain'].isn(['1', '2', '3'])]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:57,deployability,cluster,clusters,57,"As we just had this in another issue, subset to multiple clusters:. ```. adata_subset = adata[adata.obs['louvain'].isn(['1', '2', '3'])]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:38,integrability,sub,subset,38,"As we just had this in another issue, subset to multiple clusters:. ```. adata_subset = adata[adata.obs['louvain'].isn(['1', '2', '3'])]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:490,availability,sli,sliced,490,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:283,deployability,modul,module,283,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:0,integrability,sub,subsetting,0,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:283,modifiability,modul,module,283,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:382,modifiability,pac,packages,382,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:639,modifiability,pac,packages,639,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:951,modifiability,pac,packages,951,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:1024,modifiability,layer,layers,1024,"'ve this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of the set:. ```py. >>> set(tis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:1298,modifiability,pac,packages,1298,"--> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of the set:. ```py. >>> set(tiss.obs['cell_ontology_class']). {'B cell',. 'NA',. 'T cell',. 'dendritic cell',. 'macrophage',. 'natural killer cell'}. ```. it does work for louvain though:. ```python. >>> tiss[tiss.obs['louvain']=='0']`. ```. View of AnnData object with n_obs × n_vars = 5862 × 19860`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:1629,modifiability,pac,packages,1629,"--> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of the set:. ```py. >>> set(tiss.obs['cell_ontology_class']). {'B cell',. 'NA',. 'T cell',. 'dendritic cell',. 'macrophage',. 'natural killer cell'}. ```. it does work for louvain though:. ```python. >>> tiss[tiss.obs['louvain']=='0']`. ```. View of AnnData object with n_obs × n_vars = 5862 × 19860`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:11,reliability,doe,doesn,11,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:490,reliability,sli,sliced,490,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:2154,reliability,doe,does,2154,"--> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of the set:. ```py. >>> set(tiss.obs['cell_ontology_class']). {'B cell',. 'NA',. 'T cell',. 'dendritic cell',. 'macrophage',. 'natural killer cell'}. ```. it does work for louvain though:. ```python. >>> tiss[tiss.obs['louvain']=='0']`. ```. View of AnnData object with n_obs × n_vars = 5862 × 19860`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:255,safety,input,input-,255,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:283,safety,modul,module,283,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:211,testability,Trace,Traceback,211,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:255,usability,input,input-,255,"subsetting doesn't work:. I've this object. ```python. >>> tiss. ```. AnnData object with n_obs × n_vars = 29322 × 19860. ```python. >>> tiss[tiss.obs['cell_ontology_class']=='B cell']. ```. ```pytb. IndexError Traceback (most recent call last). <ipython-input-269-28b4524131cb> in <module>(). ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7. ```. even though it's part of t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:27,deployability,fail,fails,27,"It should work. Since what fails is some clean up of annData categories,. you may want to check that `tiss.obs['cell_ontology_class']` is set as. categorical. Also check `'B cell' in. tiss.obs['cell_ontology_class'].cat.categories` to avoid a typo. Finally, I think that I had a similar issue which I resolved by removing a adata.uns elements that end in `_colors`. In your case I think you would need to do `del tiss.uns['cell_ontology_class_colors]`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:27,reliability,fail,fails,27,"It should work. Since what fails is some clean up of annData categories,. you may want to check that `tiss.obs['cell_ontology_class']` is set as. categorical. Also check `'B cell' in. tiss.obs['cell_ontology_class'].cat.categories` to avoid a typo. Finally, I think that I had a similar issue which I resolved by removing a adata.uns elements that end in `_colors`. In your case I think you would need to do `del tiss.uns['cell_ontology_class_colors]`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/226:235,safety,avoid,avoid,235,"It should work. Since what fails is some clean up of annData categories,. you may want to check that `tiss.obs['cell_ontology_class']` is set as. categorical. Also check `'B cell' in. tiss.obs['cell_ontology_class'].cat.categories` to avoid a typo. Finally, I think that I had a similar issue which I resolved by removing a adata.uns elements that end in `_colors`. In your case I think you would need to do `del tiss.uns['cell_ontology_class_colors]`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226
https://github.com/scverse/scanpy/issues/227:64,deployability,updat,update,64,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227
https://github.com/scverse/scanpy/issues/227:64,safety,updat,update,64,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227
https://github.com/scverse/scanpy/issues/227:64,security,updat,update,64,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227
https://github.com/scverse/scanpy/pull/228:17,energy efficiency,cool,cool,17,"This looks super cool! Alternatively, one can assign colors to cell types (both rows and columns) and display a legend for cell type - color matching similar to `pheatmap` in R. E.g. . ![image](https://user-images.githubusercontent.com/1140359/43774912-1047c734-9a4b-11e8-9014-20ad46be869c.png). But I think this brackets are more readable as it eliminates the need for color matching.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:202,usability,user,user-images,202,"This looks super cool! Alternatively, one can assign colors to cell types (both rows and columns) and display a legend for cell type - color matching similar to `pheatmap` in R. E.g. . ![image](https://user-images.githubusercontent.com/1140359/43774912-1047c734-9a4b-11e8-9014-20ad46be869c.png). But I think this brackets are more readable as it eliminates the need for color matching.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:174,availability,reliab,reliably,174,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:240,energy efficiency,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:250,energy efficiency,current,current,250,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:240,integrability,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:240,interoperability,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:240,modifiability,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:174,reliability,reliab,reliably,174,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:118,safety,avoid,avoid,118,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:100,usability,person,person,100,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:122,availability,sli,slightly,122,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:51,deployability,fail,fail,51,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:51,reliability,fail,fail,51,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:122,reliability,sli,slightly,122,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:21,safety,test,tests,21,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:203,safety,test,testing,203,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:161,security,access,access,161,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:21,testability,test,tests,21,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:203,testability,test,testing,203,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/issues/229:28,availability,error,error,28,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:95,deployability,observ,observed,95,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:28,performance,error,error,28,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:28,safety,error,error,28,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:95,testability,observ,observed,95,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:28,usability,error,error,28,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:41,usability,document,documentation,41,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:123,usability,interact,interact,123,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:137,usability,prefer,prefer,137,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:85,reliability,doe,doesn,85,"I would suggest to put back `frameon=False` by default. For several scatter plots it doesn't matter to much (eg. tsne, umap, but for some others is not so good e.g :. ```python. sc.pl.scatter(adata, x='n_counts', y='percent_mito'). ```. ![image](https://user-images.githubusercontent.com/4964309/43883398-4d299d76-9bb3-11e8-8594-49127f4f21e9.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:254,usability,user,user-images,254,"I would suggest to put back `frameon=False` by default. For several scatter plots it doesn't matter to much (eg. tsne, umap, but for some others is not so good e.g :. ```python. sc.pl.scatter(adata, x='n_counts', y='percent_mito'). ```. ![image](https://user-images.githubusercontent.com/4964309/43883398-4d299d76-9bb3-11e8-8594-49127f4f21e9.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/229:301,performance,time,time,301,"Yes, I also noticed this already and thought about distinguishing cases where there are informative axes labels and ticks and those where there are not... But you're right, it's cleaner to just set the default to `True` again. I'll do that along with merging your pull request... Just need a bit more time to go through it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229
https://github.com/scverse/scanpy/issues/230:34,integrability,protocol,protocol,34,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:34,interoperability,protocol,protocol,34,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:76,performance,time,timepoint,76,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:118,reliability,doe,does,118,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:103,testability,regress,regressing,103,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:36,deployability,contain,contain,36,can you check that adata.X does not contain columns or rows with only zeros? ```python. print(np.any(adata.X.sum(axis=0) == 0)). print(np.any(adata.X.sum(axis=1) == 0)). ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:27,reliability,doe,does,27,can you check that adata.X does not contain columns or rows with only zeros? ```python. print(np.any(adata.X.sum(axis=0) == 0)). print(np.any(adata.X.sum(axis=1) == 0)). ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:271,performance,time,timepoint,271,"Hello Fidelram, . Here's the output. Looks like a I have column(s) with zeros. Any suggestions for a remedy and/or possible explanation for why this occurs after removing certain indices from the anndata structure? I'm almost done processing each of my datasets for each timepoint and it hasn't been a problem except for one. ```py. >>> print(np.any(adata.X.sum(axis=0) == 0)). True. >>> print(np.any(adata.X.sum(axis=1) == 0)). False. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:101,safety,reme,remedy,101,"Hello Fidelram, . Here's the output. Looks like a I have column(s) with zeros. Any suggestions for a remedy and/or possible explanation for why this occurs after removing certain indices from the anndata structure? I'm almost done processing each of my datasets for each timepoint and it hasn't been a problem except for one. ```py. >>> print(np.any(adata.X.sum(axis=0) == 0)). True. >>> print(np.any(adata.X.sum(axis=1) == 0)). False. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:310,safety,except,except,310,"Hello Fidelram, . Here's the output. Looks like a I have column(s) with zeros. Any suggestions for a remedy and/or possible explanation for why this occurs after removing certain indices from the anndata structure? I'm almost done processing each of my datasets for each timepoint and it hasn't been a problem except for one. ```py. >>> print(np.any(adata.X.sum(axis=0) == 0)). True. >>> print(np.any(adata.X.sum(axis=1) == 0)). False. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:592,performance,time,timepoint,592,"You can do:. ```. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. To remove the problematic genes. Probably after removing some cells, some genes no longer had any value in. the matrix. Let me know if this helps with your problem. On Fri, Aug 10, 2018 at 2:33 AM jayypaul <notifications@github.com> wrote:. > Hello Fidelram,. >. > Here's the output. Looks like a I have column(s) with zeros. Any. > suggestions for a remedy and/or possible explanation for why this occurs. > after removing certain indices from the anndata structure? I'm almost done. > processing each of my datasets for each timepoint and it hasn't been a. > problem except for one. >. > print(np.any(adata.X.sum(axis=0) == 0)). > True. > print(np.any(adata.X.sum(axis=1) == 0)). > False. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-411939560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fnAqNW3l-t4w865sLwW6-_2zPU4ks5uPNTDgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:416,safety,reme,remedy,416,"You can do:. ```. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. To remove the problematic genes. Probably after removing some cells, some genes no longer had any value in. the matrix. Let me know if this helps with your problem. On Fri, Aug 10, 2018 at 2:33 AM jayypaul <notifications@github.com> wrote:. > Hello Fidelram,. >. > Here's the output. Looks like a I have column(s) with zeros. Any. > suggestions for a remedy and/or possible explanation for why this occurs. > after removing certain indices from the anndata structure? I'm almost done. > processing each of my datasets for each timepoint and it hasn't been a. > problem except for one. >. > print(np.any(adata.X.sum(axis=0) == 0)). > True. > print(np.any(adata.X.sum(axis=1) == 0)). > False. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-411939560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fnAqNW3l-t4w865sLwW6-_2zPU4ks5uPNTDgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:634,safety,except,except,634,"You can do:. ```. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. To remove the problematic genes. Probably after removing some cells, some genes no longer had any value in. the matrix. Let me know if this helps with your problem. On Fri, Aug 10, 2018 at 2:33 AM jayypaul <notifications@github.com> wrote:. > Hello Fidelram,. >. > Here's the output. Looks like a I have column(s) with zeros. Any. > suggestions for a remedy and/or possible explanation for why this occurs. > after removing certain indices from the anndata structure? I'm almost done. > processing each of my datasets for each timepoint and it hasn't been a. > problem except for one. >. > print(np.any(adata.X.sum(axis=0) == 0)). > True. > print(np.any(adata.X.sum(axis=1) == 0)). > False. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-411939560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fnAqNW3l-t4w865sLwW6-_2zPU4ks5uPNTDgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1008,security,auth,auth,1008,"You can do:. ```. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. To remove the problematic genes. Probably after removing some cells, some genes no longer had any value in. the matrix. Let me know if this helps with your problem. On Fri, Aug 10, 2018 at 2:33 AM jayypaul <notifications@github.com> wrote:. > Hello Fidelram,. >. > Here's the output. Looks like a I have column(s) with zeros. Any. > suggestions for a remedy and/or possible explanation for why this occurs. > after removing certain indices from the anndata structure? I'm almost done. > processing each of my datasets for each timepoint and it hasn't been a. > problem except for one. >. > print(np.any(adata.X.sum(axis=0) == 0)). > True. > print(np.any(adata.X.sum(axis=1) == 0)). > False. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-411939560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fnAqNW3l-t4w865sLwW6-_2zPU4ks5uPNTDgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:205,usability,help,helps,205,"You can do:. ```. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. To remove the problematic genes. Probably after removing some cells, some genes no longer had any value in. the matrix. Let me know if this helps with your problem. On Fri, Aug 10, 2018 at 2:33 AM jayypaul <notifications@github.com> wrote:. > Hello Fidelram,. >. > Here's the output. Looks like a I have column(s) with zeros. Any. > suggestions for a remedy and/or possible explanation for why this occurs. > after removing certain indices from the anndata structure? I'm almost done. > processing each of my datasets for each timepoint and it hasn't been a. > problem except for one. >. > print(np.any(adata.X.sum(axis=0) == 0)). > True. > print(np.any(adata.X.sum(axis=1) == 0)). > False. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-411939560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fnAqNW3l-t4w865sLwW6-_2zPU4ks5uPNTDgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:178,deployability,modul,module,178,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:230,deployability,Version,Versions,230,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:393,deployability,Version,Versions,393,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:581,deployability,Version,Versions,581,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:741,deployability,Version,Versions,741,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:918,deployability,Version,Versions,918,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1109,deployability,Version,Versions,1109," Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1291,deployability,Version,Versions,1291,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:966,energy efficiency,core,core,966,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1157,energy efficiency,core,core,1157,"din>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1339,energy efficiency,core,core,1339,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1744,energy efficiency,load,loading,1744,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:230,integrability,Version,Versions,230,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:393,integrability,Version,Versions,393,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:581,integrability,Version,Versions,581,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:741,integrability,Version,Versions,741,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:918,integrability,Version,Versions,918,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1109,integrability,Version,Versions,1109," Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1291,integrability,Version,Versions,1291,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:178,modifiability,modul,module,178,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:230,modifiability,Version,Versions,230,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:262,modifiability,pac,packages,262,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:393,modifiability,Version,Versions,393,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:425,modifiability,pac,packages,425,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:581,modifiability,Version,Versions,581,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:613,modifiability,pac,packages,613,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:741,modifiability,Version,Versions,741,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:773,modifiability,pac,packages,773,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:918,modifiability,Version,Versions,918,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:950,modifiability,pac,packages,950,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1109,modifiability,Version,Versions,1109," Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1141,modifiability,pac,packages,1141,"t):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1291,modifiability,Version,Versions,1291,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1323,modifiability,pac,packages,1323,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1744,performance,load,loading,1744,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:178,safety,modul,module,178,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:114,testability,Trace,Traceback,114,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1816,testability,regress,regress,1816,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2101,testability,regress,regression,2101,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:15,usability,command,command,15,"I've tried the command ... ```py. adata = adata[:,adata.X.sum(axis=0) > 0]. ```. .. and it kicks back: . ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1530,usability,command,command,1530,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2255,usability,feedback,feedback,2255,"ckages/anndata/base.py"", line 1205, in __getitem__. return self._getitem_view(index). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__. self._init_as_view(X, oidx, vidx). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view. var_sub = adata_ref.var.iloc[vidx_normalized]. File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__. return self._getitem_axis(maybe_callable, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis. return self._getbool_axis(key, axis=axis). File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis. inds, = key.nonzero(). ValueError: too many values to unpack (expected 1). ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True. print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py. keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]. adata = adata[keep_cells, :]. ```. or . ```py. adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells . ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion? Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:462,deployability,Version,Versions,462,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:633,deployability,Version,Versions,633,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:831,deployability,Version,Versions,831,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:999,deployability,Version,Versions,999,"ually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1186,deployability,Version,Versions,1186," at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1385,deployability,Version,Versions,1385,". > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1577,deployability,Version,Versions,1577,"x). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are rece",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1234,energy efficiency,core,core,1234,"> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1433,energy efficiency,core,core,1433,"rameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1625,energy efficiency,core,core,1625,"ework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2044,energy efficiency,load,loading,2044,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:462,integrability,Version,Versions,462,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:633,integrability,Version,Versions,633,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:831,integrability,Version,Versions,831,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:999,integrability,Version,Versions,999,"ually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1186,integrability,Version,Versions,1186," at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1385,integrability,Version,Versions,1385,". > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1577,integrability,Version,Versions,1577,"x). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are rece",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:462,modifiability,Version,Versions,462,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:494,modifiability,pac,packages,494,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:633,modifiability,Version,Versions,633,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:665,modifiability,pac,packages,665,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:831,modifiability,Version,Versions,831,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:863,modifiability,pac,packages,863,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:999,modifiability,Version,Versions,999,"ually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1031,modifiability,pac,packages,1031,"day and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > rig",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1186,modifiability,Version,Versions,1186," at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1218,modifiability,pac,packages,1218,"ons@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1385,modifiability,Version,Versions,1385,". > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1417,modifiability,pac,packages,1417," > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, onc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1577,modifiability,Version,Versions,1577,"x). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are rece",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1609,modifiability,pac,packages,1609,"ks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2044,performance,load,loading,2044,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2811,security,auth,auth,2811,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:355,testability,Trace,Traceback,355,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2119,testability,regress,regress,2119,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2387,testability,regress,regression,2387,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:263,usability,command,command,263,"actually, I had the same problem today and solved the problem by removing. zero value columns. Maybe you want to try:. ```. sc.pp.filter_genes(adata, min_counts=3). ```. On Fri, Aug 10, 2018 at 4:25 PM jayypaul <notifications@github.com> wrote:. > I've tried the command ... >. > adata = adata[:,adata.X.sum(axis=0) > 0]. >. > .. and it kicks back:. >. > Traceback (most recent call last):. > File """", line 1, in. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1205, in *getitem*. > return self._getitem_view(index). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 1209, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:1823,usability,command,command,1823,"ork/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:2549,usability,feedback,feedback,2549,"ta/base.py"",. > line 635, in *init*. > self._init_as_view(X, oidx, vidx). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",. > line 661, in _init_as_view. > var_sub = adata_ref.var.iloc[vidx_normalized]. > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1478, in *getitem*. > return self._getitem_axis(maybe_callable, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 2087, in _getitem_axis. > return self._getbool_axis(key, axis=axis). > File. > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",. > line 1494, in _getbool_axis. > inds, = key.nonzero(). > ValueError: too many values to unpack (expected 1). >. > I've tried several variations of this yet I don't see why your command. > wouldn't work, it seems like it should do what you intend .. >. > Note however, I ran :. >. > print(np.any(adata.X.sum(axis=0) == 0)) # True. > print(np.any(adata.X.sum(axis=1) == 0)) # False. >. > right after loading the dataset and it still shows True and False, yet if. > I were to regress out WITHOUT removing cell types via:. >. > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]. > adata = adata[Temp,:]. >. > or. >. > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells. >. > ... the regression will work. However, once I remove, it won't. I could. > try to remove the 0 columns with R, unless you have another suggestion? >. > Thank you for any feedback. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:119,energy efficiency,load,loaded,119,"Thanks fellas, it worked. . ```py. pp.filter_genes(adata, min_counts=1). ``` . Weird, because the other datasets, when loaded fresh have the same pattern:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True . print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:119,performance,load,loaded,119,"Thanks fellas, it worked. . ```py. pp.filter_genes(adata, min_counts=1). ``` . Weird, because the other datasets, when loaded fresh have the same pattern:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True . print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:321,testability,regress,regress,321,"Thanks fellas, it worked. . ```py. pp.filter_genes(adata, min_counts=1). ``` . Weird, because the other datasets, when loaded fresh have the same pattern:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True . print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:352,usability,help,help,352,"Thanks fellas, it worked. . ```py. pp.filter_genes(adata, min_counts=1). ``` . Weird, because the other datasets, when loaded fresh have the same pattern:. ```py. print(np.any(adata.X.sum(axis=0) == 0)) # True . print(np.any(adata.X.sum(axis=1) == 0)) # False. ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:93,availability,error,error,93,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:51,integrability,filter,filtering,51,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:120,integrability,filter,filtering,120,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:93,performance,error,error,93,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:93,safety,error,error,93,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:93,usability,error,error,93,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns? ```py. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ``` .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/231:142,integrability,sub,subsetted,142,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:217,modifiability,paramet,parameter,217,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:64,performance,time,times,64,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:128,testability,simpl,simply,128,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:128,usability,simpl,simply,128,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:40,integrability,sub,subsetted,40,For me the difference between passing a subsetted AnnData to using groups would be that when using groups all other cells are plotted but greyed out. I find that usefull sometimes. So I would like to retain the feature.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:226,availability,error,error,226,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:170,deployability,automat,automatically,170,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:232,integrability,messag,message,232,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:232,interoperability,messag,message,232,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:226,performance,error,error,226,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:216,safety,avoid,avoid,216,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:226,safety,error,error,226,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:170,testability,automat,automatically,170,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:226,usability,error,error,226,"umap expects a list as group, so it will work if you do:. ```python. sc.pl.umap(adata, color='blobs', groups=['Zero']). ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:19,deployability,automat,automatic,19,"Thanks, Fidel. The automatic conversion was indeed missing. I've added it here: https://github.com/theislab/scanpy/commit/9ad11aa4911bd3665b27e32b5a822f3c41274b5a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:29,interoperability,convers,conversion,29,"Thanks, Fidel. The automatic conversion was indeed missing. I've added it here: https://github.com/theislab/scanpy/commit/9ad11aa4911bd3665b27e32b5a822f3c41274b5a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:19,testability,automat,automatic,19,"Thanks, Fidel. The automatic conversion was indeed missing. I've added it here: https://github.com/theislab/scanpy/commit/9ad11aa4911bd3665b27e32b5a822f3c41274b5a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/232:212,deployability,observ,observed,212,Scanpy only interprets categorical annotation and string annotation with a small number of categories as categorical data. It definitely doesn't do so for numerical data. Can you give a bit more details? I never observed your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:137,reliability,doe,doesn,137,Scanpy only interprets categorical annotation and string annotation with a small number of categories as categorical data. It definitely doesn't do so for numerical data. Can you give a bit more details? I never observed your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:212,testability,observ,observed,212,Scanpy only interprets categorical annotation and string annotation with a small number of categories as categorical data. It definitely doesn't do so for numerical data. Can you give a bit more details? I never observed your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:23,performance,time,time,23,I'm having a very hard time of understanding what should go wrong here. No clue whatsoever. 🙈,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:31,testability,understand,understanding,31,I'm having a very hard time of understanding what should go wrong here. No clue whatsoever. 🙈,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:6,safety,valid,validate,6,"I can validate this problem. Actually, any value from C0 to C9 causes the problem but each case gets a different color. ![image](https://user-images.githubusercontent.com/4964309/44329411-a82ce700-a464-11e8-940c-c85a67bf579a.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:6,security,validat,validate,6,"I can validate this problem. Actually, any value from C0 to C9 causes the problem but each case gets a different color. ![image](https://user-images.githubusercontent.com/4964309/44329411-a82ce700-a464-11e8-940c-c85a67bf579a.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:137,usability,user,user-images,137,"I can validate this problem. Actually, any value from C0 to C9 causes the problem but each case gets a different color. ![image](https://user-images.githubusercontent.com/4964309/44329411-a82ce700-a464-11e8-940c-c85a67bf579a.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:41,safety,valid,valid,41,"Ok, good! Then this means that 'C0' is a valid `matplotlib` color string. I'll catch this case manually and then it will work! Thanks for the clarification everyone.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/issues/232:48,interoperability,specif,specific,48,"Unfortunately, the fix is incomplete: for those specific genes the name of gene is missing from the plot (no title for the scatter).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232
https://github.com/scverse/scanpy/pull/234:0,energy efficiency,Cool,Cool,0,Cool!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/pull/234:20,reliability,doe,doesn,20,"Right now the image doesn’t float on GitHub, since the two mentioned PRs/issues aren’t merged/fixed. If you’re OK with that we can leave it like that; If not we could separate the README from what’s used in the documentation and PyPI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/pull/234:211,usability,document,documentation,211,"Right now the image doesn’t float on GitHub, since the two mentioned PRs/issues aren’t merged/fixed. If you’re OK with that we can leave it like that; If not we could separate the README from what’s used in the documentation and PyPI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/issues/235:19,reliability,doe,does,19,The default method does work however ...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/issues/235:94,availability,cluster,clustering,94,"Hm, sorry for the late response. I can't reproduce this. Does it work for you in the standard clustering tutorial?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/issues/235:94,deployability,cluster,clustering,94,"Hm, sorry for the late response. I can't reproduce this. Does it work for you in the standard clustering tutorial?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/issues/235:85,interoperability,standard,standard,85,"Hm, sorry for the late response. I can't reproduce this. Does it work for you in the standard clustering tutorial?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/issues/235:57,reliability,Doe,Does,57,"Hm, sorry for the late response. I can't reproduce this. Does it work for you in the standard clustering tutorial?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/pull/236:10,modifiability,layer,layers,10,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing? Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:88,modifiability,layer,layers,88,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing? Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:139,modifiability,layer,layers,139,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing? Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:118,usability,usab,usable,118,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing? Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:120,integrability,transform,transformation,120,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:120,interoperability,transform,transformation,120,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:208,interoperability,stub,stub,208,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:42,modifiability,layer,layers,42,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:283,modifiability,layer,layers,283,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:390,modifiability,pac,package,390,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:176,security,loss,loss,176,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:208,testability,stub,stub,208,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:349,energy efficiency,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:349,integrability,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:349,interoperability,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:130,modifiability,layer,layers,130,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:165,modifiability,layer,layer,165,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:202,modifiability,layer,layers,202,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:285,modifiability,layer,layers,285,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:349,modifiability,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:317,performance,time,timeline,317,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/pull/236:90,usability,tool,tool,90,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236
https://github.com/scverse/scanpy/issues/240:73,availability,cluster,clusters,73,Do you know what are the advantages of using such network in the louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:73,deployability,cluster,clusters,73,Do you know what are the advantages of using such network in the louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:50,performance,network,network,50,Do you know what are the advantages of using such network in the louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:50,security,network,network,50,Do you know what are the advantages of using such network in the louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:102,availability,cluster,clusters,102,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:293,availability,cluster,cluster,293,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:38,deployability,version,version,38,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:102,deployability,cluster,clusters,102,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:293,deployability,cluster,cluster,293,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:38,integrability,version,version,38,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:38,modifiability,version,version,38,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:208,reliability,doe,does,208,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:11,safety,test,test,11,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:87,security,ident,identify,87,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:123,security,ident,identified,123,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:11,testability,test,test,11,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:347,usability,user,user-images,347,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:83,availability,cluster,clustering,83,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:199,availability,cluster,clustering,199,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:60,deployability,version,version,60,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:83,deployability,cluster,clustering,83,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:149,deployability,scale,scale,149,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:199,deployability,cluster,clustering,199,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:149,energy efficiency,scale,scale,149,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:60,integrability,version,version,60,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:60,modifiability,version,version,60,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:149,modifiability,scal,scale,149,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:173,modifiability,paramet,parameter,173,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:149,performance,scale,scale,149,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:241,performance,network,network,241,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:355,performance,perform,perform,355,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:241,security,network,network,241,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:355,usability,perform,perform,355,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:525,availability,cluster,clustering,525,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:525,deployability,cluster,clustering,525,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:366,integrability,sub,subpopulations,366,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1187,interoperability,specif,specify,1187,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:581,modifiability,paramet,parameter,581,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:979,modifiability,pac,package,979,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1066,modifiability,pac,package,1066,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:330,performance,perform,perform,330,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:648,performance,network,network,648,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1267,performance,memor,memory,1267,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:37,safety,compl,complete,37,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:37,security,compl,complete,37,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:348,security,ident,identifying,348,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:648,security,network,network,648,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:685,testability,verif,verifying,685,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:0,usability,Intuit,Intuitively,0,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:330,usability,perform,perform,330,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:737,usability,document,documentation,737,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:995,usability,usab,usability,995,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1103,usability,user,user,1103,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1171,usability,user,user,1171,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1267,usability,memor,memory,1267,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](. https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it? Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](. https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:160,availability,cluster,clustering,160,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:279,availability,cluster,clusters,279,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:356,availability,cluster,cluster,356,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:567,availability,cluster,cluster,567,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:77,deployability,version,version,77,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:160,deployability,cluster,clustering,160,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:279,deployability,cluster,clusters,279,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:325,deployability,version,version,325,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:356,deployability,cluster,cluster,356,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:567,deployability,cluster,cluster,567,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:77,integrability,version,version,77,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:325,integrability,version,version,325,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:537,integrability,event,eventually,537,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:77,modifiability,version,version,77,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:230,modifiability,paramet,parameter,230,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:325,modifiability,version,version,325,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:171,reliability,doe,does,171,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:337,security,ident,identify,337,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:603,security,modif,modifications,603,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:390,usability,user,user-images,390,"@LuckyMD I changed the resolution of the method and I find that the weighted version produces results that are different. In other words, the difference in the clustering does not seem to come from differences in the `resolution` parameter. Here is an example. In both cases, 13 clusters are found; however only the weighted version can identify the small cluster No. 13:. ![image](https://user-images.githubusercontent.com/4964309/44648651-fe100a80-a9e1-11e8-88bf-8e096b0bb350.png). Increasing the resolution of the non-weighted method eventually discerns the small cluster 13. I would vote to add the modifications from @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:302,availability,cluster,cluster,302,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:614,availability,robust,robust,614,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,deployability,version,version,72,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:302,deployability,cluster,cluster,302,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,integrability,version,version,72,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,modifiability,version,version,72,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:234,performance,perform,perform,234,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:259,performance,network,network,259,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:466,performance,network,network,466,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:321,reliability,doe,does,321,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:614,reliability,robust,robust,614,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:614,safety,robust,robust,614,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:683,safety,test,testing,683,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:259,security,network,network,259,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:466,security,network,network,466,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:683,testability,test,testing,683,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:823,testability,verif,verify,823,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:180,usability,interact,interaction,180,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:234,usability,perform,perform,234,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:226,availability,cluster,clusters,226,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:246,availability,cluster,cluster,246,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:278,availability,cluster,cluster,278,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:226,deployability,cluster,clusters,226,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:246,deployability,cluster,cluster,246,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:278,deployability,cluster,cluster,278,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:76,integrability,discover,discovery,76,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:191,integrability,pub,published,191,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:76,interoperability,discover,discovery,76,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:76,usability,discov,discovery,76,"@LuckyMD The data I am using is from the recent single cell paper about the discovery of the Ionocytes [Plasschaert, L. et al. 2018.](https://doi.org/10.1038/s41586-018-0394-6). Based on the published markers I can label some clusters: the small cluster 11 is the Ionocytes and cluster 10 is labeled a PNEC/Brush.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:90,availability,cluster,cluster,90,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:113,availability,cluster,cluster,113,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:192,availability,cluster,cluster,192,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:90,deployability,cluster,cluster,90,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:113,deployability,cluster,cluster,113,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:192,deployability,cluster,cluster,192,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:70,performance,perform,performs,70,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:131,security,ident,identified,131,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:70,usability,perform,performs,70,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition? I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:81,availability,cluster,clusters,81,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:140,availability,cluster,clusters,140,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:168,availability,cluster,clustering,168,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:81,deployability,cluster,clusters,81,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:140,deployability,cluster,clusters,140,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:168,deployability,cluster,clustering,168,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:64,security,ident,identifies,64,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:116,security,ident,identification,116,"@LuckyMD I would say that is the other way around:. * weighted: identifies small clusters better. . * non-weighted: identification of small clusters results in further clustering of larger groups. However, the differences are not extremely large.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:31,availability,cluster,clusters,31,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:140,availability,cluster,clusters,140,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:31,deployability,cluster,clusters,31,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,deployability,version,version,72,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:140,deployability,cluster,clusters,140,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,integrability,version,version,72,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:72,modifiability,version,version,72,"Sorry, I thought you meant the clusters 10 and 11 from the non-weighted version. It would be interesting to see how the labels look for the clusters where the two methods don't agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:250,availability,cluster,cluster,250,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:250,deployability,cluster,cluster,250,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:131,performance,network,network,131,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:263,performance,network,network,263,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:131,security,network,network,131,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:166,security,access,access,166,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:263,security,network,network,263,"So, I'll go ahead and start a pull request? Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:201,deployability,api,api,201,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:193,energy efficiency,current,current,193,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:201,integrability,api,api,201,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:201,interoperability,api,api,201,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:206,usability,Person,Personally,206,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:256,usability,user,user,256,"Oh, any idea when that is? As an alternative to allowing more storage, I'd be happy to be able to get a graph returned and later pass it in as an argument, I'm just not sure that fits with the current api. Personally, I feel like it's ultimately up to the user how much they store in an object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:147,availability,cluster,clustering,147,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:201,availability,state,state,201,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:304,availability,cluster,clustering,304,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:405,availability,cluster,clusters,405,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:446,availability,cluster,clustering,446,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:496,availability,cluster,clustering,496,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:542,availability,cluster,clustering,542,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:635,availability,cluster,clusters,635,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:690,availability,cluster,clustering,690,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:850,availability,cluster,clustering,850,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:894,availability,cluster,clustering,894,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1913,availability,avail,available,1913,"nnected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2803,availability,cluster,clustering,2803," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:147,deployability,cluster,clustering,147,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:304,deployability,cluster,clustering,304,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:405,deployability,cluster,clusters,405,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:446,deployability,cluster,clustering,446,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:496,deployability,cluster,clustering,496,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:542,deployability,cluster,clustering,542,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:635,deployability,cluster,clusters,635,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:690,deployability,cluster,clustering,690,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:850,deployability,cluster,clustering,850,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:894,deployability,cluster,clustering,894,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2294,deployability,manag,managed,2294," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2694,deployability,version,version,2694," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2803,deployability,cluster,clustering,2803," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:975,energy efficiency,CPU,CPU,975,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2294,energy efficiency,manag,managed,2294," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:3022,energy efficiency,current,currently,3022," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:201,integrability,state,state,201,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1357,integrability,sub,subpopulations,1357,"ogy of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2694,integrability,version,version,2694," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:73,interoperability,specif,specific,73,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2694,modifiability,version,version,2694," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:964,performance,memor,memory,964,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:975,performance,CPU,CPU,975,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:979,performance,time,time,979,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1321,performance,perform,perform,1321,": use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2191,performance,time,time,2191," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:658,reliability,doe,doesn,658,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:673,reliability,doe,doesn,673,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1913,reliability,availab,available,1913,"nnected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2174,reliability,doe,doesn,2174," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2350,reliability,pra,practice,2350," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:133,safety,detect,detection,133,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1030,safety,compl,complete,1030,"ssion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1913,safety,avail,available,1913,"nnected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2294,safety,manag,managed,2294," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:133,security,detect,detection,133,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1030,security,compl,complete,1030,"ssion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1339,security,ident,identifying,1339," respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graph",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1913,security,availab,available,1913,"nnected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2259,testability,context,context,2259," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2479,testability,simpl,simple,2479," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2633,testability,simpl,simple,2633," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:644,usability,clear,clearly,644,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:964,usability,memor,memory,964,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:993,usability,Intuit,Intuitively,993,"Hi all,. thanks for the nice discussion! Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:1321,usability,perform,perform,1321,": use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise). > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2369,usability,clear,clearly,2369," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2479,usability,simpl,simple,2479," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2633,usability,simpl,simple,2633," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:2652,usability,user,user,2652," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:25,availability,state,statement,25,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:25,integrability,state,statement,25,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:182,usability,learn,learning,182,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:20,availability,state,statement,20,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:286,deployability,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:20,integrability,state,statement,20,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:286,integrability,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:286,modifiability,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:159,safety,input,input,159,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:286,safety,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:251,testability,simpl,simple,251,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:286,testability,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:159,usability,input,input,159,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:251,usability,simpl,simple,251,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:911,deployability,scale,scales,911,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:268,energy efficiency,optim,optimization,268,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:911,energy efficiency,scale,scales,911,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:446,integrability,transform,transforming,446,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:446,interoperability,transform,transforming,446,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:911,modifiability,scal,scales,911,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:268,performance,optimiz,optimization,268,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:911,performance,scale,scales,911,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:738,reliability,doe,does,738,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:132,safety,input,input,132,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:15,usability,close,close,15,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:132,usability,input,input,132,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:285,usability,learn,learning,285,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:401,usability,learn,learning,401,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:715,usability,learn,learned,715,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:876,usability,learn,learn,876,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/240:935,usability,help,helps,935,"I guess we can close with this. And sorry, I forgot to answer the above:. > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240
https://github.com/scverse/scanpy/issues/242:11,availability,error,error,11,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:11,performance,error,error,11,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:11,safety,error,error,11,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:11,usability,error,error,11,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:195,usability,help,helps,195,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:164,deployability,API,API,164,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:202,deployability,api,api,202,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1797,deployability,modul,module,1797,"');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1897,deployability,version,versions,1897,"',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1929,deployability,Version,Versions,1929,"'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2231,deployability,version,versions,2231,"8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2263,deployability,Version,Versions,2263,"ial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3072,deployability,version,versions,3072," ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3104,deployability,Version,Versions,3104,"gIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3338,deployability,version,versions,3338,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3370,deployability,Version,Versions,3370,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3614,deployability,version,versions,3614,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3646,deployability,Version,Versions,3646,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3918,deployability,version,versions,3918,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3950,deployability,Version,Versions,3950,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:164,integrability,API,API,164,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:202,integrability,api,api,202,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1897,integrability,version,versions,1897,"',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1929,integrability,Version,Versions,1929,"'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2231,integrability,version,versions,2231,"8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2263,integrability,Version,Versions,2263,"ial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3072,integrability,version,versions,3072," ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3104,integrability,Version,Versions,3104,"gIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3338,integrability,version,versions,3338,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3370,integrability,Version,Versions,3370,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3614,integrability,version,versions,3614,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3646,integrability,Version,Versions,3646,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3918,integrability,version,versions,3918,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3950,integrability,Version,Versions,3950,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:164,interoperability,API,API,164,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:202,interoperability,api,api,202,"Try with `useast.ensembl.org`. I am unable to retrieve anything from `asia`. `useast` works, see below. I think this is a problem on ensembl servers, not of scanpy API. ```python. In [1]: import scanpy.api as sc. In [4]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'mmusculus');. In [5]: mito. Out[5]:. Index(['mt-Tf', 'mt-Rnr1', 'mt-Tv', 'mt-Rnr2', 'mt-Tl1', 'mt-Nd1', 'mt-Ti',. 'mt-Tq', 'mt-Tm', 'mt-Nd2', 'mt-Tw', 'mt-Ta', 'mt-Tn', 'mt-Tc', 'mt-Ty',. 'mt-Co1', 'mt-Ts1', 'mt-Td', 'mt-Co2', 'mt-Tk', 'mt-Atp8', 'mt-Atp6',. 'mt-Co3', 'mt-Tg', 'mt-Nd3', 'mt-Tr', 'mt-Nd4l', 'mt-Nd4', 'mt-Th',. 'mt-Ts2', 'mt-Tl2', 'mt-Nd5', 'mt-Nd6', 'mt-Te', 'mt-Cytb', 'mt-Tt',. 'mt-Tp'],. dtype='object', name='symbol'). In [6]: mito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2120,interoperability,xml,xml,2120,"-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1797,modifiability,modul,module,1797,"');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1897,modifiability,version,versions,1897,"',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1929,modifiability,Version,Versions,1929,"'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1961,modifiability,pac,packages,1961,"C', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2231,modifiability,version,versions,2231,"8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2263,modifiability,Version,Versions,2263,"ial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2295,modifiability,pac,packages,2295,"sapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3072,modifiability,version,versions,3072," ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3104,modifiability,Version,Versions,3104,"gIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3136,modifiability,pac,packages,3136,"der=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3338,modifiability,version,versions,3338,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3370,modifiability,Version,Versions,3370,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3402,modifiability,pac,packages,3402,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3614,modifiability,version,versions,3614,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3646,modifiability,Version,Versions,3646,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3678,modifiability,pac,packages,3678,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3918,modifiability,version,versions,3918,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3950,modifiability,Version,Versions,3950,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3982,modifiability,pac,packages,3982,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1771,safety,input,input-,1771,"st.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1797,safety,modul,module,1797,"');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1727,testability,Trace,Traceback,1727,"ito = sc.queries.mitochondrial_genes('useast.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, linete",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1771,usability,input,input-,1771,"st.ensembl.org', 'hsapiens');. In [7]: mito. Out[7]:. Index(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1', 'MT-TI',. 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN', 'MT-TC', 'MT-TY',. 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK', 'MT-ATP8', 'MT-ATP6',. 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR', 'MT-ND4L', 'MT-ND4', 'MT-TH',. 'MT-TS2', 'MT-TL2', 'MT-ND5', 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT',. 'MT-TP'],. dtype='object', name='symbol'). In [8]: mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. You must set the host (e.g. f.host='www.ensembl.org'. ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-8-69981c8665eb> in <module>(). ----> 1 mito = sc.queries.mitochondrial_genes('asia.ensembl.org', 'hsapiens');. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40. 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3591,usability,close,close,3591,"o/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677. --> 678 return _read(filepath_or_buffer, kwds). 679. 680 parser_f.__name__ = name. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438. 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441. 442 if chunksize or iterator:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786. --> 787 self._make_engine(self.engine). 788. 789 def close(self):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707. -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709. 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:21,availability,error,error,21,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:683,deployability,modul,module,683,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3807,deployability,modul,module,3807,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:935,interoperability,xml,xml,935,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:683,modifiability,modul,module,683,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1067,modifiability,pac,packages,1067,"ndrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/loca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1867,modifiability,pac,packages,1867,"ndrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). E",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2092,modifiability,pac,packages,2092," in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2327,modifiability,pac,packages,2327," na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2588,modifiability,pac,packages,2588,"or_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3807,modifiability,modul,module,3807,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:21,performance,error,error,21,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:21,safety,error,error,21,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:657,safety,input,input-,657,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:683,safety,modul,module,683,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3003,safety,test,test,3003,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3807,safety,modul,module,3807,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:159,testability,trace,traceback,159,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:613,testability,Trace,Traceback,613,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3003,testability,test,test,3003,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3582,testability,assert,assertthat,3582,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3637,testability,assert,assertthat,3637,"r/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart). asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart). useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)). assertthat::assert_that(all(useastresult == asiaresult)). ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:21,usability,error,error,21,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:657,usability,input,input-,657,"I still get the same error with `useast`. ```python. sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ```. <details>. <summary>The output and traceback</summary>. ```python. You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . You must set the host (e.g. f.host='www.ensembl.org' . ---------------------------------------------------------------------------. EmptyDataError Traceback (most recent call last). <ipython-input-4-66c3fcd14dab> in <module>(). ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org). 40 . 41 # parsing mitochondrial gene symbols. ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None). 43 res.columns = ['symbol', 'chromosome_name']. 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2283,usability,close,close,2283," skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision). 676 skip_blank_lines=skip_blank_lines). 677 . --> 678 return _read(filepath_or_buffer, kwds). 679 . 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds). 438 . 439 # Create the parser. --> 440 parser = TextFileReader(filepath_or_buffer, **kwds). 441 . 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds). 785 self.options['has_index_names'] = kwds['has_index_names']. 786 . --> 787 self._make_engine(self.engine). 788 . 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine). 1012 def _make_engine(self, engine='c'):. 1013 if engine == 'c':. -> 1014 self._engine = CParserWrapper(self.f, **self.options). 1015 else:. 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds). 1706 kwds['usecols'] = self.usecols. 1707 . -> 1708 self._reader = parsers.TextReader(src, **kwds). 1709 . 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file. ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R. library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""). asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""). useastmart = useMart(biomart=""ensembl"", dataset=""hsapien",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:119,interoperability,registr,registry,119,"Just found this:. ```python. >>> from bioservices import BioMart. >>> mart = BioMart(host=""www.ensembl.org""). >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . >>> mart.host = ""www.ensembl.org"". >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . ```. So I'll go open an issue with the `bioservices` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:230,interoperability,registr,registry,230,"Just found this:. ```python. >>> from bioservices import BioMart. >>> mart = BioMart(host=""www.ensembl.org""). >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . >>> mart.host = ""www.ensembl.org"". >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . ```. So I'll go open an issue with the `bioservices` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:350,modifiability,pac,package,350,"Just found this:. ```python. >>> from bioservices import BioMart. >>> mart = BioMart(host=""www.ensembl.org""). >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . >>> mart.host = ""www.ensembl.org"". >>> mart.registry(). You must set the host (e.g. f.host='www.ensembl.org' . ```. So I'll go open an issue with the `bioservices` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:32,deployability,version,version,32,"Weird behavior, I have the same version `1.5.2` but no issues..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:32,integrability,version,version,32,"Weird behavior, I have the same version `1.5.2` but no issues..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:32,modifiability,version,version,32,"Weird behavior, I have the same version `1.5.2` but no issues..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:6,usability,behavi,behavior,6,"Weird behavior, I have the same version `1.5.2` but no issues..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:405,availability,down,down,405,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:192,deployability,fail,fails,192,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:453,deployability,updat,updating,453,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:231,performance,time,time,231,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:152,reliability,doe,doesn,152,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:192,reliability,fail,fails,192,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:202,reliability,doe,does,202,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:453,safety,updat,updating,453,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:453,security,updat,updating,453,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:160,availability,avail,available,160,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:297,availability,avail,available,297,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:310,availability,Servic,Service,310,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:330,availability,Unavail,Unavailable,330,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:381,availability,servic,service,381,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:409,availability,mainten,maintenance,409,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:421,availability,downtim,downtime,421,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:310,deployability,Servic,Service,310,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:381,deployability,servic,service,381,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:421,deployability,downtim,downtime,421,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:310,integrability,Servic,Service,310,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:381,integrability,servic,service,381,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:310,modifiability,Servic,Service,310,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:381,modifiability,servic,service,381,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:160,reliability,availab,available,160,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:297,reliability,availab,available,297,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:409,reliability,mainten,maintenance,409,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:421,reliability,downtim,downtime,421,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:160,safety,avail,available,160,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:297,safety,avail,available,297,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:160,security,availab,available,160,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:297,security,availab,available,297,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:491,usability,close,close,491,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable. > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,deployability,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:752,deployability,contain,containing,752,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:895,deployability,instal,install,895,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:919,deployability,modul,module,919,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2539,energy efficiency,draw,drawback,2539,"s. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""useast.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). ```. The drawback with [pybiomart](https://github.com/jrderuiter/pybiomart) is it's unclear how actively it's maintained. However if this looks reasonable to you, I can open a PR with this change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,integrability,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:74,integrability,interfac,interfaces,74,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1092,integrability,filter,filters,1092,"which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:74,interoperability,interfac,interfaces,74,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:281,interoperability,specif,specific,281,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1030,interoperability,format,format,1030,"n issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,modifiability,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:74,modifiability,interfac,interfaces,74,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:316,modifiability,Paramet,Parameters,316,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:919,modifiability,modul,module,919,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2640,modifiability,maintain,maintained,2640,"s. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""useast.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). ```. The drawback with [pybiomart](https://github.com/jrderuiter/pybiomart) is it's unclear how actively it's maintained. However if this looks reasonable to you, I can open a PR with this change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,safety,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:696,safety,valid,valid,696,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:841,safety,except,except,841,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:919,safety,modul,module,919,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:2640,safety,maintain,maintained,2640,"s. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""useast.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). ```. The drawback with [pybiomart](https://github.com/jrderuiter/pybiomart) is it's unclear how actively it's maintained. However if this looks reasonable to you, I can open a PR with this change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:763,security,ident,identifiers,763,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,testability,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python. def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):. """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters. ----------. org : {{""hsapiens"", ""mmusculus"", ""drerio""}}. Organism to query. Must be an organism in ensembl biomart. fieldname : `str`, optional (default: ""external_gene_name""). Biomart attribute field to return. Possible values include . ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",. and ""zfin_id_symbol"". host : {{""www.ensembl.org"", ...}}. A valid BioMart host URL. Returns. -------. An `np.array` containing identifiers for mitochondrial genes. """""". try:. from pybiomart import Server. except ImportError:. raise ImportError(. ""You need to install the `pybiomart` module.""). server = Server(host). dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]. .datasets[""{}_gene_ensembl"".format(org)]). res = dataset.query(. attributes=[attrname], . filters={""chromosome_name"": [""MT""]},. use_attr_names=True. ). return res[attrname].values. ```. Running it:. ```python. >>> mitochondrial_genes(""hsapiens""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',. 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object). >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""). array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',. 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',. 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',. 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',. 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:135,deployability,modul,module,135,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:23,energy efficiency,charg,charge,23,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:99,modifiability,paramet,parameter,99,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:135,modifiability,modul,module,135,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:198,modifiability,maintain,maintained,198,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:135,safety,modul,module,135,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:198,safety,maintain,maintained,198,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:165,integrability,interfac,interfacing,165,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:165,interoperability,interfac,interfacing,165,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:165,modifiability,interfac,interfacing,165,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:200,testability,coverag,coverage,200,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:441,testability,simpl,simple,441,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:122,usability,minim,minimalistic,122,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:252,usability,prefer,prefer,252,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:441,usability,simpl,simple,441,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:223,deployability,contain,container,223,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:247,deployability,updat,updated,247,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:296,deployability,contain,container,296,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,deployability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,integrability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,interoperability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,modifiability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,reliability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:247,safety,updat,updated,247,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:247,security,updat,updated,247,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,security,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:395,testability,integr,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:42,usability,tool,tool,42,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:163,deployability,API,API,163,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:187,deployability,version,version,187,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:271,deployability,contain,container,271,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:163,integrability,API,API,163,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:187,integrability,version,version,187,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:163,interoperability,API,API,163,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:187,modifiability,version,version,187,"It's probably not a quick fix, you are right. Just thought that if future ID mapping is considered here, it would be good to add an option. . I could add a python API (I have a barebones version already) and we could add it to `sce` for anyone who is running a local BED container anyway?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:8,deployability,updat,update,8,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1118,deployability,contain,containing,1118,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:222,interoperability,coordinat,coordinates,222,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:280,interoperability,coordinat,coordinates,280,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:306,interoperability,coordinat,coordinates,306,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:447,interoperability,specif,specify,447,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:1177,interoperability,coordinat,coordinates,1177,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:695,modifiability,Paramet,Parameters,695,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:8,safety,updat,update,8,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:941,safety,valid,valid,941,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:8,security,updat,update,8,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:123,testability,simpl,simple,123,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:389,testability,simpl,simple,389,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:123,usability,simpl,simple,123,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:389,usability,simpl,simple,389,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:442,usability,user,user,442,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>. <summary>Here's a doc-string for what I'm thinking:</summary>. ```python. def biomart_annotations(org, attrs, host=""www.ensembl.org""):. """""". Retrieve gene annotations from ensembl biomart. Parameters. ----------. org : `str`. Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",. ""mmusculus"", ""drerio"", etc. attrs : `List[str]`. Attributes to query biomart for. host : `str`, optional (default: ""www.ensembl.org""). A valid BioMart host URL. Alternative values include archive urls (like. ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns. -------. A `pd.DataFrame` containing annotations. Examples. --------. Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(. ""hsapiens"",. [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],. ).set_index(""ensembl_gene_id""). >>> adata.var[annot.columns] = annot. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:33,energy efficiency,cool,cool,33,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:60,integrability,coupl,couple,60,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:60,modifiability,coupl,couple,60,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:67,performance,time,times,67,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:23,reliability,doe,does,23,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:60,testability,coupl,couple,60,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:18,usability,tool,tool,18,"@LuckyMD that BED tool does look cool, and I can think of a couple times it'd would've been really useful. Is it easy to get information beyond gene-ids out of it? For instance, could I give a uniprot id and get transcript locations from ensembl?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:109,integrability,pub,publicly,109,Yes. The only problem with BED is that it’s super large and needs quite some memory to run as well. Having a publicly hosted instance backed by some rich company would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:77,performance,memor,memory,77,Yes. The only problem with BED is that it’s super large and needs quite some memory to run as well. Having a publicly hosted instance backed by some rich company would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:77,usability,memor,memory,77,Yes. The only problem with BED is that it’s super large and needs quite some memory to run as well. Having a publicly hosted instance backed by some rich company would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:146,deployability,build,build,146,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:175,deployability,version,versions,175,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:729,deployability,contain,contains,729,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:352,energy efficiency,idl,idly,352,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:175,integrability,version,versions,175,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:982,integrability,pub,public,982,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:352,interoperability,idl,idly,352,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:175,modifiability,version,versions,175,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:912,modifiability,Reu,Reuters,912,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:285,performance,memor,memory,285,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:433,performance,cach,cache,433,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:525,performance,memor,memory,525,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:285,usability,memor,memory,285,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:525,usability,memor,memory,525,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/pull/244:44,deployability,api,api,44,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:101,deployability,api,api,101,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:44,integrability,api,api,44,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:101,integrability,api,api,101,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:44,interoperability,api,api,44,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:101,interoperability,api,api,101,Could you please add images to [scanpy/docs/api](https://github.com/theislab/scanpy/tree/master/docs/api)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:100,deployability,continu,continue,100,"@flying-sheep Sure, I can add the images as well. But first, I want to know if @falexwolf agrees to continue working on these changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:501,availability,sli,slight,501,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:301,deployability,releas,release,301,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:581,deployability,fail,fail,581,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:471,energy efficiency,Current,Currently,471,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:135,integrability,sub,subsequently,135,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:501,reliability,sli,slight,501,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:581,reliability,fail,fail,581,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:575,safety,test,tests,575,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:181,testability,plan,plan,181,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:575,testability,test,tests,575,"This looks good! I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:. - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. . - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:688,availability,slo,slowly,688,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:710,availability,redund,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:531,deployability,fail,failing,531,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:576,deployability,modul,module,576,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:710,deployability,redundan,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:576,modifiability,modul,module,576,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:531,reliability,fail,failing,531,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:688,reliability,slo,slowly,688,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:710,reliability,redundan,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:100,safety,test,tested,100,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:521,safety,test,tests,521,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:576,safety,modul,module,576,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:710,safety,redund,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:744,safety,test,tests,744,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:100,testability,test,tested,100,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:521,testability,test,tests,521,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:744,testability,test,tests,744,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:26,usability,support,support,26,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:191,availability,redund,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:191,deployability,redundan,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:249,integrability,event,eventually,249,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:191,reliability,redundan,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:191,safety,redund,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:222,safety,test,tests,222,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:222,testability,test,tests,222,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:6,deployability,updat,updated,6,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:204,deployability,updat,update,204,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:6,safety,updat,updated,6,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:204,safety,updat,update,204,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:6,security,updat,updated,6,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:204,security,updat,update,204,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:38,deployability,updat,updated,38,"@falexwolf Can you take a look at the updated gist: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe. Most of the previous and new plots are quite similar. For diffmap I see different results but I suspect that there is a bug in the previous code. Also, I don't have any example with arrows. Do you have any?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:38,safety,updat,updated,38,"@falexwolf Can you take a look at the updated gist: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe. Most of the previous and new plots are quite similar. For diffmap I see different results but I suspect that there is a bug in the previous code. Also, I don't have any example with arrows. Do you have any?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:38,security,updat,updated,38,"@falexwolf Can you take a look at the updated gist: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe. Most of the previous and new plots are quite similar. For diffmap I see different results but I suspect that there is a bug in the previous code. Also, I don't have any example with arrows. Do you have any?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:213,deployability,updat,updating,213,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:101,energy efficiency,Current,Currently,101,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:181,energy efficiency,optim,optimal,181,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:213,safety,updat,updating,213,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:226,safety,test,test,226,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:213,security,updat,updating,213,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:226,testability,test,test,226,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:30,usability,efficien,efficient,30,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```. adata[:, 'gene_name'].X. ```. This is easy but not optimal. Any idea? I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:282,deployability,automat,automatic,282,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:14,energy efficiency,optim,optimal,14,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:207,integrability,transform,transforming,207,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:39,interoperability,specif,specific,39,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:207,interoperability,transform,transforming,207,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:282,testability,automat,automatic,282,"Why is it not optimal, can you be more specific and maybe provide an example? It's supposed to be the canonical way. It should be as fast as it possibly gets. If you have sparse data, it could be worthwhile transforming `.tocsc()` before. I don't know whether I'd want to make this automatic, but one could think about it. If there's something to improve here, this would be a top priority for me and I'd immediately fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:118,availability,redund,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:118,deployability,redundan,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:83,reliability,doe,does,83,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:118,reliability,redundan,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:118,safety,redund,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:146,security,access,access,146,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:66,availability,sli,slicing,66,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:179,availability,slo,slows,179,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:185,availability,down,down,185,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:324,availability,sli,slicing,324,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:30,performance,time,time,30,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:66,reliability,sli,slicing,66,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:179,reliability,slo,slows,179,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:240,reliability,doe,does,240,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:324,reliability,sli,slicing,324,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:280,security,access,accessor,280,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:263,testability,simpl,simple,263,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:263,usability,simpl,simple,263,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:9,safety,test,test,9,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:120,safety,test,test,120,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:9,testability,test,test,9,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:120,testability,test,test,120,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:256,availability,cluster,clustering,256,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:256,deployability,cluster,clustering,256,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:11,safety,test,test,11,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:122,safety,test,test,122,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:130,safety,compl,completely,130,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:377,safety,test,tests,377,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:481,safety,test,tests,481,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:130,security,compl,completely,130,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:11,testability,test,test,11,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:122,testability,test,test,122,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:187,testability,simpl,simply,187,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:377,testability,test,tests,377,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:481,testability,test,tests,481,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:187,usability,simpl,simply,187,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:48,interoperability,coordinat,coordinates,48,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:134,interoperability,coordinat,coordinates,134,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:20,safety,test,test,20,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:200,safety,test,test,200,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:20,testability,test,test,20,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:200,testability,test,test,200,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:199,security,auth,authored,199,"If you want, we can merge now. On Wed, Sep 26, 2018 at 2:52 AM Alex Wolf <notifications@github.com> wrote:. > Sure, makes sense! Keep it in this case. 🙂. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424548766>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1U60F_oXSH_N6GfAzyzfIcdxgvYwks5ues_RgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:414,security,auth,auth,414,"If you want, we can merge now. On Wed, Sep 26, 2018 at 2:52 AM Alex Wolf <notifications@github.com> wrote:. > Sure, makes sense! Keep it in this case. 🙂. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424548766>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1U60F_oXSH_N6GfAzyzfIcdxgvYwks5ues_RgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:46,integrability,coupl,couple,46,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:46,modifiability,coupl,couple,46,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:64,safety,test,test,64,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:161,safety,test,tests,161,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:46,testability,coupl,couple,46,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:64,testability,test,test,64,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:161,testability,test,tests,161,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:322,energy efficiency,Current,Currently,322,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:617,integrability,coupl,couple,617,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:617,modifiability,coupl,couple,617,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:635,safety,test,test,635,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:738,safety,test,tests,738,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:793,security,auth,authored,793,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:1008,security,auth,auth,1008,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:617,testability,coupl,couple,617,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:635,testability,test,test,635,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:738,testability,test,tests,738,"No problem. Maybe something we should consider comes from my attempt to use. plotly with the scatter functions output (probably for bokeh is similar). Plotly has a function to convert a matplotlib fig object to plotly. However, for this to work the figure object (the one returned by. pyplot.figure()) has to be returned. Currently, only the axes object are. returned. Thus, we should consider returning the fig object instead of the. axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it. > out myself, I might also push little changes to it. I'm super happy to. > merge after these tests. 😄. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:569,energy efficiency,predict,prediction,569,"All of this is really nice! :smile:. Regarding your comment on plotly: returning a fig instead of an ax would follow a different convention as in seaborn; which I'd try to mimic as closely as possible. But for the scatter plots, which are the only ones that would profit a lot from interactive exploration, one could think about breaking this convention. Regarding bugs for now: One bug I could quickly fix myself, two further bugs that I've stumbled across in the past few hours and couldn't fix right away: If I'm calling. ```. sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). ```. the color maps are ignored. And if I'm calling. ```. sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. ). ```. the first plot has a different shape than the second.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:569,safety,predict,prediction,569,"All of this is really nice! :smile:. Regarding your comment on plotly: returning a fig instead of an ax would follow a different convention as in seaborn; which I'd try to mimic as closely as possible. But for the scatter plots, which are the only ones that would profit a lot from interactive exploration, one could think about breaking this convention. Regarding bugs for now: One bug I could quickly fix myself, two further bugs that I've stumbled across in the past few hours and couldn't fix right away: If I'm calling. ```. sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). ```. the color maps are ignored. And if I'm calling. ```. sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. ). ```. the first plot has a different shape than the second.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:181,usability,close,closely,181,"All of this is really nice! :smile:. Regarding your comment on plotly: returning a fig instead of an ax would follow a different convention as in seaborn; which I'd try to mimic as closely as possible. But for the scatter plots, which are the only ones that would profit a lot from interactive exploration, one could think about breaking this convention. Regarding bugs for now: One bug I could quickly fix myself, two further bugs that I've stumbled across in the past few hours and couldn't fix right away: If I'm calling. ```. sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). ```. the color maps are ignored. And if I'm calling. ```. sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. ). ```. the first plot has a different shape than the second.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:282,usability,interact,interactive,282,"All of this is really nice! :smile:. Regarding your comment on plotly: returning a fig instead of an ax would follow a different convention as in seaborn; which I'd try to mimic as closely as possible. But for the scatter plots, which are the only ones that would profit a lot from interactive exploration, one could think about breaking this convention. Regarding bugs for now: One bug I could quickly fix myself, two further bugs that I've stumbled across in the past few hours and couldn't fix right away: If I'm calling. ```. sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). ```. the color maps are ignored. And if I'm calling. ```. sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. ). ```. the first plot has a different shape than the second.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:797,energy efficiency,predict,prediction,797,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:797,safety,predict,prediction,797,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:1108,security,auth,authored,1108,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:1323,security,auth,auth,1323,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:392,usability,close,closely,392,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:496,usability,interact,interactive,496,"I will get that fixed soon. I don't know why the palette assignment is not. working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄. >. > Regarding your comment on plotly: returning a fig instead of an ax would. > follow a different convention as in seaborn; which I'd try to mimic as. > closely as possible. But for the scatter plots, which are the only ones. > that would profit a lot from interactive exploration, one could think about. > breaking this convention. >. > Regarding bugs for now: One bug I could quickly fix myself, two further. > bugs that I've stumbled across in the past few hours and couldn't fix right. > away: If I'm calling. >. > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]). >. > the color maps are ignored. And if I'm calling. >. > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'. > ). >. > the first plot has a different shape than the second. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:643,deployability,updat,update,643,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:527,safety,avoid,avoid,527,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:574,safety,test,tests,574,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:643,safety,updat,update,643,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:466,security,modif,modify,466,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:643,security,updat,update,643,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:574,testability,test,tests,574,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:217,usability,user,user-images,217,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:355,usability,user,user-images,355,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:110,deployability,continu,continuous,110,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:717,deployability,scale,scales,717,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:717,energy efficiency,scale,scales,717,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:423,modifiability,pac,packages,423,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:717,modifiability,scal,scales,717,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:717,performance,scale,scales,717,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:324,testability,simpl,simply,324,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:170,usability,user,user-images,170,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:324,usability,simpl,simply,324,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:1046,usability,user,user-images,1046,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... . ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:241,deployability,continu,continues,241,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:. * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image. * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:452,energy efficiency,reduc,reduce,452,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:. * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image. * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:598,interoperability,standard,standard,598,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:. * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image. * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:40,usability,close,closer,40,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:. * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image. * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:194,energy efficiency,draw,drawing,194,"I'd definitely say that `rcParams['figure.figsize']` should be respected. Nonetheless, the width of a panel could be `1.2 * rcParams['figure.figsize']` or something like this so that the actual drawing region becomes a square if someone passes a square `figsize` (e.g. `(4, 4)`). This is how I wrote it initially. But as I wanted this to hold _exactly_, I couldn't use `matplotlib.GridSpec` and then I produced all this mess... So, having something that resembles a square a bit more, would be pretty, I think; but one should not get into writing messy custom code again... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:553,usability,custom,custom,553,"I'd definitely say that `rcParams['figure.figsize']` should be respected. Nonetheless, the width of a panel could be `1.2 * rcParams['figure.figsize']` or something like this so that the actual drawing region becomes a square if someone passes a square `figsize` (e.g. `(4, 4)`). This is how I wrote it initially. But as I wanted this to hold _exactly_, I couldn't use `matplotlib.GridSpec` and then I produced all this mess... So, having something that resembles a square a bit more, would be pretty, I think; but one should not get into writing messy custom code again... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:40,deployability,instal,installed,40,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:134,deployability,fail,failing,134,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:148,deployability,updat,updating,148,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:171,deployability,version,version,171,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:182,deployability,updat,update,182,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:171,integrability,version,version,171,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:171,modifiability,version,version,171,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:134,reliability,fail,failing,134,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:93,safety,test,tests,93,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:148,safety,updat,updating,148,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:182,safety,updat,update,182,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:192,safety,test,tests,192,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:148,security,updat,updating,148,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:182,security,updat,update,182,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:93,testability,test,tests,93,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:192,testability,test,tests,192,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:81,availability,toler,tolerance,81,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:361,modifiability,paramet,parameters,361,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:81,reliability,toleran,tolerance,81,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:133,safety,test,tests,133,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,safety,test,test,253,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:292,safety,test,tests,292,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:406,safety,test,tests,406,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:381,security,modif,modified,381,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:133,testability,test,tests,133,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,testability,test,test,253,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:292,testability,test,tests,292,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:406,testability,test,tests,406,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:266,deployability,scale,scale,266,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:266,energy efficiency,scale,scale,266,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:494,energy efficiency,reduc,reduce,494,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:515,energy efficiency,load,load,515,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:54,integrability,coupl,couple,54,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:152,interoperability,share,shared,152,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:408,interoperability,coordinat,coordinates,408,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:454,interoperability,coordinat,coordinates,454,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:54,modifiability,coupl,couple,54,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:266,modifiability,scal,scale,266,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:266,performance,scale,scale,266,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:515,performance,load,load,515,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:54,testability,coupl,couple,54,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on? * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:659,availability,slo,slow,659,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:83,deployability,scale,scale,83,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,deployability,scale,scale,253,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:83,energy efficiency,scale,scale,83,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,energy efficiency,scale,scale,253,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:567,energy efficiency,optim,optimized,567,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:307,interoperability,coordinat,coordinates,307,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:83,modifiability,scal,scale,83,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,modifiability,scal,scale,253,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:83,performance,scale,scale,83,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:253,performance,scale,scale,253,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:567,performance,optimiz,optimized,567,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:632,reliability,doe,does,632,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:659,reliability,slo,slow,659,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:171,usability,behavi,behaviour,171,"@ivirshup:. * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative. * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:311,availability,cluster,clusters,311,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:338,availability,slo,slow,338,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:311,deployability,cluster,clusters,311,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:410,deployability,updat,update,410,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:14,energy efficiency,Cool,Cool,14,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:610,energy efficiency,optim,optimization,610,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:544,modifiability,exten,extents,544,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:488,performance,time,time,488,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:610,performance,optimiz,optimization,610,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:338,reliability,slo,slow,338,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:410,safety,updat,update,410,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:410,security,updat,update,410,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:248,usability,experien,experience,248,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊. * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:77,energy efficiency,optim,optimized,77,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. “A Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.”* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:358,energy efficiency,Current,Currently,358,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. “A Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.”* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:77,performance,optimiz,optimized,77,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. “A Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.”* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/245:144,safety,test,tests,144,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:169,safety,test,test,169,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:195,safety,compl,completely,195,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:195,security,compl,completely,195,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:144,testability,test,tests,144,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:169,testability,test,test,169,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:190,safety,test,tests,190,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:233,safety,test,test,233,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:190,testability,test,tests,190,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:233,testability,test,test,233,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:181,usability,cursor,cursory,181,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:441,availability,cluster,clustering,441,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:441,deployability,cluster,clustering,441,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:432,interoperability,standard,standard,432,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:469,reliability,doe,doesn,469,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:382,safety,test,test,382,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:382,testability,test,test,382,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/issues/246:7,deployability,instal,installed,7,I also installed DCA and tensorflow in the meantime... Maybe it has to do with different backend functions being used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:118,deployability,log,log,118,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:44,modifiability,paramet,parameter,44,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:118,safety,log,log,118,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:118,security,log,log,118,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:118,testability,log,log,118,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/247:33,deployability,version,version,33,"Hi,. This happens because of the version of loompy, that is older than 2.0.10. It should work with 2.0.10.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:33,integrability,version,version,33,"Hi,. This happens because of the version of loompy, that is older than 2.0.10. It should work with 2.0.10.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:33,modifiability,version,version,33,"Hi,. This happens because of the version of loompy, that is older than 2.0.10. It should work with 2.0.10.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:67,availability,error,error,67,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:2,deployability,upgrad,upgraded,2,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:2,modifiability,upgrad,upgraded,2,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:67,performance,error,error,67,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:67,safety,error,error,67,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:67,usability,error,error,67,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:120,usability,user,user-images,120,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:261,usability,user,user-images,261,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:210,deployability,releas,releases,210,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:335,modifiability,layer,layers,335,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/pull/248:280,availability,cluster,clustering,280,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:806,availability,cluster,clustering,806,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:280,deployability,cluster,clustering,280,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:806,deployability,cluster,clustering,806,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1068,energy efficiency,current,current,1068,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:572,integrability,Event,Eventually,572,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:789,interoperability,standard,standard,789,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:892,interoperability,standard,standard,892,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1111,interoperability,prox,proxies,1111,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1750,interoperability,specif,specify,1750,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:672,modifiability,exten,extending,672,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1546,modifiability,pac,package,1546,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1736,modifiability,paramet,parameter,1736,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:837,performance,network,network,837,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1000,performance,network,networks,1000,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1023,performance,network,network,1023,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1259,performance,network,network,1259,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1346,performance,network,network,1346,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1359,performance,network,network,1359,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:2,safety,Test,Tests,2,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:42,safety,test,tests,42,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:113,safety,test,tested,113,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:213,safety,test,test,213,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:480,safety,test,test,480,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:489,safety,test,tests,489,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:625,safety,test,tested,625,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1414,safety,compl,completely,1414,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:837,security,network,network,837,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1000,security,network,networks,1000,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1023,security,network,network,1023,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1259,security,network,network,1259,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1346,security,network,network,1346,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1359,security,network,network,1359,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1414,security,compl,completely,1414,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:2,testability,Test,Tests,2,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:42,testability,test,tests,42,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:113,testability,test,tested,113,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:213,testability,test,test,213,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:480,testability,test,test,480,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:489,testability,test,tests,489,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:625,testability,test,tested,625,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:901,usability,user,user,901,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1053,usability,interact,interact,1053,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1311,usability,interact,interact,1311,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1391,usability,interact,interactions,1391,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:719,interoperability,specif,specifying,719,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:3,safety,Test,Tests,3,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:520,safety,test,tests,520,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:305,security,hack,hack,305,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:3,testability,Test,Tests,3,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:520,testability,test,tests,520,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:333,usability,person,personal,333,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests? ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:438,deployability,version,version,438,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:438,integrability,version,version,438,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:438,modifiability,version,version,438,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:9,safety,test,tests,9,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:47,safety,test,tests,47,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:112,safety,test,tests,112,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:9,testability,test,tests,9,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:47,testability,test,tests,47,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:112,testability,test,tests,112,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:160,availability,failur,failure,160,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:225,availability,error,error,225,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:325,availability,error,error,325,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:561,availability,error,error,561,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:160,deployability,fail,failure,160,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:409,deployability,fail,fails,409,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:442,energy efficiency,current,current,442,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:567,integrability,messag,message,567,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:567,interoperability,messag,message,567,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:160,performance,failur,failure,160,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:225,performance,error,error,225,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:325,performance,error,error,325,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:561,performance,error,error,561,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:160,reliability,fail,failure,160,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:409,reliability,fail,fails,409,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:76,safety,test,tests,76,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:225,safety,error,error,225,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,safety,test,tests,274,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:325,safety,error,error,325,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:383,safety,test,tests,383,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:419,safety,test,tests,419,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:561,safety,error,error,561,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:76,testability,test,tests,76,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,testability,test,tests,274,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:383,testability,test,tests,383,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:395,testability,assert,assert,395,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:419,testability,test,tests,419,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:225,usability,error,error,225,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:325,usability,error,error,325,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:491,usability,interact,interactively,491,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:553,usability,help,helpful,553,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:561,usability,error,error,561,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:99,availability,state,state,99,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,availability,error,error,274,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:99,integrability,state,state,99,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:171,integrability,messag,message,171,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:171,interoperability,messag,message,171,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:114,modifiability,maintain,maintained,114,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,performance,error,error,274,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:114,safety,maintain,maintained,114,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,safety,error,error,274,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:330,safety,except,exception,330,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:274,usability,error,error,274,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```. libc++abi.dylib: terminating with uncaught exception of type char const*. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:463,availability,redund,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:91,deployability,contain,contains,91,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:463,deployability,redundan,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:463,reliability,redundan,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:463,safety,redund,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:297,usability,document,documentation,297,"Yeah, I think I messed up a rebase by having `master` point to my fork instead of this repo. The merge looked fine locally, but I wasn't sure what was going on with the diff here. I did a little more rebase-ing and I think the diff should look much more normal now. One question about numpy style documentation: for the `partition_type` argument, the user should be passing in a class, not an instance. How do I document that? With type annotations I think I would do something like: `Type[louvain.MutableVertexPartition]`, but I'm not sure if that applies here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:351,usability,user,user,351,"Yeah, I think I messed up a rebase by having `master` point to my fork instead of this repo. The merge looked fine locally, but I wasn't sure what was going on with the diff here. I did a little more rebase-ing and I think the diff should look much more normal now. One question about numpy style documentation: for the `partition_type` argument, the user should be passing in a class, not an instance. How do I document that? With type annotations I think I would do something like: `Type[louvain.MutableVertexPartition]`, but I'm not sure if that applies here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:412,usability,document,document,412,"Yeah, I think I messed up a rebase by having `master` point to my fork instead of this repo. The merge looked fine locally, but I wasn't sure what was going on with the diff here. I did a little more rebase-ing and I think the diff should look much more normal now. One question about numpy style documentation: for the `partition_type` argument, the user should be passing in a class, not an instance. How do I document that? With type annotations I think I would do something like: `Type[louvain.MutableVertexPartition]`, but I'm not sure if that applies here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/issues/252:907,availability,error,error,907,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:46,energy efficiency,heat,heatmap,46,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:1485,integrability,sub,subscribed,1485,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:250,modifiability,pac,package,250,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:907,performance,error,error,907,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:0,reliability,doe,does,0,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:913,reliability,doe,doesn,913,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:907,safety,error,error,907,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:1682,security,auth,auth,1682,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:907,usability,error,error,907,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:1205,usability,user,user-images,1205,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:1341,usability,user,user-images,1341,"does the problem also happen in matrixplot or heatmap? seems to me like an. issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>. wrote:. > Thanks for all the work in developing this package, it's truly fantastic. >. > I ran into what seems like a bug in the new plotting function. > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes. > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between. > two groups), then 'Tnf' is only plotted once on the first group, and any. > following groups with the same gene are truncated. You can see this in the. > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'. > should be plotted for each group, but it is only plotted on group M1,. > therefore truncating group M2. When I plot the same data using. > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly. > plotted twice. >. > I know this is a small bug that most people will probably not run across,. > but just in case you're comparing expression across similar groups this. > might be a useful fix. Thanks! >. > [image: stacked_violin_global]. > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>. >. > [image: dotplot_global]. > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:36,deployability,stack,stacked,36,"No, the problem also happens in the stacked violins. Matrix plot and heat map are both fine - you're right it's probably something with seaborn then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:69,energy efficiency,heat,heat,69,"No, the problem also happens in the stacked violins. Matrix plot and heat map are both fine - you're right it's probably something with seaborn then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:156,security,auth,authored,156,"Thanks! On Mon, Sep 3, 2018 at 3:55 AM Fidel Ramirez <notifications@github.com>. wrote:. > I will take a look. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252#issuecomment-418032207>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AjZyyOz23rdOlRGyBCCMqB_uoeSfpdr4ks5uXOB8gaJpZM4WVgcM>. > . >. -- . Andrés Muñoz-Rojas, Ph.D. Postdoctoral Associate. Department of Biomedical Engineering. Yale University.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:373,security,auth,auth,373,"Thanks! On Mon, Sep 3, 2018 at 3:55 AM Fidel Ramirez <notifications@github.com>. wrote:. > I will take a look. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/252#issuecomment-418032207>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AjZyyOz23rdOlRGyBCCMqB_uoeSfpdr4ks5uXOB8gaJpZM4WVgcM>. > . >. -- . Andrés Muñoz-Rojas, Ph.D. Postdoctoral Associate. Department of Biomedical Engineering. Yale University.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:32,availability,error,error,32,I am also running into the same error. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:32,performance,error,error,32,I am also running into the same error. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:32,safety,error,error,32,I am also running into the same error. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/252:32,usability,error,error,32,I am also running into the same error. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/253:36,reliability,doe,does,36,Sorry if not super relevant but how does this look like compared to matplotlib + ipywidgets?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:625,deployability,scale,scale,625,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:880,deployability,api,api,880,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1400,deployability,stack,stack,1400,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:174,energy efficiency,power,powerful,174,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:371,energy efficiency,reduc,reduced,371,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:625,energy efficiency,scale,scale,625,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1276,energy efficiency,Current,Currently,1276,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:880,integrability,api,api,880,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:516,interoperability,share,share,516,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:880,interoperability,api,api,880,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1587,interoperability,Bind,Binder,1587,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:625,modifiability,scal,scale,625,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1587,modifiability,Bind,Binder,1587,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:625,performance,scale,scale,625,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:722,safety,avoid,avoid,722,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:62,usability,interact,interactivity,62,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:409,usability,interact,interactively,409,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:535,usability,visual,visualization,535,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:810,usability,prototyp,prototypes,810,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:898,usability,interact,interact,898,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:75,integrability,topic,topic,75,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:195,integrability,wrap,wrapper,195,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:195,interoperability,wrapper,wrapper,195,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:340,modifiability,pac,package,340,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:84,safety,compl,completely,84,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:84,security,compl,completely,84,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:95,testability,understand,understand,95,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:182,testability,simpl,simple,182,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:182,usability,simpl,simple,182,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:66,integrability,coupl,couple,66,We have also been thinking about this a bit and we came up with a couple of functions in this repo here: https://github.com/theislab/interactive_plotting.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:66,modifiability,coupl,couple,66,We have also been thinking about this a bit and we came up with a couple of functions in this repo here: https://github.com/theislab/interactive_plotting.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:66,testability,coupl,couple,66,We have also been thinking about this a bit and we came up with a couple of functions in this repo here: https://github.com/theislab/interactive_plotting.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/254:40,integrability,compon,components,40,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:131,integrability,compon,components,131,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:182,integrability,compon,components,182,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:40,interoperability,compon,components,40,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:131,interoperability,compon,components,131,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:182,interoperability,compon,components,182,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:40,modifiability,compon,components,40,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:131,modifiability,compon,components,131,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:182,modifiability,compon,components,182,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:19,usability,support,support,19,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:168,usability,command,command,168,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:218,usability,help,helps,218,"Hi,. It seems that support for lists of components has not been added to the function. You should be able to use `sc.pl.pca(adata, components='1,2')` and then a second command with `components='2,3'` though. Hope that helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:71,usability,document,documentation,71,"@falexwolf do you want this functionality, or should I just change the documentation? I guess I would otherwise copy the `sc.pl.diffmap()` code in embeddings.py over to the `sc.pl.pca()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:211,availability,consist,consistently,211,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:85,deployability,modul,module,85,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:27,energy efficiency,current,currently,27,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:126,energy efficiency,current,current,126,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:85,modifiability,modul,module,85,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:37,safety,compl,completely,37,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:85,safety,modul,module,85,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:37,security,compl,completely,37,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:211,usability,consist,consistently,211,"Hey @LuckyMD, @fidelram is currently completely rewriting the whole scatter plotting module. So let's not address this in the current code. But, yes in principle, we should add it so that diffmap and pca behave consistently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:29,reliability,doe,does,29,I just checked and this also does not work in the code that I have. I will check if I can make it work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:49,usability,help,helps,49,"@LuckyMD, thanks for the workaround which indeed helps for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/255:37,usability,help,helpful,37,"Hi,. You may find the tutorials more helpful. You can find them here:. https://scanpy.readthedocs.io/en/latest/examples.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/255
https://github.com/scverse/scanpy/issues/256:150,deployability,log,logging,150,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:173,deployability,log,logging,173,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:317,deployability,pipelin,pipeline,317,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:317,integrability,pipelin,pipeline,317,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:69,interoperability,format,formatting,69,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:193,modifiability,pac,package,193,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:150,safety,log,logging,150,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:173,safety,log,logging,173,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:150,security,log,logging,150,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:173,security,log,logging,173,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:150,testability,log,logging,150,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:173,testability,log,logging,173,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:124,usability,close,closely,124,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:27,reliability,doe,doesn,27,Sounds good! And that code doesn’t look much longer than what we have right now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/257:52,availability,down,down,52,"Sorry, this seems to be a UMAP issue. To really dig down to this, you'd probably run this using the UMAP package and submit a bug report there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/257
https://github.com/scverse/scanpy/issues/257:117,integrability,sub,submit,117,"Sorry, this seems to be a UMAP issue. To really dig down to this, you'd probably run this using the UMAP package and submit a bug report there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/257
https://github.com/scverse/scanpy/issues/257:105,modifiability,pac,package,105,"Sorry, this seems to be a UMAP issue. To really dig down to this, you'd probably run this using the UMAP package and submit a bug report there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/257
https://github.com/scverse/scanpy/issues/262:269,deployability,updat,update,269,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:45,energy efficiency,load,load,45,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:45,performance,load,load,45,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:235,safety,compl,complicated,235,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:269,safety,updat,update,269,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:235,security,compl,complicated,235,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:269,security,updat,update,269,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:634,usability,help,helps,634,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:111,interoperability,standard,standard,111,"Hi, this is very useful. But I think I formulated my question the wrong way. . Can I export the h5ad file to a standard 10X h5 file? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:74,interoperability,format,format,74,"That, I can't help you with I'm afraid. I'm not as familiar with the h5ad format.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:14,usability,help,help,14,"That, I can't help you with I'm afraid. I'm not as familiar with the h5ad format.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:150,availability,down,downstream,150,"Hi @cartal, it wouldn't be very hard to export to a 10x h5 file, but I'd need to write a custom function for it. Why is it needed? Does 10x offer any downstream analysis that you'd want to use on the data? I thought there are none, hence there is only `sc.read_10x_h5` and no `sc.write_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:131,reliability,Doe,Does,131,"Hi @cartal, it wouldn't be very hard to export to a 10x h5 file, but I'd need to write a custom function for it. Why is it needed? Does 10x offer any downstream analysis that you'd want to use on the data? I thought there are none, hence there is only `sc.read_10x_h5` and no `sc.write_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:89,usability,custom,custom,89,"Hi @cartal, it wouldn't be very hard to export to a 10x h5 file, but I'd need to write a custom function for it. Why is it needed? Does 10x offer any downstream analysis that you'd want to use on the data? I thought there are none, hence there is only `sc.read_10x_h5` and no `sc.write_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:185,availability,cluster,cluster,185,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
