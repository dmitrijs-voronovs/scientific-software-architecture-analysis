id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/1:0,safety,TEST,TEST,0,TEST 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/1
https://github.com/google/deepvariant/issues/1:0,testability,TEST,TEST,0,TEST 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/1
https://github.com/google/deepvariant/issues/2:219,deployability,instal,install,219,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:424,deployability,version,versions,424,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:492,deployability,instal,install,492,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:90,energy efficiency,Cloud,Cloud,90,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:111,energy efficiency,Cloud,Cloud,111,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:307,energy efficiency,Cloud,Cloud,307,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:345,energy efficiency,current,current,345,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:424,integrability,version,versions,424,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:117,interoperability,Platform,Platform,117,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/2:424,modifiability,version,versions,424,"Hi Arkanosis,. thanks for your question! It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well. However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues. Thanks! -pichuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2
https://github.com/google/deepvariant/issues/3:212,interoperability,specif,specific,212,"Hi animesh,. Thanks for writing. If you want to use DeepVariant to call variants from reads from a new strain, aligning to your de-novo assembly and applying DeepVariant may work. Unfortunately we don't have any specific recommendations for how you should do that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:38,energy efficiency,optim,optimized,38,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:125,energy efficiency,model,model,125,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:38,performance,optimiz,optimized,38,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:125,security,model,model,125,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:22,energy efficiency,model,model,22,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:84,safety,test,tested,84,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:22,security,model,model,22,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:84,testability,test,tested,84,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:4,usability,experien,experience,4,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:107,energy efficiency,frequenc,frequency,107,"If anyone is interested, I've recently tried DeepVariant on a bacterial dataset. When the alternate allele frequency is high enough, it makes a genotype call like this:. GT:GQ:DP:AD:VAF:PL 1/1:9:13:[1, 10]:0.769231:[9, 19, 0]. When the alternate allele frequency is too low, the genotype call looks like this (heterozygous):. GT:GQ:DP:AD:VAF:PL 0/1:7:8:[6, 2]:0.25:[6, 0, 35]. One possible solution for haploid organisms is to add a post-processing step to filter the calls where the alternate allele frequency is below some threshold.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:253,energy efficiency,frequenc,frequency,253,"If anyone is interested, I've recently tried DeepVariant on a bacterial dataset. When the alternate allele frequency is high enough, it makes a genotype call like this:. GT:GQ:DP:AD:VAF:PL 1/1:9:13:[1, 10]:0.769231:[9, 19, 0]. When the alternate allele frequency is too low, the genotype call looks like this (heterozygous):. GT:GQ:DP:AD:VAF:PL 0/1:7:8:[6, 2]:0.25:[6, 0, 35]. One possible solution for haploid organisms is to add a post-processing step to filter the calls where the alternate allele frequency is below some threshold.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:501,energy efficiency,frequenc,frequency,501,"If anyone is interested, I've recently tried DeepVariant on a bacterial dataset. When the alternate allele frequency is high enough, it makes a genotype call like this:. GT:GQ:DP:AD:VAF:PL 1/1:9:13:[1, 10]:0.769231:[9, 19, 0]. When the alternate allele frequency is too low, the genotype call looks like this (heterozygous):. GT:GQ:DP:AD:VAF:PL 0/1:7:8:[6, 2]:0.25:[6, 0, 35]. One possible solution for haploid organisms is to add a post-processing step to filter the calls where the alternate allele frequency is below some threshold.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/issues/3:457,integrability,filter,filter,457,"If anyone is interested, I've recently tried DeepVariant on a bacterial dataset. When the alternate allele frequency is high enough, it makes a genotype call like this:. GT:GQ:DP:AD:VAF:PL 1/1:9:13:[1, 10]:0.769231:[9, 19, 0]. When the alternate allele frequency is too low, the genotype call looks like this (heterozygous):. GT:GQ:DP:AD:VAF:PL 0/1:7:8:[6, 2]:0.25:[6, 0, 35]. One possible solution for haploid organisms is to add a post-processing step to filter the calls where the alternate allele frequency is below some threshold.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3
https://github.com/google/deepvariant/pull/4:302,deployability,patch,patch,302,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4
https://github.com/google/deepvariant/pull/4:352,deployability,releas,release,352,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4
https://github.com/google/deepvariant/pull/4:302,safety,patch,patch,302,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4
https://github.com/google/deepvariant/pull/4:302,security,patch,patch,302,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4
https://github.com/google/deepvariant/pull/4:60,deployability,releas,release,60,The change is in internally and will appear in the next OSS release. Thanks again!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4
https://github.com/google/deepvariant/issues/5:95,deployability,instal,install,95,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:23,usability,clear,clear,23,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:99,availability,error,error,99,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:193,availability,down,downloads,193,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:148,energy efficiency,Cloud,Cloud,148,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:172,energy efficiency,cloud,cloud,172,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:105,integrability,messag,messages,105,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:105,interoperability,messag,messages,105,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:63,modifiability,pac,package,63,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:99,performance,error,error,99,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:99,safety,error,error,99,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:99,usability,error,error,99,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:73,deployability,Build,Build,73,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:160,deployability,build,build-test,160,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:55,energy efficiency,Cloud,Cloud,55,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:83,safety,Test,Test,83,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:166,safety,test,test,166,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:83,testability,Test,Test,83,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:166,testability,test,test,166,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/5:88,usability,guid,guide,88,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5
https://github.com/google/deepvariant/issues/6:5,energy efficiency,current,currently,5,"Hi,. currently the instructions are written for Ubuntu 16. I've not tried it on CentOS before. If you have access to a Ubuntu 16 machine, can you give it a try and see if you're still seen the same issue? I can also try running on CentOS and report back, but that might take a while for me to do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:107,security,access,access,107,"Hi,. currently the instructions are written for Ubuntu 16. I've not tried it on CentOS before. If you have access to a Ubuntu 16 machine, can you give it a try and see if you're still seen the same issue? I can also try running on CentOS and report back, but that might take a while for me to do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:18,deployability,build,build-prereq,18,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:149,deployability,build,build-test,149,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:10,reliability,Doe,Does,10,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:35,safety,compl,complete,35,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:155,safety,test,test,155,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:35,security,compl,complete,35,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:155,testability,test,test,155,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:20,safety,permiss,permission,20,well，I have no root permission,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:44,deployability,build,build-prereq,44,"huangl07, you will need to successfully run build-prereq.sh first. Please try it on a Ubuntu 16 machine where you have permission. Thanks pgrosu for pointing out that issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:119,safety,permiss,permission,119,"huangl07, you will need to successfully run build-prereq.sh first. Please try it on a Ubuntu 16 machine where you have permission. Thanks pgrosu for pointing out that issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:30,deployability,build,build-prereq,30,"Got the same problem, but run build-prereq.sh is not possible on a CentOS 7 basis !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:169,deployability,build,build-test,169,@ardoli You will need to use Ubuntu 14 or 16 - see the following: . https://github.com/google/deepvariant/blob/59738f0ca91df3757d754e7ce6507f614816fd1c/docs/deepvariant-build-test.md.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:175,safety,test,test,175,@ardoli You will need to use Ubuntu 14 or 16 - see the following: . https://github.com/google/deepvariant/blob/59738f0ca91df3757d754e7ce6507f614816fd1c/docs/deepvariant-build-test.md.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:175,testability,test,test,175,@ardoli You will need to use Ubuntu 14 or 16 - see the following: . https://github.com/google/deepvariant/blob/59738f0ca91df3757d754e7ce6507f614816fd1c/docs/deepvariant-build-test.md.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:240,deployability,instal,installation,240,"Thank's for the answer, I saw it, but all servers in the genomic research center I work are on CentOS-7 and Ubuntu is not an option. So I can't use Ubuntu, and of course, no way to send human genomic data on the cloud. Maybe a more generic installation procedure, not stick on ubuntu, could be a good idea, at least for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:212,energy efficiency,cloud,cloud,212,"Thank's for the answer, I saw it, but all servers in the genomic research center I work are on CentOS-7 and Ubuntu is not an option. So I can't use Ubuntu, and of course, no way to send human genomic data on the cloud. Maybe a more generic installation procedure, not stick on ubuntu, could be a good idea, at least for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:273,deployability,instal,install,273,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:182,interoperability,share,share,182,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:215,performance,time,time,215,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:35,usability,custom,custom,35,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:377,deployability,modul,modular,377,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:385,deployability,pipelin,pipeline,385,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:377,integrability,modular,modular,377,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:385,integrability,pipelin,pipeline,385,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:377,modifiability,modul,modular,377,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:456,performance,network,networks,456,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:377,safety,modul,modular,377,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:135,security,privil,privileges,135,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:456,security,network,networks,456,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:366,testability,simpl,simple,366,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:377,testability,modula,modular,377,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:241,usability,user,user-space,241,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:366,usability,simpl,simple,366,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:424,usability,prefer,preferred,424,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image. * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:38,deployability,instal,installed,38,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:651,deployability,instal,install,651,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:129,interoperability,share,share,129,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:210,interoperability,share,share,210,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:560,interoperability,share,share,560,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:121,performance,time,time,121,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:593,performance,time,time,593,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:996,security,auth,auth,996,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:413,usability,custom,custom,413,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:396,availability,down,downloads,396,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:12,deployability,build,build,12,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:49,deployability,updat,updating,49,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:139,deployability,releas,releases,139,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:172,deployability,releas,release,172,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:281,deployability,contain,container,281,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:383,deployability,build,build,383,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:546,deployability,version,version,546,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:546,integrability,version,version,546,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:546,modifiability,version,version,546,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:554,reliability,doe,does,554,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:49,safety,updat,updating,49,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:49,security,updat,updating,49,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:592,availability,down,downloads,592,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:68,deployability,build,build,68,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:208,deployability,build,build,208,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:245,deployability,updat,updating,245,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:335,deployability,releas,releases,335,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:368,deployability,releas,release,368,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:477,deployability,contain,container,477,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:579,deployability,build,build,579,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:742,deployability,version,version,742,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:742,integrability,version,version,742,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:742,modifiability,version,version,742,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:750,reliability,doe,does,750,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:245,safety,updat,updating,245,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:245,security,updat,updating,245,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:1022,security,auth,auth,1022,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:. https://github.com/ink1/deepvariant/releases/tag/v0.5.2a. The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:0,deployability,Build,Building,0,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:238,integrability,coupl,couple,238,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:75,modifiability,portab,portable,75,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:238,modifiability,coupl,couple,238,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:98,safety,test,test,98,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:176,safety,test,test,176,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:203,safety,compl,complete,203,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:203,security,compl,complete,203,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:64,testability,simpl,simple,64,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:98,testability,test,test,98,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:176,testability,test,test,176,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:238,testability,coupl,couple,238,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:64,usability,simpl,simple,64,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes. Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:53,availability,ERROR,ERROR,53,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:239,availability,ERROR,ERROR,239,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:113,deployability,BUILD,BUILD,113,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:299,deployability,BUILD,BUILD,299,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:383,deployability,contain,container,383,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:486,deployability,build,build-prereq,486,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:363,energy efficiency,Power,Power,363,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:53,performance,ERROR,ERROR,53,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:239,performance,ERROR,ERROR,239,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:53,safety,ERROR,ERROR,53,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:184,safety,input,input,184,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:239,safety,ERROR,ERROR,239,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:312,safety,input,input,312,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:53,usability,ERROR,ERROR,53,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:184,usability,input,input,184,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:239,usability,ERROR,ERROR,239,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:312,usability,input,input,312,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/6:586,usability,help,help,586,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6
https://github.com/google/deepvariant/issues/7:9,deployability,log,log,9,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:133,deployability,fail,fail,133,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:233,deployability,version,version,233,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:90,energy efficiency,Cloud,Cloud,90,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:263,energy efficiency,cloud,cloud,263,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:233,integrability,version,version,233,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:96,interoperability,Platform,Platform,96,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:233,modifiability,version,version,233,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:133,reliability,fail,fail,133,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:9,safety,log,log,9,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:9,security,log,log,9,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:61,security,authenticat,authentication,61,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:9,testability,log,log,9,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:14,deployability,version,version,14,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:143,deployability,build,build,143,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:14,integrability,version,version,14,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:14,modifiability,version,version,14,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:306,modifiability,paramet,parameter,306,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:41,usability,user,user-images,41,"Here is my OS version:. ![image](https://user-images.githubusercontent.com/15261087/33801703-8ccaeb16-dd9e-11e7-9ba8-cca6cc36556f.png). When i build deepvariant，i set the TF_NEED_GCP to 0. But the make_smaple_test and the example of make_samples in ""DeepVariant quick start"" use GCP default, so Is there a parameter to use local deepvariant, not in GCP.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:152,deployability,log,login,152,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:177,deployability,log,login,177,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:152,safety,log,login,152,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:177,safety,log,login,177,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:127,security,auth,auth,127,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:152,security,log,login,152,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:172,security,auth,auth,172,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:177,security,log,login,177,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:152,testability,log,login,152,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:177,testability,log,login,177,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:103,usability,command,commands,103,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:20,availability,error,error,20,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:31,availability,error,error,31,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:151,deployability,instal,install,151,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:20,performance,error,error,20,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:31,performance,error,error,31,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:20,safety,error,error,20,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:31,safety,error,error,31,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:20,usability,error,error,20,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:31,usability,error,error,31,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:223,usability,user,user-images,223,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:335,usability,user,user-images,335,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run . 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. . ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:28,deployability,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:86,deployability,version,version,86,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:28,integrability,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:86,integrability,version,version,86,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:28,modifiability,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:86,modifiability,version,version,86,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:28,safety,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/7:28,testability,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7
https://github.com/google/deepvariant/issues/8:63,availability,avail,availability,63,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:101,availability,avail,available,101,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:150,energy efficiency,cloud,cloud,150,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:63,reliability,availab,availability,63,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:101,reliability,availab,available,101,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:63,safety,avail,availability,63,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:101,safety,avail,available,101,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:63,security,availab,availability,63,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/8:101,security,availab,available,101,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability. ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8
https://github.com/google/deepvariant/issues/9:339,availability,avail,available,339,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:284,deployability,updat,updating,284,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:339,reliability,availab,available,339,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:284,safety,updat,updating,284,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:339,safety,avail,available,339,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:176,security,team,team,176,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:284,security,updat,updating,284,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:339,security,availab,available,339,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:60,energy efficiency,Cloud,Cloud,60,"@chapmanb Awesome!! . Also, will we be forced to use Google Cloud Platform or is there a way to utilize a local server? For example, a local server at a university?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:66,interoperability,Platform,Platform,66,"@chapmanb Awesome!! . Also, will we be forced to use Google Cloud Platform or is there a way to utilize a local server? For example, a local server at a university?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:50,energy efficiency,Cloud,Cloud,50,@MediciPrime There's no requirement to use Google Cloud Platform right now. In what way are you being forced to use GCP today?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:56,interoperability,Platform,Platform,56,@MediciPrime There's no requirement to use Google Cloud Platform right now. In what way are you being forced to use GCP today?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:79,deployability,build,build,79,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:132,deployability,instal,install,132,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:204,deployability,instal,installing,204,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:535,deployability,build,build,535,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:147,energy efficiency,Cloud,Cloud,147,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:222,energy efficiency,Cloud,Cloud,222,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:327,energy efficiency,Cloud,Cloud,327,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:361,energy efficiency,Cloud,Cloud,361,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:463,energy efficiency,Cloud,Cloud,463,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:469,interoperability,Platform,Platform,469,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:35,performance,time,time,35,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:552,safety,compl,complete,552,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:552,security,compl,complete,552,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:503,testability,simpl,simply,503,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:43,usability,help,help,43,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:188,usability,command,command,188,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:503,usability,simpl,simply,503,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:117,availability,down,download,117,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:43,deployability,build,build-test,43,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:191,deployability,build,build,191,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:49,safety,test,test,49,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:49,testability,test,test,49,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:19,deployability,updat,updated,19,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:194,energy efficiency,Cloud,Cloud,194,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:19,safety,updat,updated,19,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:19,security,updat,updated,19,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:43,usability,guid,guide,43,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:145,usability,feedback,feedback,145,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:170,usability,clear,clear,170,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:39,availability,avail,available,39,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:471,availability,avail,available,471,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:91,deployability,instal,install,91,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:117,deployability,instal,install,117,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:708,deployability,instal,installing,708,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:374,energy efficiency,model,models,374,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:182,integrability,wrap,wrapper,182,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:311,integrability,wrap,wrapping,311,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:182,interoperability,wrapper,wrapper,182,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:532,interoperability,standard,standard,532,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:8,modifiability,pac,package,8,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:656,modifiability,portab,portability,656,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:690,modifiability,pac,package,690,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:792,modifiability,pac,package,792,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:39,reliability,availab,available,39,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:471,reliability,availab,available,471,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:39,safety,avail,available,39,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:471,safety,avail,available,471,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:39,security,availab,available,39,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:374,security,model,models,374,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:452,security,expos,expose,452,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:471,security,availab,available,471,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:414,usability,command,command,414,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:698,usability,help,helps,698,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/9:759,usability,feedback,feedback,759,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:. ```. conda install -c conda-forge -c bioconda deepvariant. ```. It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9
https://github.com/google/deepvariant/issues/10:59,energy efficiency,model,model,59,"Hi,. can you tell me where you got your inception_v3.ckpt* model files? And, can you paste the content of your test_train.config.txt file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:95,performance,content,content,95,"Hi,. can you tell me where you got your inception_v3.ckpt* model files? And, can you paste the content of your test_train.config.txt file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:59,security,model,model,59,"Hi,. can you tell me where you got your inception_v3.ckpt* model files? And, can you paste the content of your test_train.config.txt file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:40,energy efficiency,model,model,40,"Oh!! By the way, if you want to train a model, you shouldn't be using make_examples.zip (like you listed in your example). You should be using: model_train.zip. Here is an example of a latest small training run that I did:. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt /home/pichuan/training/training.dataset_config.pbtxt \. --train_dir /home/pichuan/train_dir \. --start_from_checkpoint /home/pichuan/inception_v3_ckpt/model.ckpt. My training.dataset_config.pbtxt looks like this:. name: ""small_dataset"". tfrecord_path: ""/home/pichuan/training/small_dataset.training.shuffled.examples-?????-of-00010.tfrecord.gz"". num_examples: 72178.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:437,energy efficiency,model,model,437,"Oh!! By the way, if you want to train a model, you shouldn't be using make_examples.zip (like you listed in your example). You should be using: model_train.zip. Here is an example of a latest small training run that I did:. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt /home/pichuan/training/training.dataset_config.pbtxt \. --train_dir /home/pichuan/train_dir \. --start_from_checkpoint /home/pichuan/inception_v3_ckpt/model.ckpt. My training.dataset_config.pbtxt looks like this:. name: ""small_dataset"". tfrecord_path: ""/home/pichuan/training/small_dataset.training.shuffled.examples-?????-of-00010.tfrecord.gz"". num_examples: 72178.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:40,security,model,model,40,"Oh!! By the way, if you want to train a model, you shouldn't be using make_examples.zip (like you listed in your example). You should be using: model_train.zip. Here is an example of a latest small training run that I did:. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt /home/pichuan/training/training.dataset_config.pbtxt \. --train_dir /home/pichuan/train_dir \. --start_from_checkpoint /home/pichuan/inception_v3_ckpt/model.ckpt. My training.dataset_config.pbtxt looks like this:. name: ""small_dataset"". tfrecord_path: ""/home/pichuan/training/small_dataset.training.shuffled.examples-?????-of-00010.tfrecord.gz"". num_examples: 72178.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:437,security,model,model,437,"Oh!! By the way, if you want to train a model, you shouldn't be using make_examples.zip (like you listed in your example). You should be using: model_train.zip. Here is an example of a latest small training run that I did:. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt /home/pichuan/training/training.dataset_config.pbtxt \. --train_dir /home/pichuan/train_dir \. --start_from_checkpoint /home/pichuan/inception_v3_ckpt/model.ckpt. My training.dataset_config.pbtxt looks like this:. name: ""small_dataset"". tfrecord_path: ""/home/pichuan/training/small_dataset.training.shuffled.examples-?????-of-00010.tfrecord.gz"". num_examples: 72178.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:369,availability,down,downloaded,369,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:443,availability,sli,slim,443,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:415,energy efficiency,model,models,415,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:443,reliability,sli,slim,443,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:492,safety,test,test-training-dataset,492,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:415,security,model,models,415,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:492,testability,test,test-training-dataset,492,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:64,usability,command,command,64,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:103,usability,command,command,103,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. . This is my correct command:. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt. ```. The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data . This is my config file:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 1. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:112,availability,slo,slow,112,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:112,reliability,slo,slow,112,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:363,safety,test,test-training-dataset,363,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:363,testability,test,test-training-dataset,363,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:235,usability,user,user-images,235,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:526,usability,command,command,526,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:. ```. name: ""test-training-dataset"". tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz"". num_examples: 2. ```. This is my command. ```. python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:395,deployability,build,build,395,"First of all, the num_examples is meant to be how many tensorflow.Example are actually in the file pointed at tfrecord_path. You should set it correctly. (I assume you're not actually only training on 2 examples?). Second, if you want to run training, I highly recommend that you use a machine with GPU. See: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-details.md on how to build with GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:299,energy efficiency,GPU,GPU,299,"First of all, the num_examples is meant to be how many tensorflow.Example are actually in the file pointed at tfrecord_path. You should set it correctly. (I assume you're not actually only training on 2 examples?). Second, if you want to run training, I highly recommend that you use a machine with GPU. See: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-details.md on how to build with GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:406,energy efficiency,GPU,GPU,406,"First of all, the num_examples is meant to be how many tensorflow.Example are actually in the file pointed at tfrecord_path. You should set it correctly. (I assume you're not actually only training on 2 examples?). Second, if you want to run training, I highly recommend that you use a machine with GPU. See: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-details.md on how to build with GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:299,performance,GPU,GPU,299,"First of all, the num_examples is meant to be how many tensorflow.Example are actually in the file pointed at tfrecord_path. You should set it correctly. (I assume you're not actually only training on 2 examples?). Second, if you want to run training, I highly recommend that you use a machine with GPU. See: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-details.md on how to build with GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:406,performance,GPU,GPU,406,"First of all, the num_examples is meant to be how many tensorflow.Example are actually in the file pointed at tfrecord_path. You should set it correctly. (I assume you're not actually only training on 2 examples?). Second, if you want to run training, I highly recommend that you use a machine with GPU. See: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-details.md on how to build with GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:284,usability,command,command,284,"The tfrecord file i used come from a sample data. By the way if i have many tfrecord files come from some different sample, what should i set ""tfrecord_path""? Use wildcard like this ""/leostore/analysis/development/liteng/deepvariant_test/train_set/*tfrecord.gz"" or just use the ""cat"" command to combine all tfrecord files to a tfrecord file ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:593,reliability,doe,does,593,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there. But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: . https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211. It does affect the learning rate decay, but it doesn't have to be exact. I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:637,reliability,doe,doesn,637,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there. But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: . https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211. It does affect the learning rate decay, but it doesn't have to be exact. I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:5,safety,reme,remember,5,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there. But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: . https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211. It does affect the learning rate decay, but it doesn't have to be exact. I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:392,safety,reme,remember,392,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there. But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: . https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211. It does affect the learning rate decay, but it doesn't have to be exact. I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:609,usability,learn,learning,609,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there. But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: . https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211. It does affect the learning rate decay, but it doesn't have to be exact. I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:0,deployability,Updat,Update,0,"Update:. if you look at https://github.com/google/deepvariant/blob/r0.5/docs/visualizing_examples.ipynb. the code in read_tfrecords is an example of how you can read a tfrecord file, and count examples if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:0,safety,Updat,Update,0,"Update:. if you look at https://github.com/google/deepvariant/blob/r0.5/docs/visualizing_examples.ipynb. the code in read_tfrecords is an example of how you can read a tfrecord file, and count examples if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/10:0,security,Updat,Update,0,"Update:. if you look at https://github.com/google/deepvariant/blob/r0.5/docs/visualizing_examples.ipynb. the code in read_tfrecords is an example of how you can read a tfrecord file, and count examples if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10
https://github.com/google/deepvariant/issues/11:227,deployability,updat,update,227,"Hi Tomasz,. this issue might be the same as an earlier one. See this comment for a temporary solution:. https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:182,energy efficiency,current,currently,182,"Hi Tomasz,. this issue might be the same as an earlier one. See this comment for a temporary solution:. https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:227,safety,updat,update,227,"Hi Tomasz,. this issue might be the same as an earlier one. See this comment for a temporary solution:. https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:227,security,updat,update,227,"Hi Tomasz,. this issue might be the same as an earlier one. See this comment for a temporary solution:. https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:312,safety,input,input,312,"Hi,. This temporary fixed worked out:. 1. Set TF_NEED_GCP in settings.sh to 0. export TF_NEED_GCP=0. 2. Comment out line in make_examples.py and in call_variants.py. ""# htslib_gcp_oauth.init()"". 3. build_and_test.sh works. 4. I am able to run . python-bazel-bin/deepvariant/make_examples --mode calling --ref ../input/ucsc.hg19.chr20.unittest.fasta --reads ../input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --examples ../output/output.examples.tfrecord. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:360,safety,input,input,360,"Hi,. This temporary fixed worked out:. 1. Set TF_NEED_GCP in settings.sh to 0. export TF_NEED_GCP=0. 2. Comment out line in make_examples.py and in call_variants.py. ""# htslib_gcp_oauth.init()"". 3. build_and_test.sh works. 4. I am able to run . python-bazel-bin/deepvariant/make_examples --mode calling --ref ../input/ucsc.hg19.chr20.unittest.fasta --reads ../input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --examples ../output/output.examples.tfrecord. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:334,testability,unit,unittest,334,"Hi,. This temporary fixed worked out:. 1. Set TF_NEED_GCP in settings.sh to 0. export TF_NEED_GCP=0. 2. Comment out line in make_examples.py and in call_variants.py. ""# htslib_gcp_oauth.init()"". 3. build_and_test.sh works. 4. I am able to run . python-bazel-bin/deepvariant/make_examples --mode calling --ref ../input/ucsc.hg19.chr20.unittest.fasta --reads ../input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --examples ../output/output.examples.tfrecord. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:312,usability,input,input,312,"Hi,. This temporary fixed worked out:. 1. Set TF_NEED_GCP in settings.sh to 0. export TF_NEED_GCP=0. 2. Comment out line in make_examples.py and in call_variants.py. ""# htslib_gcp_oauth.init()"". 3. build_and_test.sh works. 4. I am able to run . python-bazel-bin/deepvariant/make_examples --mode calling --ref ../input/ucsc.hg19.chr20.unittest.fasta --reads ../input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --examples ../output/output.examples.tfrecord. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:360,usability,input,input,360,"Hi,. This temporary fixed worked out:. 1. Set TF_NEED_GCP in settings.sh to 0. export TF_NEED_GCP=0. 2. Comment out line in make_examples.py and in call_variants.py. ""# htslib_gcp_oauth.init()"". 3. build_and_test.sh works. 4. I am able to run . python-bazel-bin/deepvariant/make_examples --mode calling --ref ../input/ucsc.hg19.chr20.unittest.fasta --reads ../input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --examples ../output/output.examples.tfrecord. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:98,deployability,version,version,98,@tstokowy thanks for the pointer. We have a fix for this internally and it will arrive in the OSS version on our next push to the repo.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:98,integrability,version,version,98,@tstokowy thanks for the pointer. We have a fix for this internally and it will arrive in the OSS version on our next push to the repo.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/11:98,modifiability,version,version,98,@tstokowy thanks for the pointer. We have a fix for this internally and it will arrive in the OSS version on our next push to the repo.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11
https://github.com/google/deepvariant/issues/12:93,usability,command,command,93,`limits.h` should be in `/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed`. Try the following command:. `ls -l /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1192,deployability,BUILD,BUILD,1192,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1253,deployability,contain,contains,1253,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1521,deployability,BUILD,BUILD,1521,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1202,interoperability,specif,specifically,1202,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:201,modifiability,exten,extend,201,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1448,performance,cach,cache,1448,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1335,reliability,doe,doesnot,1335,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:400,security,modif,modified,400,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1139,security,modif,modified,1139,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:124,testability,understand,understanding,124,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:517,usability,tool,tools,517,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:638,usability,support,support,638,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```. /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h. /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h. /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h. /usr/include/c++/v1/support/ibm/limits.h. /usr/include/c++/4.8/tr1/limits.h. /usr/include/c++/5/tr1/limits.h. /usr/include/limits.h. /usr/include/linux/limits.h. /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h. /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h. /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h. /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h. ```. ```. includes = [. include_htslib,. ""."",. ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",. ]. ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:54,availability,error,errors,54,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:92,deployability,releas,release,92,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:54,performance,error,errors,54,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:54,safety,error,errors,54,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:268,security,modif,modification,268,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:7,usability,person,personally,7,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:54,usability,error,errors,54,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:49,deployability,build,building,49,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:96,deployability,build,build-prereqs,96,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:183,deployability,instal,installed,183,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:251,deployability,upgrad,upgrades,251,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:444,deployability,instal,installation,444,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:508,deployability,instal,installation,508,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:357,interoperability,distribut,distribution,357,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:251,modifiability,upgrad,upgrades,251,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:93,deployability,version,version,93,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:135,deployability,stage,stage,135,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:249,deployability,build,building,249,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:297,deployability,Instal,Install,297,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:322,deployability,build,build-prereq,322,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:556,deployability,INSTAL,INSTALL,556,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:584,deployability,instal,install,584,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:93,integrability,version,version,93,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:388,interoperability,specif,specific,388,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:616,interoperability,share,shared,616,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:93,modifiability,version,version,93,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:104,safety,test,test,104,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:366,safety,compl,complaint,366,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:919,safety,test,testing,919,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:366,security,compl,complaint,366,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:434,security,modif,modified,434,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:104,testability,test,test,104,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:645,testability,understand,understanding,645,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:919,testability,test,testing,919,"@depristo Thanks a lot for the clarification. . In my case, I am trying to have a standalone version to test with, without GCP at this stage. . In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, . since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:. ```. export DV_PLATFORM=""ubuntu-16"". cd .. git clone https://github.com/google/clif . cd clif. ./INSTALL.sh. python setup.py install. sudo ldconfig # Reload shared libraries. ```. To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ? `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:374,availability,echo,echo,374,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:526,availability,down,download,526,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:741,availability,echo,echo,741,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:934,availability,echo,echo,934,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1138,availability,echo,echo,1138,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:144,deployability,build,build,144,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:168,deployability,Instal,Install,168,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:204,deployability,instal,install,204,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:218,deployability,build,build,218,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:255,deployability,instal,install,255,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:309,deployability,instal,install,309,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:349,deployability,instal,install,349,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:383,deployability,build,building,383,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:419,deployability,instal,install,419,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:517,deployability,releas,releases,517,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:713,deployability,instal,install,713,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:750,deployability,build,building,750,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:854,deployability,INSTAL,INSTALL,854,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:890,deployability,INSTAL,INSTALL,890,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:921,deployability,INSTAL,INSTALL,921,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1323,deployability,INSTAL,INSTALL,1323,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1353,deployability,instal,install,1353,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1621,deployability,instal,install,1621,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1844,deployability,version,version,1844,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:224,integrability,sub,subversion,224,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:647,integrability,configur,configure,647,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1844,integrability,version,version,1844,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1787,interoperability,distribut,distribute,1787,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:647,modifiability,configur,configure,647,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:952,modifiability,pac,package,952,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1156,modifiability,pac,package,1156,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1844,modifiability,version,version,1844,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:0,reliability,Doe,Does,0,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:49,security,modif,modification,49,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:193,security,apt,apt-get,193,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:244,security,apt,apt-get,244,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:411,security,apt,apt-get,411,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:647,security,configur,configure,647,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1231,security,ident,identical,1231,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:1870,security,team,team,1870,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:150,usability,command,commands,150,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```. # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:91,deployability,fail,failed,91,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:. `--prepend clif/python/types.h`. That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:339,deployability,instal,installation,339,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:. `--prepend clif/python/types.h`. That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:410,deployability,releas,release,410,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:. `--prepend clif/python/types.h`. That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:91,reliability,fail,failed,91,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:. `--prepend clif/python/types.h`. That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:194,security,modif,modifying,194,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:. `--prepend clif/python/types.h`. That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:77,availability,operat,operations,77,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:156,deployability,build,builds,156,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:39,reliability,doe,doesn,39,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:134,security,sandbox,sandboxing,134,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:247,deployability,Build,Build,247,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:411,deployability,instal,installation,411,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:565,deployability,log,logic,565,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:378,modifiability,variab,variable,378,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:194,performance,time,time,194,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:434,reliability,doe,doesnot,434,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:87,safety,compl,complain,87,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:253,safety,compl,completed,253,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:486,safety,test,tested,486,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:565,safety,log,logic,565,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:87,security,compl,complain,87,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:253,security,compl,completed,253,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:565,security,log,logic,565,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:486,testability,test,tested,486,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:565,testability,log,logic,565,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:304,usability,close,closed,304,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/12:442,usability,help,help,442,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```. (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s. (18:20:33) INFO: Build completed successfully, 2 total actions. ```. Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12
https://github.com/google/deepvariant/issues/13:3,deployability,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:725,deployability,releas,release,725,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:839,deployability,log,logic,839,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:461,energy efficiency,current,currently,461,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:3,integrability,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:540,interoperability,format,format,540,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:605,interoperability,format,formatted,605,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1002,interoperability,specif,specific,1002,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:3,modifiability,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:623,modifiability,exten,extending,623,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1121,modifiability,maintain,maintained,1121,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:196,reliability,doe,doesn,196,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:3,safety,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:839,safety,log,logic,839,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1121,safety,maintain,maintained,1121,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:839,security,log,logic,839,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1045,security,auth,author,1045,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:3,testability,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:36,testability,simpl,simplest,36,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:170,testability,simpl,simplistic,170,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:839,testability,log,logic,839,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:36,usability,simpl,simplest,36,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:170,usability,simpl,simplistic,170,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:480,usability,usab,usable,480,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:573,usability,support,support,573,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1060,usability,tool,tool,1060,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/13:1488,usability,help,helps,1488,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13
https://github.com/google/deepvariant/issues/14:157,availability,avail,available,157,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:157,reliability,availab,available,157,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:157,safety,avail,available,157,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:157,security,availab,available,157,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:293,security,token,token,293,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:352,security,token,token,352,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:215,energy efficiency,model,models,215,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:227,energy efficiency,model,models,227,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:190,safety,input,input,190,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:201,safety,input,input,201,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:215,security,model,models,215,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:227,security,model,models,227,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:190,usability,input,input,190,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/14:201,usability,input,input,201,"We believe this should be fixed now in the 0.4.1 docker image. . ```. IMAGE_VERSION=0.4.1. gcloud docker -- pull gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \. gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION. ```. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14
https://github.com/google/deepvariant/issues/15:81,usability,Person,Personally,81,"Thank you! We weren't 100% certain the ideal dataset to evaluate the callset on. Personally I like to use two evaluation intervals: RefSeq alone and the capture intervals. RefSeq is nice because it is entirely orthogonal to the capture approach, which I feel is fair since the capture intervals are often only those where the capture proves to be effective, not actually where one wants to analyze their exome data (closer to RefSeq IMO). But I like replacing our refseq + 50 bp intervals with the capture intervals. . Out of curiosity, why intersect the capture intervals upfront with the confident regions? Happy and VCFEval both will internally do that intersection if you call on the capture intervals and provide the confident regions for evaluation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:347,usability,effectiv,effective,347,"Thank you! We weren't 100% certain the ideal dataset to evaluate the callset on. Personally I like to use two evaluation intervals: RefSeq alone and the capture intervals. RefSeq is nice because it is entirely orthogonal to the capture approach, which I feel is fair since the capture intervals are often only those where the capture proves to be effective, not actually where one wants to analyze their exome data (closer to RefSeq IMO). But I like replacing our refseq + 50 bp intervals with the capture intervals. . Out of curiosity, why intersect the capture intervals upfront with the confident regions? Happy and VCFEval both will internally do that intersection if you call on the capture intervals and provide the confident regions for evaluation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:416,usability,close,closer,416,"Thank you! We weren't 100% certain the ideal dataset to evaluate the callset on. Personally I like to use two evaluation intervals: RefSeq alone and the capture intervals. RefSeq is nice because it is entirely orthogonal to the capture approach, which I feel is fair since the capture intervals are often only those where the capture proves to be effective, not actually where one wants to analyze their exome data (closer to RefSeq IMO). But I like replacing our refseq + 50 bp intervals with the capture intervals. . Out of curiosity, why intersect the capture intervals upfront with the confident regions? Happy and VCFEval both will internally do that intersection if you call on the capture intervals and provide the confident regions for evaluation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:124,performance,perform,performing,124,"That's a good question. We use this BED file to provide to the calling programs where possible so that we don't use compute performing calling in regions we won't be evaluating on. You are correct, doing the intersection in hap.py will work just fine in terms of the final results. Here is the non-intersected BED file: . *edited by adding file attachment. [agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz](https://github.com/google/deepvariant/files/3875984/agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:124,usability,perform,performing,124,"That's a good question. We use this BED file to provide to the calling programs where possible so that we don't use compute performing calling in regions we won't be evaluating on. You are correct, doing the intersection in hap.py will work just fine in terms of the final results. Here is the non-intersected BED file: . *edited by adding file attachment. [agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz](https://github.com/google/deepvariant/files/3875984/agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:92,deployability,updat,update,92,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:56,integrability,compon,component,56,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:56,interoperability,compon,component,56,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:56,modifiability,compon,component,56,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:92,safety,updat,update,92,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/15:92,security,updat,update,92,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15
https://github.com/google/deepvariant/issues/16:96,availability,operat,operations,96,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:65,energy efficiency,CPU,CPU,65,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:65,performance,CPU,CPU,65,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:111,performance,perform,performed,111,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:185,reliability,diagno,diagnose,185,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:185,testability,diagno,diagnose,185,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:111,usability,perform,performed,111,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:174,usability,help,help,174,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:506,availability,echo,echo,506,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:253,deployability,contain,container,253,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:35,energy efficiency,CPU,CPU,35,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:687,energy efficiency,core,core,687,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:35,performance,CPU,CPU,35,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:310,usability,command,command,310,"Thank you for your reply @scott7z. CPU and OS infomation:. `uname -a`. Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux . `head -n 1 /etc/issue`. Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command. `./make_examples`. Nothing happened. `python make_examples.zip`. Still nothing happened. `unzip make_examples.zip`. `cd runfiles/genomics/deepvariant` . #Add deepvariant to PYTHONPATH . `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` . `python make_examples.py`. Then I got. ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference. [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:83,usability,command,commands,83,"OK, still trying to follow what is happening. Can you give us the full sequence of commands you typed to get the docker image and then run it? You shouldn't have to unzip any files; python knows how to execute them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:374,availability,down,downloaded,374,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:20,deployability,contain,contains,20,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:214,deployability,build,build,214,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:308,deployability,contain,contains,308,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:334,energy efficiency,model,models,334,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:407,energy efficiency,cloud,cloud,407,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:358,safety,test,testdata,358,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:904,safety,test,testdata,904,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:972,safety,test,testdata,972,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:334,security,model,models,334,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:358,testability,test,testdata,358,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:904,testability,test,testdata,904,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:929,testability,unit,unittest,929,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:972,testability,test,testdata,972,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:111,usability,user,user-images,111,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:476,usability,user,user-images,476,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:635,usability,workflow,workflow,635,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:748,usability,user,user-images,748,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:1147,usability,command,command,1147,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image. ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png). `docker build -f dockerfile -t deepvariant_1214 .`. `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url). ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png). `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`. Then I `cd /opt/deepvariant/bin/`. ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png). `./make_examples \. --mode calling \. --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --examples /data/quickstart-output/examples.tfrecord.gz`. When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:15,deployability,updat,updates,15,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:46,deployability,build,building,46,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:15,safety,updat,updates,15,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:15,security,updat,updates,15,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:57,usability,custom,customized,57,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:125,usability,confirm,confirm,125,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md. - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:50,energy efficiency,cpu,cpuinfo,50,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:131,energy efficiency,CPU,CPU,131,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:200,integrability,messag,message,200,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:200,interoperability,messag,message,200,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:50,performance,cpu,cpuinfo,50,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:131,performance,CPU,CPU,131,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:69,usability,help,help,69,"Also, if you can show us the output of. cat /proc/cpuinfo. that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,. which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:147,deployability,instal,installed,147,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:0,energy efficiency,Cloud,Cloud,0,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:15,energy efficiency,CPU,CPU,15,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:263,energy efficiency,CPU,CPU,263,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:6,interoperability,Platform,Platform,6,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:15,performance,CPU,CPU,15,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:263,performance,CPU,CPU,263,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:50,usability,user,user-images,50,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:298,usability,user,user-images,298,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:410,usability,user,user-images,410,Cloud Platform CPU Information：. ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar . Laptop CPU information：. ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png). ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:30,energy efficiency,CPU,CPU,30,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:39,energy efficiency,cloud,cloud,39,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:166,energy efficiency,CPU,CPU,166,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:45,interoperability,platform,platform,45,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:223,interoperability,platform,platform,223,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:30,performance,CPU,CPU,30,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:166,performance,CPU,CPU,166,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:59,reliability,doe,doesn,59,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:112,reliability,doe,does,112,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:67,usability,support,support,67,"It looks to me like the first CPU (the cloud platform one) doesn't support AVX,. while the second (your laptop) does. Our precompiled binarines require that from the CPU, so it looks like you'll need. to run on a different platform, or will need to compile everything from source. (DeepVariant, and Tensorflow, and probably CLIF,).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:13,usability,close,close,13,"I'm going to close this issue, since we seem to have gotten to the bottom of it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:212,interoperability,platform,platform,212,"Thanks for your help @scott7z @arostamianfar . According to your suggestion I compiled DeepVariant, Tensorflow, and CLIF from the source. But nothing changed, still can not get the result. Then, I ran on another platform which support AVX2 and got the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:16,usability,help,help,16,"Thanks for your help @scott7z @arostamianfar . According to your suggestion I compiled DeepVariant, Tensorflow, and CLIF from the source. But nothing changed, still can not get the result. Then, I ran on another platform which support AVX2 and got the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:227,usability,support,support,227,"Thanks for your help @scott7z @arostamianfar . According to your suggestion I compiled DeepVariant, Tensorflow, and CLIF from the source. But nothing changed, still can not get the result. Then, I ran on another platform which support AVX2 and got the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:686,availability,avail,available,686,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:418,deployability,build,build,418,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:482,deployability,build,build,482,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:163,energy efficiency,CPU,CPU,163,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:297,energy efficiency,CPU,CPU,297,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:619,interoperability,architectur,architecture,619,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:365,modifiability,variab,variable,365,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:163,performance,CPU,CPU,163,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:272,performance,tune,tuned,272,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:297,performance,CPU,CPU,297,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:167,reliability,doe,doesn,167,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:686,reliability,availab,available,686,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:608,safety,detect,detect,608,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:686,safety,avail,available,686,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:608,security,detect,detect,608,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:686,security,availab,available,686,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:175,usability,support,support,175,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:211,usability,minim,minimal,211,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system. The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,. The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture. So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:50,deployability,build,building,50,"You'll probably need to do something similar when building Tensorflow, too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:141,interoperability,platform,platform,141,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:173,safety,compl,completed,173,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:199,safety,test,test,199,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:173,security,compl,completed,173,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:87,testability,plan,plan,87,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:199,testability,test,test,199,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/16:155,usability,support,supports,155,Thanks for your reminder @scott7z . I ignored what you mentioned. But for now I do not plan to make any further changes. I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16
https://github.com/google/deepvariant/issues/17:351,availability,down,download,351,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:118,deployability,instal,install,118,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:317,deployability,version,version,317,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:491,deployability,build,build,491,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:588,deployability,build,build-test,588,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:38,energy efficiency,Cloud,Cloud,38,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:153,energy efficiency,Cloud,Cloud,153,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:240,energy efficiency,Cloud,Cloud,240,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:386,energy efficiency,cloud,cloud,386,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:317,integrability,version,version,317,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:317,modifiability,version,version,317,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:449,reliability,doe,doesn,449,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:594,safety,test,test,594,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:615,safety,input,input,615,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:835,safety,test,test-data,835,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:277,security,access,access,277,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:594,testability,test,test,594,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:835,testability,test,test-data,835,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:615,usability,input,input,615,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud. One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:0,reliability,Doe,Does,0,Does this work to fetch a single file? `curl http://storage.googleapis.com/deepvariant/binaries/DeepVariant/0.4.1/DeepVariant-0.4.1+cl-178668854/make_examples.zip >make_examples.zip`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:54,availability,down,down,54,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:98,deployability,instal,installing,98,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:71,performance,network,network,71,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:71,security,network,network,71,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:13,usability,close,close,13,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:346,integrability,event,event-,346,"Thank you sirs. Busy these days. I will try again. 2017-12-19 8:51 GMT+08:00 scott7z <notifications@github.com>:. > Closed #17 <https://github.com/google/deepvariant/issues/17>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/17#event-1393235778>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AWVBJpuqyKMlE6-mK3MvA5HTqGuFahpOks5tBwiQgaJpZM4RC-3W>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:223,security,auth,authored,223,"Thank you sirs. Busy these days. I will try again. 2017-12-19 8:51 GMT+08:00 scott7z <notifications@github.com>:. > Closed #17 <https://github.com/google/deepvariant/issues/17>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/17#event-1393235778>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AWVBJpuqyKMlE6-mK3MvA5HTqGuFahpOks5tBwiQgaJpZM4RC-3W>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:436,security,auth,auth,436,"Thank you sirs. Busy these days. I will try again. 2017-12-19 8:51 GMT+08:00 scott7z <notifications@github.com>:. > Closed #17 <https://github.com/google/deepvariant/issues/17>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/17#event-1393235778>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AWVBJpuqyKMlE6-mK3MvA5HTqGuFahpOks5tBwiQgaJpZM4RC-3W>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/issues/17:116,usability,Close,Closed,116,"Thank you sirs. Busy these days. I will try again. 2017-12-19 8:51 GMT+08:00 scott7z <notifications@github.com>:. > Closed #17 <https://github.com/google/deepvariant/issues/17>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/17#event-1393235778>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AWVBJpuqyKMlE6-mK3MvA5HTqGuFahpOks5tBwiQgaJpZM4RC-3W>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17
https://github.com/google/deepvariant/pull/18:2,security,sign,signed,2,I signed it!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:300,deployability,patch,patch,300,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:350,deployability,releas,release,350,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:300,safety,patch,patch,300,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:300,security,patch,patch,300,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:169,availability,avail,available,169,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:0,deployability,Patch,Patching,0,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:27,deployability,releas,releasing,27,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:169,reliability,availab,available,169,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:0,safety,Patch,Patching,0,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:169,safety,avail,available,169,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:0,security,Patch,Patching,0,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:169,security,availab,available,169,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:191,usability,help,helpful,191,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:199,usability,support,support,199,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/pull/18:39,deployability,releas,release,39,This is in and will go out in the next release. Thank you so much Brad!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18
https://github.com/google/deepvariant/issues/19:108,usability,close,closer,108,I tried it and I'm seeing it as well. I wonder if this could be something outside DeepVariant. We'll take a closer look and get back to you later.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:19,availability,down,downgrading,19,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:223,availability,down,download,223,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:111,deployability,instal,install,111,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:128,deployability,version,version,128,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:214,deployability,releas,releases,214,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:276,deployability,instal,installer-linux-,276,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:351,deployability,instal,installer-linux-,351,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:128,integrability,version,version,128,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:128,modifiability,version,version,128,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:6,usability,confirm,confirm,6,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:379,usability,user,user,379,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me. Here's what I did to install an older version of bazel:. ```. BAZEL_VERSION=0.8.1. wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh. chmod +x bazel-*.sh. ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user. ```. Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:21,deployability,instal,install,21,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:119,deployability,continu,continuous,119,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,deployability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,integrability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,interoperability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,modifiability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:105,reliability,doe,does,105,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,reliability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:100,security,team,team,100,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,security,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:130,testability,integr,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:202,testability,simpl,simple,202,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:202,usability,simpl,simple,202,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:0,deployability,Instal,Installing,0,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:65,deployability,updat,updating,65,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:75,deployability,build,build-prereq,75,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:95,deployability,instal,install,95,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:114,deployability,version,version,114,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:114,integrability,version,version,114,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:105,interoperability,specif,specific,105,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:114,modifiability,version,version,114,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:65,safety,updat,updating,65,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:65,security,updat,updating,65,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:154,usability,help,help,154,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:54,deployability,version,version,54,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like. export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a"". (That compiles, but is otherwise untested.).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:54,integrability,version,version,54,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like. export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a"". (That compiles, but is otherwise untested.).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:45,interoperability,specif,specific,45,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like. export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a"". (That compiles, but is otherwise untested.).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:54,modifiability,version,version,54,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like. export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a"". (That compiles, but is otherwise untested.).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:105,modifiability,evolv,evolved,105,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like. export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a"". (That compiles, but is otherwise untested.).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:222,deployability,releas,release,222,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:310,deployability,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:328,deployability,continu,continue,328,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:427,deployability,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:667,deployability,log,log,667,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:932,deployability,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:310,integrability,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:413,integrability,complian,compliance,413,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:427,integrability,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:932,integrability,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:146,interoperability,specif,specific,146,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:213,interoperability,specif,specific,213,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:310,modifiability,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:340,modifiability,evolv,evolve,340,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:427,modifiability,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:449,modifiability,maintain,maintainers,449,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:932,modifiability,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:395,reliability,pra,practical,395,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:310,safety,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:413,safety,compl,compliance,413,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:427,safety,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:449,safety,maintain,maintainers,449,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:667,safety,log,log,667,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:932,safety,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:413,security,compl,compliance,413,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:667,security,log,log,667,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:310,testability,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:367,testability,simpl,simplify,367,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:427,testability,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:667,testability,log,log,667,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:932,testability,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:259,usability,support,supporting,259,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:357,usability,prefer,prefer,357,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/19:367,usability,simpl,simplify,367,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash. $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'. 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a. 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2. $. ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19
https://github.com/google/deepvariant/issues/20:25,deployability,build,build-prereq,25,"We've pushed a change to build-prereq.sh to pin bazel to 0.8.1, which should resolve this issue. Please reopen if necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/20
https://github.com/google/deepvariant/issues/21:191,availability,avail,available,191,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:93,deployability,updat,update,93,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:816,deployability,build,build,816,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:861,deployability,Build,Building,861,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:913,deployability,build,build-test,913,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:240,energy efficiency,cloud,cloud,240,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:357,energy efficiency,CPU,CPU,357,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:426,energy efficiency,cpu,cpuinfo,426,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:675,energy efficiency,optim,optimization,675,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:843,energy efficiency,CPU,CPU,843,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:382,integrability,Bridg,Bridge,382,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:382,interoperability,Bridg,Bridge,382,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:357,performance,CPU,CPU,357,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:426,performance,cpu,cpuinfo,426,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:675,performance,optimiz,optimization,675,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:843,performance,CPU,CPU,843,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:191,reliability,availab,available,191,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:93,safety,updat,update,93,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:191,safety,avail,available,191,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:874,safety,test,testing,874,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:919,safety,test,test,919,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:93,security,updat,update,93,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:191,security,availab,available,191,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:874,testability,test,testing,874,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:919,testability,test,test,919,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:59,usability,document,documentation,59,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:395,usability,support,supports,395,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:751,usability,clear,clear,751,"Hi Paul, thanks for mentioning this issue. I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for. Would it be ok for you to build DeepVariant for your CPU by following [Building and testing. DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:210,availability,state,statement,210,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:463,deployability,releas,release-notes,463,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:300,energy efficiency,cpu,cpuinfo,300,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:210,integrability,state,statement,210,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:235,interoperability,specif,specific,235,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:300,performance,cpu,cpuinfo,300,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:203,testability,assert,assert,203,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:91,usability,user,users,91,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:523,usability,custom,customized,523,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,. `p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:97,availability,error,error,97,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:103,integrability,messag,messages,103,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:103,interoperability,messag,messages,103,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:97,performance,error,error,97,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:216,performance,time,time,216,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:97,safety,error,error,97,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:97,usability,error,error,97,Thanks Paul. I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1051,availability,sli,slim,1051,"e as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2273,availability,down,down,2273,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2285,availability,avail,available,2285,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3182,availability,down,down,3182,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1309,deployability,pipelin,pipelines,1309," the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1629,deployability,contain,contain,1629," 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2496,deployability,pipelin,pipelines,2496,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,deployability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2938,deployability,pipelin,pipelines,2938,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,deployability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3079,deployability,modul,modularity,3079,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:200,energy efficiency,Current,Currently,200,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:477,energy efficiency,model,model,477,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:512,energy efficiency,predict,predicted,512,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:828,energy efficiency,model,models,828,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1023,energy efficiency,model,models,1023,"ffort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-exca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1225,energy efficiency,model,model,1225," wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2378,energy efficiency,model,model,2378,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2630,energy efficiency,model,models,2630,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:69,integrability,sub,sub-,69,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:229,integrability,wrap,wrapper,229,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:781,integrability,abstract,abstracting,781,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1309,integrability,pipelin,pipelines,1309," the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1408,integrability,transform,transformation,1408,"h is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1703,integrability,transform,transformed,1703,".88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1748,integrability,messag,message,1748," but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- ena",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1770,integrability,filter,filter-inspected,1770,"is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2155,integrability,sub,subtle,2155,"our input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2447,integrability,sub,subtypes,2447,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2496,integrability,pipelin,pipelines,2496,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,integrability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2938,integrability,pipelin,pipelines,2938,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,integrability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3079,integrability,modular,modularity,3079,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:229,interoperability,wrapper,wrapper,229,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:865,interoperability,platform,platform,865,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1408,interoperability,transform,transformation,1408,"h is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1651,interoperability,standard,standards,1651,"_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functiona",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1703,interoperability,transform,transformed,1703,".88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1748,interoperability,messag,message,1748," but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- ena",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2228,interoperability,specif,specific,2228,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,interoperability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,interoperability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:781,modifiability,abstract,abstracting,781,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1138,modifiability,require chang,require changes,1138,"nother level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is someth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1461,modifiability,interm,intermediate,1461,"rough the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,modifiability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,modifiability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3079,modifiability,modul,modularity,3079,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3095,modifiability,refact,refactoring,3095,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:35,performance,Time,Time,35,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1108,performance,network,network,1108,"r and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2566,performance,cach,cached,2566,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3095,performance,refactor,refactoring,3095,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1051,reliability,sli,slim,1051,"e as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2285,reliability,availab,available,2285,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,reliability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,reliability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:286,safety,input,input,286,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:512,safety,predict,predicted,512,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1162,safety,input,input,1162," analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1812,safety,valid,validation,1812,"nals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2285,safety,avail,available,2285,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3079,safety,modul,modularity,3079,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:20,security,team,team,20,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:477,security,model,model,477,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:828,security,model,models,828,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1023,security,model,models,1023,"ffort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-exca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1108,security,network,network,1108,"r and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1225,security,model,model,1225," wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1812,security,validat,validation,1812,"nals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2285,security,availab,available,2285,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2378,security,model,model,2378,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2630,security,model,models,2630,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,security,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,security,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2758,testability,integr,integrated,2758,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2962,testability,integr,integration,2962,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:3079,testability,modula,modularity,3079,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:286,usability,input,input,286,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:744,usability,help,helpful,744,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1162,usability,input,input,1162," analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```. genotype_probabilities: 0.9999428988. genotype_probabilities: 1.8287e-05. genotype_probabilities: 3.88142e-05. ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2435,usability,behavi,behavior,2435,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:2688,usability,Person,Personlized,2688,"ome more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```. variant {. reference_bases: ""A"". alternate_bases: ""C"". calls {. info {. key: ""AD"". ... call_set_name: ""Sample_Diag-excap51-HG002-EEogPU"". }. end: 1115835. reference_name: ""1"". start: 1115834. ... ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:341,deployability,releas,releases,341,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:427,deployability,build,build,427,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:200,energy efficiency,model,model,200,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:200,security,model,model,200,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:161,usability,efficien,efficiently,161,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:191,usability,learn,learning,191,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:354,usability,User,Users,354,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:238,deployability,observ,observed,238,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1138,deployability,build,builds,1138,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1153,deployability,version,versions,1153,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1210,deployability,modul,modularity,1210,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1251,deployability,pipelin,pipeline,1251,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1451,deployability,pipelin,pipeline,1451,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:273,energy efficiency,optim,optimization,273,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:385,energy efficiency,optim,optimizations,385,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:465,energy efficiency,model,model,465,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:647,energy efficiency,optim,optimizations,647,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:765,energy efficiency,Optim,Optimization,765,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1308,energy efficiency,model,models,1308,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:689,integrability,batch,batch,689,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1153,integrability,version,versions,1153,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1210,integrability,modular,modularity,1210,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1251,integrability,pipelin,pipeline,1251,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1276,integrability,plug-in,plug-in,1276,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1451,integrability,pipelin,pipeline,1451,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:727,interoperability,format,formatted,727,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:785,interoperability,Format,Format,785,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1276,interoperability,plug,plug-in,1276,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1153,modifiability,version,versions,1153,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1210,modifiability,modul,modularity,1210,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:273,performance,optimiz,optimization,273,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:359,performance,Network,Networks,359,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:385,performance,optimiz,optimizations,385,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:595,performance,perform,performance,595,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:647,performance,optimiz,optimizations,647,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:689,performance,batch,batch,689,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:765,performance,Optimiz,Optimization,765,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:811,performance,time,time,811,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1210,safety,modul,modularity,1210,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:359,security,Network,Networks,359,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:465,security,model,model,465,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1308,security,model,models,1308,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:12,testability,understand,understand,12,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:62,testability,understand,understanding,62,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:238,testability,observ,observed,238,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1210,testability,modula,modularity,1210,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:595,usability,perform,performance,595,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1242,usability,workflow,workflow,1242,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1266,usability,user,users,1266,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1505,usability,minim,minimizing,1505,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1520,usability,support,support,1520,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | . | ------------ | ----------- | ------------- | ------------- | ------------- | . | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | . | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | . | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | . | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:275,deployability,version,version,275,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:306,deployability,build,build,306,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:331,deployability,version,version,331,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:380,deployability,configurat,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:535,deployability,deploy,deployments,535,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:696,deployability,orchestr,orchestration,696,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:727,deployability,manag,manage,727,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:812,deployability,version,version,812,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:87,energy efficiency,model,models,87,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:110,energy efficiency,model,modeling,110,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:193,energy efficiency,model,models,193,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:321,energy efficiency,optim,optimized,321,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:585,energy efficiency,optim,optimized,585,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:727,energy efficiency,manag,manage,727,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:782,energy efficiency,load,load,782,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:922,energy efficiency,cloud,cloud,922,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1053,energy efficiency,optim,optimization,1053,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:275,integrability,version,version,275,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:331,integrability,version,version,331,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:380,integrability,configur,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:696,integrability,orchestr,orchestration,696,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:812,integrability,version,version,812,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:266,interoperability,specif,specific,266,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:887,interoperability,heterogen,heterogenous,887,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:275,modifiability,version,version,275,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:331,modifiability,version,version,331,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:380,modifiability,configur,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:812,modifiability,version,version,812,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1216,modifiability,deco,decoupling,1216,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:321,performance,optimiz,optimized,321,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:459,performance,perform,performance,459,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:585,performance,optimiz,optimized,585,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:625,performance,perform,performance,625,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:782,performance,load,load,782,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:801,performance,perform,performant,801,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1053,performance,optimiz,optimization,1053,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1160,performance,perform,performance,1160,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:727,safety,manag,manage,727,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:87,security,model,models,87,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:110,security,model,modeling,110,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:193,security,model,models,193,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:380,security,configur,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:459,usability,perform,performance,459,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:571,usability,custom,custom,571,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:625,usability,perform,performance,625,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:801,usability,perform,performant,801,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1160,usability,perform,performance,1160,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:188,deployability,build,building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,188,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:179,energy efficiency,power,power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,179,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:47,integrability,event,eventually,47,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:216,interoperability,platform,platform-how-deepvariant-uses-intels-avx-optimizations-,216,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:257,performance,optimiz,optimizations-,257,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:259,availability,operat,operate,259,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:479,energy efficiency,optim,optimizations-on-modern-intel-architecture,479,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:624,energy efficiency,optim,optimizations,624,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:683,energy efficiency,optim,optimization,683,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1037,energy efficiency,optim,optimizations,1037,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1098,energy efficiency,CPU,CPUs,1098,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1547,energy efficiency,optim,optimizations,1547,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:509,interoperability,architectur,architecture,509,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:914,interoperability,specif,specific-dispatching-on-intel-architectures,914,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:479,performance,optimiz,optimizations-on-modern-intel-architecture,479,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:537,performance,throughput,throughput,537,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:624,performance,optimiz,optimizations,624,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:683,performance,optimiz,optimization,683,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:718,performance,perform,performs,718,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1037,performance,optimiz,optimizations,1037,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1098,performance,CPU,CPUs,1098,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1547,performance,optimiz,optimizations,1547,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:104,safety,compl,complete,104,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:317,safety,test,test,317,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:104,security,compl,complete,104,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:556,security,sign,significant,556,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:1083,security,access,access,1083,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:317,testability,test,test,317,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:718,usability,perform,performs,718,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:804,usability,user,users,804,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/21:892,usability,guid,guide-instruction-set-specific-dispatching-on-intel-architectures,892,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl. * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21
https://github.com/google/deepvariant/issues/22:82,availability,error,error,82,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:275,availability,error,error,275,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:10,deployability,version,version,10,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:383,deployability,version,version,383,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:10,integrability,version,version,10,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:383,integrability,version,version,383,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:587,integrability,sub,subscribed,587,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:10,modifiability,version,version,10,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:383,modifiability,version,version,383,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:82,performance,error,error,82,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:275,performance,error,error,275,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:82,safety,error,error,82,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:275,safety,error,error,275,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:786,security,auth,auth,786,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:82,usability,error,error,82,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:275,usability,error,error,275,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-. > ). >. > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version. > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/. > runfiles/protobuf_archive/python/google/protob. > uf/pyext/_message.so). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:191,energy efficiency,Cloud,Cloud,191,"Hi, Thomas. The output of uname -a is. Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41 3.16.0-4-amd64 #1 SMP Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. I run it on Google Cloud Platform",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:197,interoperability,Platform,Platform,197,"Hi, Thomas. The output of uname -a is. Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41 3.16.0-4-amd64 #1 SMP Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. I run it on Google Cloud Platform",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:347,energy efficiency,cloud,cloud-platform,347,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:426,energy efficiency,cloud,cloud,426,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:937,energy efficiency,Cloud,Cloud,937,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:96,integrability,configur,configuring,96,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:353,interoperability,platform,platform,353,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:943,interoperability,Platform,Platform,943,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:96,modifiability,configur,configuring,96,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:480,performance,disk,disk-size,480,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:503,performance,disk,disk-type,503,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:532,performance,disk,disk-device-name,532,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:96,security,configur,configuring,96,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:1207,security,auth,auth,1207,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:229,usability,command,command,229,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:281,usability,USER,USER,281,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/22:452,usability,custom,custom-,452,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in. https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --boot-disk-device-name ""deepvariant-casestudy"" \. --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>. wrote:. > Hi, Thomas. > The output of uname -a is. >. > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41. > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP. > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux. >. > I run it on Google Cloud Platform. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22
https://github.com/google/deepvariant/issues/23:246,availability,down,down,246,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23
https://github.com/google/deepvariant/issues/23:269,deployability,stack,stack,269,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23
https://github.com/google/deepvariant/issues/23:310,deployability,build,build,310,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23
https://github.com/google/deepvariant/issues/23:344,performance,perform,performance,344,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23
https://github.com/google/deepvariant/issues/23:344,usability,perform,performance,344,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23
https://github.com/google/deepvariant/issues/24:59,availability,error,error,59,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:8,deployability,build,build,8,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:187,deployability,updat,updated,187,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:245,deployability,build,build,245,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:59,performance,error,error,59,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:59,safety,error,error,59,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:187,safety,updat,updated,187,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:187,security,updat,updated,187,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:59,usability,error,error,59,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:81,usability,confirm,confirm,81,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:27,deployability,updat,update,27,Closing as there's been no update to this thread in 9 days. Please reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:27,safety,updat,update,27,Closing as there's been no update to this thread in 9 days. Please reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/24:27,security,updat,update,27,Closing as there's been no update to this thread in 9 days. Please reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24
https://github.com/google/deepvariant/issues/25:400,deployability,instal,install,400,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:420,deployability,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:517,deployability,contain,container,517,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:531,deployability,build,build,531,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:556,deployability,contain,container,556,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:594,deployability,build,build,594,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:354,energy efficiency,current,current,354,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:476,energy efficiency,cloud,cloud,476,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:138,integrability,topic,topic,138,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:420,integrability,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:420,modifiability,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:420,safety,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:41,security,access,accessible,41,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:731,security,access,access,731,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:420,testability,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:304,usability,support,support,304,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:149,deployability,contain,container,149,@depristo Thanks for the prompt reply. I will let you know if I find a way to get DV working with virtualenvs. I will also try running from a Docker container.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:32,deployability,updat,update,32,@depristo RE docker docs: I can update the README in https://github.com/google/deepvariant/tree/master/deepvariant/docker to include a link to https://github.com/google/deepvariant/tree/master/docs/deepvariant-docker.md (note that this README is included inside the prebuild image as well). I think that page has all the instructions needed to use our prebuilt image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:32,safety,updat,update,32,@depristo RE docker docs: I can update the README in https://github.com/google/deepvariant/tree/master/deepvariant/docker to include a link to https://github.com/google/deepvariant/tree/master/docs/deepvariant-docker.md (note that this README is included inside the prebuild image as well). I think that page has all the instructions needed to use our prebuilt image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/25:32,security,updat,update,32,@depristo RE docker docs: I can update the README in https://github.com/google/deepvariant/tree/master/deepvariant/docker to include a link to https://github.com/google/deepvariant/tree/master/docs/deepvariant-docker.md (note that this README is included inside the prebuild image as well). I think that page has all the instructions needed to use our prebuilt image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25
https://github.com/google/deepvariant/issues/26:44,energy efficiency,Cloud,Cloud,44,Thank you for the report! Setting up Google Cloud is actually not required to use the docker images (if running locally or on a dedicated VM). So just completing the quick start should be enough. I'll remove the extra links from the page.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/26
https://github.com/google/deepvariant/issues/26:151,safety,compl,completing,151,Thank you for the report! Setting up Google Cloud is actually not required to use the docker images (if running locally or on a dedicated VM). So just completing the quick start should be enough. I'll remove the extra links from the page.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/26
https://github.com/google/deepvariant/issues/26:151,security,compl,completing,151,Thank you for the report! Setting up Google Cloud is actually not required to use the docker images (if running locally or on a dedicated VM). So just completing the quick start should be enough. I'll remove the extra links from the page.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/26
https://github.com/google/deepvariant/issues/27:11,availability,error,error,11,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:577,availability,error,error,577,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:612,availability,error,error,612,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:687,availability,error,errors,687,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:739,availability,error,error,739,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:324,deployability,pipelin,pipeline,324,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:707,deployability,log,logs,707,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:363,energy efficiency,model,model,363,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:324,integrability,pipelin,pipeline,324,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:745,integrability,messag,message,745,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:162,interoperability,format,format,162,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:745,interoperability,messag,message,745,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:11,performance,error,error,11,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:577,performance,error,error,577,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:612,performance,error,error,612,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:687,performance,error,errors,687,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:739,performance,error,error,739,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:11,safety,error,error,11,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:50,safety,input,input,50,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:63,safety,test,test-shan,63,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:577,safety,error,error,577,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:612,safety,error,error,612,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:687,safety,error,errors,687,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:707,safety,log,logs,707,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:739,safety,error,error,739,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:363,security,model,model,363,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:707,security,log,logs,707,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:63,testability,test,test-shan,63,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:707,testability,log,logs,707,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:11,usability,error,error,11,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:50,usability,input,input,50,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:349,usability,workflow,workflow,349,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:577,usability,error,error,577,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:612,usability,error,error,612,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:687,usability,error,errors,687,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:739,usability,error,error,739,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error? P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:284,energy efficiency,reduc,reduce,284,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue? @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:103,safety,test,test,103,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue? @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:315,safety,avoid,avoid,315,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue? @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:103,testability,test,test,103,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue? @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:440,availability,error,error,440,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:489,availability,error,error,489,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:852,availability,error,error,852,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:136,deployability,contain,contains,136,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:384,deployability,contain,contains,384,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:425,deployability,fail,fail,425,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:503,deployability,observ,observed,503,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:837,deployability,fail,fail,837,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:45,energy efficiency,current,current,45,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:408,energy efficiency,current,current,408,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:827,energy efficiency,current,currently,827,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:446,integrability,messag,message,446,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:495,integrability,messag,message,495,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:249,interoperability,format,format,249,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:446,interoperability,messag,message,446,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:495,interoperability,messag,message,495,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:658,interoperability,format,format,658,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:729,interoperability,specif,specifically,729,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:440,performance,error,error,440,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:489,performance,error,error,489,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:852,performance,error,error,852,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:425,reliability,fail,fail,425,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:837,reliability,fail,fail,837,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:440,safety,error,error,440,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:489,safety,error,error,489,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:546,safety,input,input,546,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:559,safety,test,test-shan,559,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:852,safety,error,error,852,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:1020,safety,compl,complete,1020,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:1042,safety,input,input,1042,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:1020,security,compl,complete,1020,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:503,testability,observ,observed,503,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:559,testability,test,test-shan,559,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:53,usability,statu,status,53,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:440,usability,error,error,440,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:489,usability,error,error,489,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:546,usability,input,input,546,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:852,usability,error,error,852,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:955,usability,user,users,955,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:1042,usability,input,input,1042,"Hi Mark and Asha,. here's what I believe the current status is:. (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug. (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:. The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:348,deployability,configurat,configuration,348,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:88,energy efficiency,cloud,cloud,88,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:373,energy efficiency,cloud,cloud,373,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:348,integrability,configur,configuration,348,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:62,interoperability,distribut,distributed,62,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:348,modifiability,configur,configuration,348,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:224,reliability,doe,doesn,224,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:338,safety,test,test,338,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:348,security,configur,configuration,348,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:338,testability,test,test,338,"yes, I think this is a real bug that still exists. Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard'). You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:45,deployability,releas,release,45,This has been fixed by the DeepVariant 0.5.1 release that just came out a few minutes ago. Thank you for raising attention to this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:43,deployability,releas,release,43,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:103,deployability,releas,release,103,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:167,deployability,version,version,167,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:733,deployability,version,version,733,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:345,energy efficiency,green,green,345,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:548,energy efficiency,green,green,548,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:167,integrability,version,version,167,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:733,integrability,version,version,733,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:167,modifiability,version,version,167,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:733,modifiability,version,version,733,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:397,usability,user,user-images,397,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:594,usability,user,user-images,594,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:171,deployability,observ,observation,171,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:242,energy efficiency,cloud,cloud,242,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:445,energy efficiency,cloud,cloud-platform,445,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:521,energy efficiency,cloud,cloud,521,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:451,interoperability,platform,platform,451,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:645,interoperability,specif,specify,645,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:569,performance,disk,disk-size,569,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:592,performance,disk,disk-type,592,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:761,performance,time,times,761,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:637,reliability,Doe,Doesn,637,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:288,security,ident,identical,288,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:171,testability,observ,observation,171,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:327,usability,command,command,327,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:382,usability,USER,USER,382,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:544,usability,custom,custom-,544,"Hi Paul,. Two quick suggestions. First, I'd recommend posting this question in a separate issue, to keep the discussion clean since this is a very interesting and general observation. Second, it's unclear to us if this is normal variation in cloud timing [not all machines you create are identical. For example, the case study command:. ```. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" --zone ""us-west1-b"". ```. Doesn't specify the exact machine type, so we're likely getting skylake processors sometimes and broadwell processors other times. That alone could account for the variation in timing we are seeing here. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:142,deployability,releas,released,142,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:240,deployability,releas,releases,240,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:524,deployability,releas,release,524,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:129,performance,time,time,129,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:540,performance,time,time,540,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:347,safety,detect,detected,347,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:459,safety,test,test,459,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:508,safety,test,test,508,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:550,safety,test,tested,550,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:347,security,detect,detected,347,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:454,testability,unit,unit,454,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:459,testability,test,test,459,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:503,testability,unit,unit,503,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:508,testability,test,test,508,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:550,testability,test,tested,550,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/27:592,usability,confirm,confirmed,592,"Hi all,. it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:. https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:. https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9. (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27
https://github.com/google/deepvariant/issues/28:101,reliability,doe,doesn,101,This is a known issue (tracked in our internal buganizer tracker). The problem is that your BAM file doesn't have a sample name associated with the reads. You can use the --sample_name argument to provide a value to make_examples.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:40,availability,error,error,40,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:8,deployability,updat,update,8,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:87,deployability,version,version,87,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:105,deployability,releas,release,105,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:46,integrability,messag,messages,46,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:87,integrability,version,version,87,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:46,interoperability,messag,messages,46,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:87,modifiability,version,version,87,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:40,performance,error,error,40,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:8,safety,updat,update,8,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:40,safety,error,error,40,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:8,security,updat,update,8,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:40,usability,error,error,40,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/28:81,usability,command,command,81,"@depristo Thank you Mark, make_examples.py is working now with the --sample_name command line argument.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28
https://github.com/google/deepvariant/issues/29:105,deployability,build,build,105,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:125,deployability,version,version,125,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:125,integrability,version,version,125,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:91,modifiability,reu,reuse,91,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:125,modifiability,version,version,125,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:46,usability,prefer,prefer,46,@chapmanb thanks for writing this up. I would prefer to get `clif` into conda-forge and we reuse this to build a relocatable version of DeepVariant. If this is not possible it would be great to address your points so we get relocatable binaries. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:408,availability,avail,available,408,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:42,deployability,build,building,42,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:111,deployability,build,building,111,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:165,deployability,version,version,165,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:389,deployability,build,building,389,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:165,integrability,version,version,165,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:165,modifiability,version,version,165,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:240,modifiability,pac,package,240,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:261,performance,time,time,261,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:408,reliability,availab,available,408,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:408,safety,avail,available,408,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:408,security,availab,available,408,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:5,availability,ping,pinged,5,I've pinged the @mrovner and @gpshead about CLIF. I'm hopeful they'll chime in here.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:19,deployability,build,building,19,We understand that building CLIF is a high tall for our users and putting efforts to make it easier.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3,testability,understand,understand,3,We understand that building CLIF is a high tall for our users and putting efforts to make it easier.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:56,usability,user,users,56,We understand that building CLIF is a high tall for our users and putting efforts to make it easier.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,deployability,Updat,Update,0,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:20,deployability,version,version,20,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,deployability,version,version,409,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:430,deployability,build,build,430,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:480,deployability,build,build,480,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:20,integrability,version,version,20,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:211,integrability,messag,message,211,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,integrability,version,version,409,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:689,integrability,messag,message,689,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:211,interoperability,messag,message,211,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:308,interoperability,incompatib,incompatible,308,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:689,interoperability,messag,message,689,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:20,modifiability,version,version,20,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,modifiability,version,version,409,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:657,reliability,pra,practice,657,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,safety,Updat,Update,0,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,security,Updat,Update,0,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:. ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same. However, it seems like bad practice to ignore that warning message that shows up in red. Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,deployability,version,version,409,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:545,deployability,updat,updating,545,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:654,deployability,updat,updating,654,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,integrability,version,version,409,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,modifiability,version,version,409,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:545,safety,updat,updating,545,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:654,safety,updat,updating,654,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:545,security,updat,updating,545,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:654,security,updat,updating,654,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:884,usability,help,helps,884,"Hi Pichuan and Brad,. So it looks like Bioconda's requirement for [numpy 1.12](https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9) originated from this issue:. https://github.com/bioconda/bioconda-recipes/issues/3961. which got merged into this PR:. https://github.com/bioconda/bioconda-recipes/pull/4888. But it seems the driver for the 1.12 version was CNVkit, which just requires >= 1.9 based on the setup here:. https://github.com/etal/cnvkit/blob/master/setup.py#L19. Maybe updating Bioconda first with 1.14 might be a good start, and seeing if that PR passes Travis, otherwise just updating the DV scripts with a [virtualenv](https://pypi.org/project/virtualenv/) instance, so that their local environment remains pristine. The alternative is for folks to use it via the Docker image. What do you think? Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:141,deployability,updat,update,141,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:309,deployability,depend,dependency,309,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:567,deployability,instal,installing,567,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:309,integrability,depend,dependency,309,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:469,interoperability,compatib,compatible,469,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:309,modifiability,depend,dependency,309,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:141,safety,updat,update,141,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:309,safety,depend,dependency,309,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:426,safety,prevent,preventing,426,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:584,safety,isol,isolated,584,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:141,security,updat,update,141,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:426,security,preven,preventing,426,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:584,security,iso,isolated,584,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:309,testability,depend,dependency,309,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:584,testability,isol,isolated,584,"Pi-Chuan -- thanks so much for looking at this. Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2891,availability,echo,echo,2891,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3043,availability,down,download,3043,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3258,availability,echo,echo,3258,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3451,availability,echo,echo,3451,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3655,availability,echo,echo,3655,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:8,deployability,updat,update,8,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,deployability,depend,dependency,23,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:153,deployability,build,build,153,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:196,deployability,releas,release,196,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:231,deployability,build,build,231,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:267,deployability,releas,released,267,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:521,deployability,build,build,521,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:767,deployability,build,building,767,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:917,deployability,build,build,917,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:946,deployability,Build,Builds,946,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1054,deployability,version,versions,1054,"iced, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1119,deployability,build,build,1119,"tly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1206,deployability,build,build,1206,"cript that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1243,deployability,build,build,1243,"ch should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1255,deployability,instal,install,1255,"ally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1459,deployability,build,build-prereq,1459,"e the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1478,deployability,build,build,1478,"re right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1507,deployability,version,versions,1507,"s used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1534,deployability,releas,released,1534," I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1707,deployability,updat,update,1707,"hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1913,deployability,instal,installation,1913,"ild it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1960,deployability,version,version,1960,"nary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2154,deployability,instal,install,2154,"# compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2188,deployability,instal,install,2188," To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2310,deployability,instal,install,2310,"/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2345,deployability,instal,install,2345,"raries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2488,deployability,instal,install,2488,"ant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2522,deployability,instal,install,2522,"ilt and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2685,deployability,Instal,Install,2685,"mporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CL",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2721,deployability,instal,install,2721,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2735,deployability,build,build,2735,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2772,deployability,instal,install,2772,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2826,deployability,instal,install,2826,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2866,deployability,instal,install,2866,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2900,deployability,build,building,2900,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2936,deployability,instal,install,2936,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3034,deployability,releas,releases,3034,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3230,deployability,instal,install,3230,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3267,deployability,build,building,3267,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3371,deployability,INSTAL,INSTALL,3371,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3407,deployability,INSTAL,INSTALL,3407,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3438,deployability,INSTAL,INSTALL,3438,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1022,energy efficiency,cloud,cloud,1022,"pendency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1582,energy efficiency,cloud,cloud,1582," it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,integrability,depend,dependency,23,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1054,integrability,version,versions,1054,"iced, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1507,integrability,version,versions,1507,"s used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1960,integrability,version,version,1960,"nary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2741,integrability,sub,subversion,2741,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3164,integrability,configur,configure,3164,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:785,interoperability,compatib,compatible,785,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1330,interoperability,share,shared,1330,"ate so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2591,interoperability,platform,platform,2591,"com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar cz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,modifiability,depend,dependency,23,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1054,modifiability,version,versions,1054,"iced, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1295,modifiability,pac,packages,1295," out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ub",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1507,modifiability,version,versions,1507,"s used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1627,modifiability,pac,packages,1627,"toc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1960,modifiability,version,version,1960,"nary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3164,modifiability,configur,configure,3164,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3469,modifiability,pac,package,3469,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3673,modifiability,pac,package,3673,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:471,performance,content,content,471,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1111,performance,time,time,1111,"and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1212,performance,time,time,1212," that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:8,safety,updat,update,8,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,safety,depend,dependency,23,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:382,safety,sanit,sanity,382,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1707,safety,updat,update,1707,"hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:8,security,updat,update,8,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:382,security,sanit,sanity,382,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:579,security,modif,modify,579,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1707,security,updat,update,1707,"hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2177,security,apt,apt-get,2177,"g and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2334,security,apt,apt-get,2334,"rotobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2511,security,apt,apt-get,2511," that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2710,security,apt,apt-get,2710,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2761,security,apt,apt-get,2761,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2928,security,apt,apt-get,2928,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:3164,security,configur,configure,3164,"date when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz. tar xvzf protobuf-cpp-3.4.1.tar.gz. (cd protobuf-3.4.1 &&. ./autogen.sh &&. ./configure &&. make -j 32 &&. make -j 32 check &&. sudo make -j 32 install &&. sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git. sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh. sed -i 's/-j 2//g' clif/INSTALL.sh. (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;. sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;. tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}"". . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,testability,depend,dependency,23,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:283,testability,plan,planning,283,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:448,usability,help,helpful,448,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:647,usability,resum,resumed,647,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:734,usability,help,helpful,734,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:854,usability,tool,tools,854,"Another update on CLIF dependency:. @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```. # Builds OSS CLIF binary for DeepVariant. #. # This script should be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2005,usability,support,support,2005," be run on a cloud VM. Known to work on some versions of. # Linux OS. #. # OSS CLIF takes a very long time to build (10+ minutes) since it needs to. # compile parts of clang and LLVM. To save this build time, we use this script. # to build CLIF, install it in /usr/local/clif, and then packages up. # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz. # called oss_clif.latest.tgz. #. # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant. # Various versions that we built and released can be found under:. # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif. #. # We do recognize that this should be temporary, and will update when there is. # an official solution from CLIF. # GitHub issues such as https://github.com/google/deepvariant/issues/29 has. # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version. # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian. if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then. export DV_PLATFORM=""ubuntu-16"". # For ubuntu 16 we install cmake. sudo -H apt-get -y install cmake. elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then. export DV_PLATFORM=""ubuntu-14"". # For ubuntu 14 we install cmake3. sudo -H apt-get -y install cmake3. elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then. export DV_PLATFORM=""debian"". # For recent debian, we install cmake. sudo -H apt-get -y install cmake. else. export DV_PLATFORM=""unknown"". exit ""unsupported platform"". fi. CLIF_DIR=/usr/local/clif. CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs. sudo -H apt-get -y install ninja-build subversion. sudo -H apt-get -y install virtualenv python-pip pkg-config. sudo -H pip install 'pyparsing>=2.2.0'. sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip. wget https://gi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:80,modifiability,pac,package,80,@pichuan numpy should not be a problem. We can pin the recipe the 1.14 for this package if its needed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:349,availability,down,downside,349,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:42,deployability,build,build,42,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:121,deployability,build,builds,121,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:183,deployability,build,build,183,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:189,deployability,instal,install,189,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:202,deployability,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:236,deployability,version,version,236,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:249,energy efficiency,reduc,reduced,249,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:202,integrability,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:236,integrability,version,version,236,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:372,interoperability,compatib,compatible,372,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:388,interoperability,share,shared,388,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:202,modifiability,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:236,modifiability,version,version,236,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:561,performance,synch,synchronize,561,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:82,safety,avoid,avoid,82,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:202,safety,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:223,safety,test,test,223,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:202,testability,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:223,testability,test,test,223,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:114,usability,custom,custom,114,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:640,usability,help,help,640,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:49,deployability,build,build,49,"Hi Brad,. just to confirm again:. if you want to build CLIF on your own, would something similar to:. https://github.com/google/deepvariant/issues/29#issuecomment-385130636. be useful for you? What is the requirement on your side? Is it for this to build on CentOS6? Is there something else? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:249,deployability,build,build,249,"Hi Brad,. just to confirm again:. if you want to build CLIF on your own, would something similar to:. https://github.com/google/deepvariant/issues/29#issuecomment-385130636. be useful for you? What is the requirement on your side? Is it for this to build on CentOS6? Is there something else? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:18,usability,confirm,confirm,18,"Hi Brad,. just to confirm again:. if you want to build CLIF on your own, would something similar to:. https://github.com/google/deepvariant/issues/29#issuecomment-385130636. be useful for you? What is the requirement on your side? Is it for this to build on CentOS6? Is there something else? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:50,deployability,build,build,50,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:122,deployability,build,build,122,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:185,deployability,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:198,deployability,instal,installable,198,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:185,integrability,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:73,interoperability,compatib,compatible,73,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,modifiability,portab,portability,144,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:185,modifiability,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:242,modifiability,pac,packages,242,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:185,safety,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:185,testability,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:366,usability,help,help,366,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,deployability,Updat,Update,0,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:266,deployability,releas,releases,266,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:326,deployability,build,build,326,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:351,deployability,updat,update,351,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,safety,Updat,Update,0,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:351,safety,updat,update,351,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:0,security,Updat,Update,0,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:351,security,updat,update,351,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:199,usability,tool,tools,199,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:370,usability,progress,progress,370,Update:. the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:. https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh. https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1600,availability,error,error,1600,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,deployability,updat,update,23,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:708,deployability,Instal,Install,708,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:737,deployability,instal,install,737,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:755,deployability,releas,release-SCL,755,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:777,deployability,instal,install,777,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1034,deployability,build,build,1034,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1082,deployability,build,build,1082,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:405,energy efficiency,cloud,cloud-platform,405,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:474,energy efficiency,cloud,cloud,474,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:411,interoperability,platform,platform,411,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1001,interoperability,share,shared,1001,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:860,modifiability,pac,packages,860,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:528,performance,disk,disk-size,528,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:551,performance,disk,disk-type,551,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1600,performance,error,error,1600,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,safety,updat,update,23,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1600,safety,error,error,1600,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:23,security,updat,update,23,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:56,security,hack,hacky,56,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:596,security,ssh,ssh,596,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:624,security,ssh,ssh,624,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:128,usability,usab,usable,128,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:353,usability,USER,USER,353,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:500,usability,custom,custom-,500,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:630,usability,USER,USER,630,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1600,usability,error,error,1600,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:. ```. # Get a machine. gcloud beta compute instances create ""${USER}-centos6"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-6"" --image-project ""centos-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \. --zone ""us-west1-b"". # ssh into it. gcloud compute ssh ${USER}-centos6 --zone us-west1-b. ```. ```. ##### On the GCE instance #####. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/. (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""). sudo ldconfig # Reload shared libraries. ```. (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:. ```. $ /usr/local/clif/bin/pyclif. usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]. [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]. [--prepend PREPEND] [--include_paths INCLUDE_PATHS]. [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]. [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]. input_filename. pyclif: error: too few arguments. ```. Please let me know once you have a chance to try it. CentOS 6 is tricky. It feels like everything is old :(. Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:542,availability,echo,echo,542,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:28,deployability,build,build,28,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:112,deployability,version,version,112,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:506,deployability,version,version,506,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:772,deployability,version,version,772,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:798,deployability,instal,installation,798,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:814,deployability,Build,Build,814,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:847,deployability,Build,Build,847,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:916,deployability,build,build,916,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:956,deployability,Build,Build,956,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1011,deployability,Build,Build,1011,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1043,deployability,Build,Build,1043,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1103,deployability,build,building,1103,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:112,integrability,version,version,112,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:506,integrability,version,version,506,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:772,integrability,version,version,772,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:112,modifiability,version,version,112,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:506,modifiability,version,version,506,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:772,modifiability,version,version,772,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:962,performance,time,time,962,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1017,performance,time,timestamp,1017,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1049,performance,time,timestamp,1049,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:661,usability,command,command,661,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:699,usability,command,command,699,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:. ```. $ ldd --version. ldd (GNU libc) 2.12. Copyright (C) 2010 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO. warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead. And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:. ```. $ /usr/local/bin/bazel version. Extracting Bazel installation... Build label: 0.11.0- (@non-git). Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar. Build time: Wed Nov 5 12:47:48 +50302 (1525237217268). Build timestamp: 1525237217268. Build timestamp as int: 1525237217268. ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:27,deployability,build,building,27,"Pichuan, when in doubt try building it statically - libc is not really that large a library.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:427,availability,state,state,427,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:725,availability,cluster,cluster,725,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1042,availability,error,error,1042,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1206,availability,servic,service,1206,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:143,deployability,build,build-and-test,143,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:217,deployability,configurat,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:725,deployability,cluster,cluster,725,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:995,deployability,configurat,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1206,deployability,servic,service,1206,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:788,energy efficiency,Cloud,Cloud,788,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:217,integrability,configur,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:427,integrability,state,state,427,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:995,integrability,configur,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1048,integrability,messag,messages,1048,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1206,integrability,servic,service,1206,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1214,integrability,compon,component,1214,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1048,interoperability,messag,messages,1048,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1214,interoperability,compon,component,1214,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:217,modifiability,configur,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:273,modifiability,pac,packages,273,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:378,modifiability,pac,package,378,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:515,modifiability,pac,package,515,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:944,modifiability,scenario,scenarios,944,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:995,modifiability,configur,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1177,modifiability,evolv,evolving,1177,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1206,modifiability,servic,service,1206,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1214,modifiability,compon,component,1214,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:850,performance,time,times,850,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1042,performance,error,error,1042,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:153,safety,test,test,153,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:863,safety,test,tested,863,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1042,safety,error,error,1042,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:217,security,configur,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:995,security,configur,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1058,security,control,control,1058,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:153,testability,test,test,153,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:863,testability,test,tested,863,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1058,testability,control,control,1058,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:132,usability,custom,customized,132,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:237,usability,user,users,237,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:472,usability,experien,experience,472,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:675,usability,experien,experience,675,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:897,usability,tool,tools,897,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1016,usability,help,helped,1016,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1042,usability,error,error,1042,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1082,usability,document,documentation,1082,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1227,usability,support,supporting,1227,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:793,availability,error,error,793,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:27,deployability,updat,update,27,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:72,deployability,updat,updated,72,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:84,deployability,build,building,84,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,deployability,depend,dependency,144,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:209,deployability,build,building,209,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:291,deployability,build,build,291,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:424,deployability,instal,install,424,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:443,deployability,build,build,443,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1219,deployability,build,build,1219,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1317,deployability,instal,install,1317,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:805,energy efficiency,load,loading,805,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,integrability,depend,dependency,144,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,interoperability,share,shared,564,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:813,interoperability,share,shared,813,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:864,interoperability,share,shared,864,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,modifiability,depend,dependency,144,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:793,performance,error,error,793,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:805,performance,load,loading,805,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:27,safety,updat,update,27,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:72,safety,updat,updated,72,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,safety,depend,dependency,144,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:515,safety,isol,isolated,515,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:793,safety,error,error,793,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:27,security,updat,update,27,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:72,security,updat,updated,72,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:515,security,iso,isolated,515,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1383,security,privil,privileges,1383,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:144,testability,depend,dependency,144,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:515,testability,isol,isolated,515,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:243,usability,progress,progress,243,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:352,usability,progress,progress,352,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:793,usability,error,error,793,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1347,usability,support,support,1347,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1361,usability,user,users,1361,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1440,usability,help,helping,1440,"Pi-Chuan;. Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:. ```. $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python. /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory. ```. and the python libraries included symlink to the system wide ones you built against:. ```. lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py. ```. I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:85,deployability,Instal,Install,85,@chapmanb Thanks for giving it a try. . Before I built I did something like:. ```. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. . Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:114,deployability,instal,install,114,@chapmanb Thanks for giving it a try. . Before I built I did something like:. ```. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. . Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:132,deployability,releas,release-SCL,132,@chapmanb Thanks for giving it a try. . Before I built I did something like:. ```. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. . Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:154,deployability,instal,install,154,@chapmanb Thanks for giving it a try. . Before I built I did something like:. ```. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. . Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:422,deployability,build,build,422,@chapmanb Thanks for giving it a try. . Before I built I did something like:. ```. # Install Python 2.7. sudo yum install -y centos-release-SCL. sudo yum install -y python27. source /opt/rh/python27/enable. ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. . Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:44,deployability,INSTAL,INSTALL,44,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:341,deployability,Instal,Install,341,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:372,deployability,instal,install,372,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:390,deployability,releas,release-SCL,390,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:414,deployability,instal,install,414,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:690,deployability,build,build,690,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:11,safety,compl,completely,11,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:11,security,compl,completely,11,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1018,security,auth,auth,1018,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:128,usability,user,user,128,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0. [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>. wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try. > Before I built I did something like:. >. > # Install Python 2.7. > sudo yum install -y centos-release-SCL. > sudo yum install -y python27. > source /opt/rh/python27/enable. >. > I think starting from there it just assumes python is in. > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can. > make it recognize python at any path. > Is there a convention that people use to build something so that they can. > point to other Python locations? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:45,deployability,build,building,45,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:418,deployability,build,build,418,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:357,interoperability,specif,specify,357,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:226,modifiability,pac,packages,226,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:37,performance,time,time,37,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:397,performance,time,time,397,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:424,performance,time,time,424,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:112,usability,user,user,112,"@mrovner Thanks Mike. That is at the time of building, correct? If I choose with a python interpreter, will the user (Brad) need to also have python at the same location? I already built one here for CentOS6: gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. But it seems like @chapmanb is having trouble using it. Ideally we'll be able to specify the location differently at run time than the one at build time. Do you think that's possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:36,deployability,INSTAL,INSTALL,36,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:62,deployability,build,building,62,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:268,deployability,modul,modules,268,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:295,deployability,build,build,295,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:559,deployability,build,building,559,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:980,deployability,build,build,980,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:10,interoperability,specif,specifying,10,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:916,interoperability,specif,specify,916,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:258,modifiability,extens,extension,258,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:268,modifiability,modul,modules,268,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:748,modifiability,pac,packages,748,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:548,performance,time,time,548,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:956,performance,time,time,956,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:986,performance,time,time,986,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:268,safety,modul,modules,268,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:330,security,control,controlled,330,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1281,security,auth,auth,1281,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:330,testability,control,controlled,330,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:107,usability,user,user,107,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:392,usability,tool,tool,392,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:626,usability,user,user,626,"Correct - specifying the Python for INSTALL is the Python for building CLIF. Is has _no_ connection to the user Python (they even can be Py2 and Py3 in. any combination). When using CLIF the default will be the same _version_ (2 or 3) for. generating Python extension modules source code as the build Python was but. even that is controlled with (presence or absence of) --py3 flag for CLIF. tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>. wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of. > building, correct? If I choose with a python interpreter, will the user. > (Brad) need to also have python at the same location? > I already built one here for CentOS6:. > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz. > But it seems like @chapmanb <https://github.com/chapmanb> is having. > trouble using it. > Ideally we'll be able to specify the location differently at run time than. > the one at build time. Do you think that's possible? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:228,availability,error,error,228,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,availability,ERROR,ERROR,409,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:479,availability,ERROR,ERROR,479,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:762,availability,ERROR,ERROR,762,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1178,availability,down,download,1178,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:112,deployability,build,build,112,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:583,deployability,BUILD,BUILD,583,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:734,deployability,fail,failed,734,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:744,deployability,build,build,744,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:866,deployability,BUILD,BUILD,866,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1008,deployability,instal,installed,1008,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1047,deployability,instal,installing,1047,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1307,deployability,instal,install,1307,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1326,deployability,build,build,1326,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1427,deployability,build,build,1427,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1494,deployability,build,build,1494,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1548,deployability,build,build,1548,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:228,performance,error,error,228,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,performance,ERROR,ERROR,409,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:479,performance,ERROR,ERROR,479,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:762,performance,ERROR,ERROR,762,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:734,reliability,fail,failed,734,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:228,safety,error,error,228,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,safety,ERROR,ERROR,409,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:424,safety,input,input,424,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:479,safety,ERROR,ERROR,479,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:660,safety,input,input,660,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:762,safety,ERROR,ERROR,762,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:880,safety,input,input,880,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1104,security,sandbox,sandboxed,1104,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:215,testability,context,context,215,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1272,testability,understand,understanding,1272,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:55,usability,help,help,55,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:228,usability,error,error,228,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:386,usability,statu,status,386,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:409,usability,ERROR,ERROR,409,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:424,usability,input,input,424,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:479,usability,ERROR,ERROR,479,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:660,usability,input,input,660,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:762,usability,ERROR,ERROR,762,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:880,usability,input,input,880,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1384,usability,help,helpful,1384,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1599,usability,help,help,1599,"Pi-Chuan and Mike;. Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:. ```. (17:56:01) INFO: Found 1 target... (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. Target //deepvariant:binaries failed to build. (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. ```. which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:830,availability,error,error,830,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1018,availability,ERROR,ERROR,1018,"t is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1090,availability,ERROR,ERROR,1090,"hon programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1377,availability,ERROR,ERROR,1377," those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1800,availability,down,download,1800,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:243,deployability,build,build,243,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:362,deployability,build,build,362,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,deployability,build,build,711,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1194,deployability,BUILD,BUILD,1194,"rs for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build agai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1347,deployability,fail,failed,1347,"nt from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1357,deployability,build,build,1357,"e build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1481,deployability,BUILD,BUILD,1481,"flect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://githu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1626,deployability,instal,installed,1626,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1665,deployability,instal,installing,1665,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1937,deployability,instal,install,1937,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1956,deployability,build,build,1956,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2063,deployability,build,build,2063,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2133,deployability,build,build,2133,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2187,deployability,build,build,2187,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:830,performance,error,error,830,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1018,performance,ERROR,ERROR,1018,"t is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1090,performance,ERROR,ERROR,1090,"hon programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1377,performance,ERROR,ERROR,1377," those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1347,reliability,fail,failed,1347,"nt from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:830,safety,error,error,830,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1018,safety,ERROR,ERROR,1018,"t is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1033,safety,input,input,1033,"of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternativ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1090,safety,ERROR,ERROR,1090,"hon programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1271,safety,input,input,1271," option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1377,safety,ERROR,ERROR,1377," those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1495,safety,input,input,1495,"t conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1723,security,sandbox,sandboxed,1723,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2516,security,auth,auth,2516,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:817,testability,context,context,817,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1902,testability,understand,understanding,1902,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:63,usability,help,helpful,63,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:106,usability,tool,tools,106,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:213,usability,user,user,213,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:310,usability,user,user,310,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:651,usability,help,help,651,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:830,usability,error,error,830,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:993,usability,statu,status,993,"I'm guessing that that is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1018,usability,ERROR,ERROR,1018,"t is an effect of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1033,usability,input,input,1033,"of Python PIP trying to be helpful. CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternativ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1090,usability,ERROR,ERROR,1090,"hon programs/tools (pyclif and pyclif_proto) which are. Python. PIP (with setup.py) creates tiny launchers for them for user. convenience, but encode build Python path and eg. --py3 option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1271,usability,input,input,1271," option into those. launchers. When user environment for CLIF use is different from the build environment. those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1377,usability,ERROR,ERROR,1377," those launchers are not correct anymore and needs to be removed/regenerated. or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1495,usability,input,input,1495,"t conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>. wrote:. > Pi-Chuan and Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2017,usability,help,helpful,2017,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2241,usability,help,help,2241,"nd Mike;. > Thanks for all this background and help. I'm trying to fit this into the. > conda recipe bazel build for DeepVariant but am not sure how to take. > advantage of using the local anaconda python in that context. The error I'm. > seeing is that bazel can't find pyclif_proto:. >. > (17:56:01) INFO: Found 1 target... > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt. > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'. > Target //deepvariant:binaries failed to build. > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist. >. > which I thought was triggered by the difficulty running pyclif without. > having the local python installed. It could also be due to not installing. > is in /usr/local/bin since I have to remain sandboxed in the work. > directory, but I did adjust the PATH to include the download location. >. > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either. > understanding how to handle a root install of the pre-build pyclif or. > tweaking to use the local python would be helpful. Alternatively, if you. > can already build DeepVariant on a CentOS6 system yourself I could use the. > pre-build binaries the way we're doing now, just with the build against an. > older glibc. Thanks again for the help with this. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>. > . >. -- . Thanks,. --Mike.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:80,energy efficiency,current,currently,80,"@chapmanb I've been spending most of the last 2 days on this, and unfortunately currently stuck at where you are as well:. https://gist.github.com/pichuan/c0d2e6cf59a0f5ae373054410e477b59. I might have to call it a day today and look at it again tomorrow...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:377,availability,error,error,377,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:458,availability,ERROR,ERROR,458,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
