id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/706:954,security,model,modelckpt-to-modelpb-savedmodel-format,954,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1085,security,model,model,1085," rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1135,security,validat,validate,1135,"rately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1161,security,model,model,1161,"a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1334,security,model,model,1334,"deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""chan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2468,security,model,model,2468,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2596,security,model,models,2596,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2666,security,model,model,2666,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2754,security,model,models,2754,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2831,security,model,model,2831,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2915,security,model,models,2915,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2989,security,model,model,2989,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3073,security,model,models,3073,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3147,security,model,model,3147,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3251,security,model,models,3251,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3328,security,model,model,3328,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1401,testability,automat,automatically,1401,"ould be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1786,testability,instrument,instrument,1786," - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:66,usability,behavi,behavior,66,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:199,usability,custom,custom,199,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:694,usability,guid,guide,694,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:719,usability,perform,performing,719,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1353,usability,perform,performing,1353,"case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3366,usability,help,helps,3366,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:170,deployability,contain,container,170,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:212,deployability,version,version,212,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:265,energy efficiency,model,model,265,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:212,integrability,version,version,212,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:300,interoperability,format,format,300,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:212,modifiability,version,version,212,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:244,modifiability,paramet,parameter,244,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:265,security,model,model,265,"HI @pgrosu ,. Thank you for your answer. This is exactly what I am looking for. I need to convert into `.pb` file because I'm running Deepvariant using NVIDIA parabricks container, which also include DeepVariant version 1.5.0 in it. There is a parameter to take in model file, but it has to be `.pb` format.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:515,deployability,resourc,resources,515,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:68,energy efficiency,model,model-file,68,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:258,energy efficiency,model,models-for-additional-gpus,258,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:331,energy efficiency,model,model,331,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:515,energy efficiency,resourc,resources,515,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1107,energy efficiency,model,model-file,1107,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:80,modifiability,paramet,parameter,80,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:299,modifiability,paramet,parameter,299,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1119,modifiability,paramet,parameter,1119,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:280,performance,gpu,gpus,280,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:515,performance,resourc,resources,515,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:515,safety,resourc,resources,515,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:938,safety,input,input,938,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:68,security,model,model-file,68,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:258,security,model,models-for-additional-gpus,258,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:331,security,model,model,331,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:503,security,team,teams,503,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1107,security,model,model-file,1107,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:515,testability,resourc,resources,515,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1081,testability,plan,planning,1081,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:214,usability,document,documentation,214,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:228,usability,tool,tooldocs,228,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:938,usability,input,input,938,"Hi Sophie,. I'm not sure these are files you can use with the `--pb-model-file` parameter of ParaBricks, as shown here (in case that might be what you are pursuing):. https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_deepvariant.html#models-for-additional-gpus. I think that parameter stands for ParaBricks model files, not ProtoBuf serialized SavedModel files. For example, if you take one of the files from the compressed file here:. https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/resources/parabricks_deepvariant_models/files. You will notice by the following embedded string structures of each file how different they are:. #### *The ParaBricks `deepvariant.eng` file*. ```Bash. $ strings deepvariant.eng | head -n 10. ptrt. <Y&:;. <X%8<. ;SCh. fct=. s%>[j|? 8)=ql >. <x6X=Q. *s9=. #&>Q;. $. ```. #### *A TensorFlow `saved_model.pb` file*. ```Bash. $ strings saved_model.pb | head -n 10. AddV2. type:. input. reduction_indices"". Tidx. output. keep_dims. bool. Tidx. type. $. ```. They do not look like the same type of structured file. Were you planning to use the `--pb-model-file` parameter of ParaBricks with a TensorFlow SavedModel file? Thanks,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:368,availability,error,errors,368,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:387,availability,error,error,387,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:512,availability,error,error,512,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:523,availability,error,error,523,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:587,availability,error,error,587,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:518,deployability,log,log,518,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:529,deployability,log,log,529,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:593,deployability,log,log,593,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:450,integrability,filter,filter,450,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:88,modifiability,paramet,parameters,88,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:276,modifiability,paramet,parameters,276,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:368,performance,error,errors,368,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:387,performance,error,error,387,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:512,performance,error,error,512,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:523,performance,error,error,523,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:587,performance,error,error,587,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:478,reliability,doe,does,478,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:368,safety,error,errors,368,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:387,safety,error,error,387,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:410,safety,input,input,410,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:512,safety,error,error,512,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:518,safety,log,log,518,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:523,safety,error,error,523,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:529,safety,log,log,529,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:587,safety,error,error,587,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:593,safety,log,log,593,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:518,security,log,log,518,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:529,security,log,log,529,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:593,security,log,log,593,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:518,testability,log,log,518,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:529,testability,log,log,529,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:593,testability,log,log,593,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:126,usability,command,command,126,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:368,usability,error,errors,368,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:387,usability,error,error,387,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:410,usability,input,input,410,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:512,usability,error,error,512,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:523,usability,error,error,523,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:587,usability,error,error,587,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean? I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:122,energy efficiency,model,model,122,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:177,energy efficiency,model,model,177,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:496,interoperability,specif,specify,496,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:484,safety,reme,remember,484,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:122,security,model,model,122,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:177,security,model,model,177,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:306,usability,command,command,306,"@sophienguyen01 I wonder if this is because the number of channels you produced in make_examples is inconsistent with the model you used for call_variants. For WGS and WES, the model has 7 channels. So, when you run make_examples, you need to add `--channels=insert_size`. If you use the `run_deepvariant` command and use WGS or WES as the `--model_type`, this will be added for you directly in the make_examples step. However, if you're running make_examples separately, you need to remember to specify that on your own.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:74,modifiability,paramet,parameter,74,"Hi @pichuan , . That may be why because I didn't provide the `--channels` parameter. what is the default value for `insert_size` ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:254,usability,custom,custom-channels,254,The flag to add is literally `--channels=insert_size`. The code that added it in run_deepvariant is https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L245. You can see https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/ for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/707:281,availability,down,downsample,281,"Hi @crazysummerW,. That might be tricky as its deeply encoded in the pileup image construction - and how it's processed - which would would require some code-rewrite to extract it properly. If you have reads that don't span more than 95 for that specific variant position - or you downsample to 95 - you can reconstruct that from the BAM or DeepVariant realigned BAM files. . There might be other avenues I haven't thought about yet, though I'll keep thinking. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:246,interoperability,specif,specific,246,"Hi @crazysummerW,. That might be tricky as its deeply encoded in the pileup image construction - and how it's processed - which would would require some code-rewrite to extract it properly. If you have reads that don't span more than 95 for that specific variant position - or you downsample to 95 - you can reconstruct that from the BAM or DeepVariant realigned BAM files. . There might be other avenues I haven't thought about yet, though I'll keep thinking. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:19,energy efficiency,Current,Currently,19,"Hi @crazysummerW,. Currently we only report DP or depth for each allele/candidate variant. We do not report forward and reverse strand support for alleles. However, strand information is encoded in the example we create for each example that the neural network consumes. So intrinsically, it is considered during genotyping but not reported in the VCF file. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:253,performance,network,network,253,"Hi @crazysummerW,. Currently we only report DP or depth for each allele/candidate variant. We do not report forward and reverse strand support for alleles. However, strand information is encoded in the example we create for each example that the neural network consumes. So intrinsically, it is considered during genotyping but not reported in the VCF file. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:253,security,network,network,253,"Hi @crazysummerW,. Currently we only report DP or depth for each allele/candidate variant. We do not report forward and reverse strand support for alleles. However, strand information is encoded in the example we create for each example that the neural network consumes. So intrinsically, it is considered during genotyping but not reported in the VCF file. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:135,usability,support,support,135,"Hi @crazysummerW,. Currently we only report DP or depth for each allele/candidate variant. We do not report forward and reverse strand support for alleles. However, strand information is encoded in the example we create for each example that the neural network consumes. So intrinsically, it is considered during genotyping but not reported in the VCF file. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:368,usability,help,helps,368,"Hi @crazysummerW,. Currently we only report DP or depth for each allele/candidate variant. We do not report forward and reverse strand support for alleles. However, strand information is encoded in the example we create for each example that the neural network consumes. So intrinsically, it is considered during genotyping but not reported in the VCF file. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/708:137,deployability,Version,Version,137,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:137,integrability,Version,Version,137,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:95,interoperability,specif,specify,95,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:127,modifiability,variab,variable,127,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:137,modifiability,Version,Version,137,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:17,reliability,Doe,Does,17,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:169,testability,understand,understand,169,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:223,deployability,contain,contain,223,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:129,energy efficiency,Current,Currently,129,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:549,interoperability,platform,platforms,549,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:561,interoperability,platform,platformnames,561,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:575,modifiability,Pac,PacBio,575,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:457,safety,input,input,457,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:457,usability,input,input,457,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1363,usability,close,closer,1363,"Hi @genieusbio ,. If I recall correctly, there was one thing that I meant to fix in that tutorial but haven't gotten around to:. Currently, our force calling only works on biallelic entries. But regular DeepVariant VCF can contain multiallelic entries. So, one more addition step I would like to add is to at least split the proposed VCF into only biallelic entries. Although, in this case, it didn't seem to be a multallelic entry. I checked:. ```. $ zcat input/force_calling.candidates.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 50 PASS platforms=4;platformnames=PacBio,10X,Illumina,CG;datasets=6;datasetnames=CCS15kb_20kb,10XChromiumLR,HiSeqPE300x,CGnormal,HiSeq250x250,HiSeqMatePair;callsets=10;callsetnames=CCS15kb_20kbDV,10XLRGATK,CCS15kb_20kbGATK4,HiSeqPE300xSentieon,CGnormal,HiSeqPE300xfreebayes,HiSeq250x250Sentieon,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable;filt=CS_CGnormal_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=AJtrio-HG003.hg38.300x.bam.bilkentuniv.072319.dups,hg38.segdups_sorted_merged,lowmappabilityall GT:PS:DP:ADALL:AD:GQ 1/1:.:803:15,330:0,101:494. ```. Just from looking at the entry, without digging into it, I would think that this entry should show up at the end. I'll take a closer look (and run through my own gist again) see if I can figure out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:438,deployability,updat,update,438,"Hi Pi-Chuan and @genieusbio,. Try it with `--regions chr1:783006-783007` - you might see magic happen :). ```Bash. $ zcat output/HG003.output.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0:0:0,33,33. $ zcat output/HG003.output.g.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0,0:0,0:0,33,33,990,990,990. ```. My preference would be to update the BED file with that region. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:438,safety,updat,update,438,"Hi Pi-Chuan and @genieusbio,. Try it with `--regions chr1:783006-783007` - you might see magic happen :). ```Bash. $ zcat output/HG003.output.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0:0:0,33,33. $ zcat output/HG003.output.g.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0,0:0,0:0,33,33,990,990,990. ```. My preference would be to update the BED file with that region. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:438,security,updat,update,438,"Hi Pi-Chuan and @genieusbio,. Try it with `--regions chr1:783006-783007` - you might see magic happen :). ```Bash. $ zcat output/HG003.output.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0:0:0,33,33. $ zcat output/HG003.output.g.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0,0:0,0:0,33,33,990,990,990. ```. My preference would be to update the BED file with that region. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:415,usability,prefer,preference,415,"Hi Pi-Chuan and @genieusbio,. Try it with `--regions chr1:783006-783007` - you might see magic happen :). ```Bash. $ zcat output/HG003.output.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0:0:0,33,33. $ zcat output/HG003.output.g.vcf.gz | grep -P 'chr1\t783006\t'. chr1 783006 . A G,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0,0:0,0:0,33,33,990,990,990. ```. My preference would be to update the BED file with that region. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:154,performance,time,time,154,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:110,safety,input,input,110,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:162,safety,test,test,162,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:162,testability,test,test,162,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:110,usability,input,input,110,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:247,usability,confirm,confirm,247,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it? I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1254,integrability,coupl,couple,1254,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1254,modifiability,coupl,couple,1254,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1264,performance,time,times,1264,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:42,safety,valid,validation,42,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:205,safety,input,input,205,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:439,safety,input,input,439,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:603,safety,input,input,603,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:612,safety,input,input,612,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:853,safety,input,input,853,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:42,security,validat,validation,42,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1254,testability,coupl,couple,1254,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:205,usability,input,input,205,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:439,usability,input,input,439,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:603,usability,input,input,603,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:612,usability,input,input,612,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:853,usability,input,input,853,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \. -v ""${PWD}/input"":""/input"" \. -v ""${PWD}/output"":""/output"" \. -v ""${PWD}/reference"":""/reference"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions chr1:783006-783007 \. --output_vcf /output/HG003.output.vcf.gz \. --output_gvcf /output/HG003.output.g.vcf.gz \. --num_shards 1 \. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:118,deployability,log,logic,118,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1698,deployability,Continu,Continuing,1698,"CF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, examples, gvcfs, runtimes = region_processor.process(region). ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:412,modifiability,exten,extended,412,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:513,modifiability,exten,extend,513,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1009,modifiability,variab,variable,1009," Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, examples, gvcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:1064,performance,content,contents,1064,"ted output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, examples, gvcfs, runtimes = region_processor.process(region). ```. Ho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:118,safety,log,logic,118,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:105,security,control,control-flow,105,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:118,security,log,logic,118,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:105,testability,control,control-flow,105,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:118,testability,log,logic,118,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:446,usability,user,user-defined,446,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:2074,usability,help,helps,2074,"CF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python. options.calling_regions.extend(parse_regions_flag(flags_obj.regions)). ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python. regions = processing_regions_from_options(options). ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. options.exclude_calling_regions). ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```. regions = regions_to_process(. contigs=contigs,. partition_size=options.allele_counter_options.partition_size,. calling_regions=calling_regions,. task_id=options.task_id,. num_shards=options.num_shards). region_list = list(regions). ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python. for region in regions:. candidates, examples, gvcfs, runtimes = region_processor.process(region). ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:694,availability,down,downside,694,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:153,interoperability,specif,specified,153,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:265,interoperability,specif,specified,265,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:841,interoperability,specif,specify,841,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:490,reliability,doe,does,490,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:176,safety,input,input,176,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:342,safety,input,input,342,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:636,safety,input,input,636,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:601,testability,simpl,simply,601,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:176,usability,input,input,176,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:342,usability,input,input,342,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:601,usability,simpl,simply,601,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:636,usability,input,input,636,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:906,usability,clear,clear,906,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```. $ head -5 ./input/idt_capture_novogene.grch38.bed. chr1 69090 70008. chr1 450739 451678. chr1 685715 686654. chr1 925941 926013. chr1 930154 930336. ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/709:160,integrability,messag,message,160,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:160,interoperability,messag,message,160,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:229,testability,understand,understandable,229,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:173,usability,clear,clearly,173,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:186,usability,feedback,feedback,186,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:347,usability,help,helpful,347,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:80,safety,review,review,80,"Hello Kishwar (@kishwarshafin),. I followed your advice and have emailed you my review report. It took a little bit longer than I thought, as I also rewrote the algorithm section besides suggested corrections. If you think it would be more efficient, feel free to email me the LaTeX documents and figures. Let me know over email what you think of the report. Best regards,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:80,testability,review,review,80,"Hello Kishwar (@kishwarshafin),. I followed your advice and have emailed you my review report. It took a little bit longer than I thought, as I also rewrote the algorithm section besides suggested corrections. If you think it would be more efficient, feel free to email me the LaTeX documents and figures. Let me know over email what you think of the report. Best regards,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:240,usability,efficien,efficient,240,"Hello Kishwar (@kishwarshafin),. I followed your advice and have emailed you my review report. It took a little bit longer than I thought, as I also rewrote the algorithm section besides suggested corrections. If you think it would be more efficient, feel free to email me the LaTeX documents and figures. Let me know over email what you think of the report. Best regards,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:283,usability,document,documents,283,"Hello Kishwar (@kishwarshafin),. I followed your advice and have emailed you my review report. It took a little bit longer than I thought, as I also rewrote the algorithm section besides suggested corrections. If you think it would be more efficient, feel free to email me the LaTeX documents and figures. Let me know over email what you think of the report. Best regards,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/710:88,reliability,pra,practical,88,"Hi @Tonitsk8264 . We recommend DeepVariantWGS preset for merging HiFi and ONT data. The practical differences between the DeepVariantWGS and DeepVariantWES presets are small, and we generally find that the effects of different choices for GLnexus preset tends to be different only for some edge cases. In any case, for your use, DeepVariantWGS is the one to choose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/710:2,testability,understand,understand,2,I understand. Thank you for answering my questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/711:560,availability,down,download,560,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:612,availability,avail,available,612,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4892,availability,checkpoint,checkpoint,4892,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:493,deployability,instal,installations,493,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1300,deployability,contain,container,1300,"es that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1501,deployability,stage,stages,1501,"ns are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1783,deployability,contain,contains,1783,"//gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1979,deployability,contain,contains,1979,"model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2102,deployability,contain,contains,2102,"000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2314,deployability,contain,container,2314,"r.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:983,energy efficiency,model,model,983,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1012,energy efficiency,model,model,1012," Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1057,energy efficiency,model,model,1057,"t's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1081,energy efficiency,model,model,1081,"c yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfreco",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1148,energy efficiency,model,model,1148," details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1172,energy efficiency,model,model,1172,"s. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1225,energy efficiency,model,model,1225,"rating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1249,energy efficiency,model,model,1249,"ne callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1950,energy efficiency,optim,optimally,1950,"specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1997,energy efficiency,gpu,gpu,1997,"WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4567,energy efficiency,CPU,CPU,4567,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4576,energy efficiency,GPU,GPU,4576,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4714,energy efficiency,model,model,4714,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4904,energy efficiency,model,model,4904,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4978,energy efficiency,model,model-,4978,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5057,energy efficiency,CPU,CPU,5057,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5473,energy efficiency,GPU,GPU,5473,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:79,integrability,pub,public,79,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:603,integrability,pub,publicly,603,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:633,integrability,pub,public-data--broad-references,633,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:711,integrability,pub,public-data--broad-references,711,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:793,integrability,pub,public-data--broad-references,793,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:870,integrability,pub,public-data--broad-references,870,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:955,interoperability,specif,specific,955,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:3153,interoperability,format,format,3153,"ostprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4343,interoperability,format,format,4343," /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4935,modifiability,extens,extension,4935,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1641,performance,parallel,parallel,1641,"-broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1683,performance,parallel,parallelization,1683,"y38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1997,performance,gpu,gpu,1997,"WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2787,performance,disk,disk,2787,"ns a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --can",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4567,performance,CPU,CPU,4567,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4576,performance,GPU,GPU,4576,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4662,performance,disk,disk,4662,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5057,performance,CPU,CPU,5057,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5113,performance,Disk,Disk,5113,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5473,performance,GPU,GPU,5473,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:612,reliability,availab,available,612,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4892,reliability,checkpoint,checkpoint,4892,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:68,safety,compl,completely,68,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:612,safety,avail,available,612,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2599,safety,INPUT,INPUT,2599,"nome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_mi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2921,safety,avoid,avoid,2921,"taset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:3049,safety,input,input,3049,"call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_cov",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:3433,safety,input,input,3433,"\. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:68,security,compl,completely,68,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:170,security,access,access,170,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:612,security,availab,available,612,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:983,security,model,model,983,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1012,security,model,model,1012," Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1057,security,model,model,1057,"t's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1081,security,model,model,1081,"c yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfreco",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1148,security,model,model,1148," details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1172,security,model,model,1172,"s. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1225,security,model,model,1225,"rating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:1249,security,model,model,1249,"ne callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4714,security,model,model,4714,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4904,security,model,model,4904,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4978,security,model,model-,4978,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:285,usability,workflow,workflow,285,"Hi @lykstudio,. Ultima has a separate fork of deepvariant. It's not completely public yet. Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,. Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. . We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file. Official gatk and picard installations are required. . ### Files required for the analysis (download locally). The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict. gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files. DeepVariant model files . * WGS calling (model 1.2). ```. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index. gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta. ```. DeepVariant-ug docker container:. ```. gcr.io/ganymede-331016/deepvariant:ug-1.4.13. ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:2599,usability,INPUT,INPUT,2599,"nome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_mi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:3049,usability,input,input,3049,"call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals. ```. mkdir out && \. picard \. IntervalListTools \. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_cov",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:3433,usability,input,input,3433,"\. SCATTER_COUNT=200 \. SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \. UNIQUE=true \. SORT=true \. BREAK_BANDS_AT_MULTIPLES_OF=100000 \. INPUT=wgs_calling_regions.hg38.interval_list \. OUTPUT=out. ```. #### Further steps describe running on a single interval. Required hardware: . * 1CPU/shard, . * 2.5GB RAM/shard, . * hard disk: 50GB/shard+size_of_bam_files/num_shards. . Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```. gatk --java-options ""-Xms1G"" PrintReads \. -I {input_bam} \. -O input.bam \. -L {interval} \. -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format. gatk IntervalListToBed -I {interval} -O interval.bed. ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ~{ref_fasta} \. --reads input.bam \. --regions out/temp_0001_of_200/scattered.interval_list \. --examples interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:4727,usability,command,command,4727,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:5195,usability,command,command,5195,"les interval001.tfrecord.gz \. --min_base_quality 5 \. --dbg_min_base_quality 0 \. --vsc_min_fraction_indels 0.06 \. --vsc_min_fraction_hmer_indels 0.12 \. --vsc_min_fraction_snps 0.12 \. --vsc_min_count_snps 2 \. --ws_min_windows_distance 20 \. --min_mapping_quality 5 \. --candidate_min_mapping_quality 5 \. --max_reads_per_partition 1500 \. --aux_fields_to_keep tp,t0 \. --skip_bq_channel \. --channels hmer_deletion_quality,hmer_insertion_quality,non_hmer_insertion_quality \. --add_ins_size_channel \. --max_ins_size 10 \. --optimal_coverages 50 \. --p_error 0.005 \. --gvcf out_temp_0001_of_200_gvcf.tfrecords.gz \. ```. ### Run CallVariants. CallVariants step runs on all examples from all samples together. Before running CallVariants, all the output example files should be copied to . the same instance and renamed in the format `examples.tfrecord-%05d-of-%05d.gz`, for example: `examples.tfrecord-00041-of-00055.gz`. **The order of the file names should correspond to the genomic order and the first index is 00000**. . Required hardware: . * 4 CPU. * 1 GPU (nvidia-tesla-p100 or nvidia-tesla-v100 or nvidia-amper-a10). * 2.5GB RAM. * hard disk: 2 * total size of example + 2 * total size of model files. command:. ```. /opt/deepvariant/bin/call_variants \. --outfile call_variants_output.tfrecord.gz \. --examples examples.tfrecord@<fill total number of shards>.gz \. -checkpoint <model name, without the .index extension: deepvariant-ultima-germline-wgs-model-v1.2.ckpt-710000>. ```. ### Run PostProcessing:. Required hardware:. * 1 CPU. * RAM: 4GB+64*size of the output of CallVariant. * Disk: 4GB + 48*size of the output of CallVariants + size of the reference genome. command:. ```. /opt/deepvariant/bin/postprocess_variants \. --ref Homo_sapiens_assembly38.fasta \. --infile call_variants_output.tfrecord.gz \. --outfile output.vcf.gz \. --vcf_stats_report=False. ```. ## Final notes. * Running deepVariant in the docker requires ability to use GPU from the docker. See instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:52,security,team,team,52,Hi @lykstudio . Just to quickly comment. The Google team has worked closely with the Ultima team and @doron-st and we can endorse their solutions for Ultima data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:92,security,team,team,92,Hi @lykstudio . Just to quickly comment. The Google team has worked closely with the Ultima team and @doron-st and we can endorse their solutions for Ultima data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:68,usability,close,closely,68,Hi @lykstudio . Just to quickly comment. The Google team has worked closely with the Ultima team and @doron-st and we can endorse their solutions for Ultima data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/712:681,availability,slo,slower,681,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:39,deployability,releas,release,39,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:292,deployability,stage,stage,292,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:434,deployability,updat,update,434,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:642,deployability,manag,manage,642,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1281,deployability,releas,release,1281,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:642,energy efficiency,manag,manage,642,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1330,interoperability,specif,specify,1330,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:177,modifiability,Pac,PacBio,177,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:211,modifiability,Pac,PacBio,211,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:451,performance,improve perform,improve performance,451,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1478,performance,perform,performance,1478,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:681,reliability,slo,slower,681,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:434,safety,updat,update,434,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:642,safety,manag,manage,642,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:434,security,updat,update,434,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1029,security,modif,modify,1029,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:412,testability,coverag,coverages,412,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1212,testability,coverag,coverage,1212,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:23,usability,close,close,23,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:459,usability,perform,performance,459,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:1478,usability,perform,performance,1478,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/713:59,availability,error,error,59,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:370,deployability,depend,depending,370,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:370,integrability,depend,depending,370,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:18,modifiability,Pac,PacBio,18,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:370,modifiability,depend,depending,370,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:59,performance,error,error,59,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:59,safety,error,error,59,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:370,safety,depend,depending,370,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:370,testability,depend,depending,370,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:59,usability,error,error,59,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:63,energy efficiency,model,models,63,"Hi @Tonitsk8264 . One correction to my answer above. Those ONT models should be valid, but I had misremembered CLR. PEPPER-DeepVariant doesn't have a CLR mode to take advantage of that preset. However, you should use PEPPER-DeepVariant for ONT R9.4. We don't have a suggestion for CLR.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:135,reliability,doe,doesn,135,"Hi @Tonitsk8264 . One correction to my answer above. Those ONT models should be valid, but I had misremembered CLR. PEPPER-DeepVariant doesn't have a CLR mode to take advantage of that preset. However, you should use PEPPER-DeepVariant for ONT R9.4. We don't have a suggestion for CLR.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:80,safety,valid,valid,80,"Hi @Tonitsk8264 . One correction to my answer above. Those ONT models should be valid, but I had misremembered CLR. PEPPER-DeepVariant doesn't have a CLR mode to take advantage of that preset. However, you should use PEPPER-DeepVariant for ONT R9.4. We don't have a suggestion for CLR.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:63,security,model,models,63,"Hi @Tonitsk8264 . One correction to my answer above. Those ONT models should be valid, but I had misremembered CLR. PEPPER-DeepVariant doesn't have a CLR mode to take advantage of that preset. However, you should use PEPPER-DeepVariant for ONT R9.4. We don't have a suggestion for CLR.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:61,energy efficiency,schedul,schedule,61,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:61,performance,schedul,schedule,61,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:14,usability,clear,clearing,14,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/714:229,availability,state,stated,229,"Hi @mlin,. In the block of code your referenced earlier `first_record` represents a first record of a grouped positions (sharing the same quantized_gq value) therefore PL is set with the correct value. In the documentation it is stated `To minimize output file size, adjacent records with equal (or similar, see discussion below) genotype qualities are merged into a single record.`. I hope it addresses your comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:229,integrability,state,stated,229,"Hi @mlin,. In the block of code your referenced earlier `first_record` represents a first record of a grouped positions (sharing the same quantized_gq value) therefore PL is set with the correct value. In the documentation it is stated `To minimize output file size, adjacent records with equal (or similar, see discussion below) genotype qualities are merged into a single record.`. I hope it addresses your comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:209,usability,document,documentation,209,"Hi @mlin,. In the block of code your referenced earlier `first_record` represents a first record of a grouped positions (sharing the same quantized_gq value) therefore PL is set with the correct value. In the documentation it is stated `To minimize output file size, adjacent records with equal (or similar, see discussion below) genotype qualities are merged into a single record.`. I hope it addresses your comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:240,usability,minim,minimize,240,"Hi @mlin,. In the block of code your referenced earlier `first_record` represents a first record of a grouped positions (sharing the same quantized_gq value) therefore PL is set with the correct value. In the documentation it is stated `To minimize output file size, adjacent records with equal (or similar, see discussion below) genotype qualities are merged into a single record.`. I hope it addresses your comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:70,usability,help,helpful,70,"@akolesnikov Thanks, that's great! One further clarification would be helpful...in a reference band,. * `MIN_DP` = minimum read depth across covered positions. * `GQ` = lowest GQ across covered positions. * `PL` = ??? across covered positions (?). IOW how is info combined/selected from all the positions covered by the reference band, if indeed it is, to generate one representative PL vector?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:115,usability,minim,minimum,115,"@akolesnikov Thanks, that's great! One further clarification would be helpful...in a reference band,. * `MIN_DP` = minimum read depth across covered positions. * `GQ` = lowest GQ across covered positions. * `PL` = ??? across covered positions (?). IOW how is info combined/selected from all the positions covered by the reference band, if indeed it is, to generate one representative PL vector?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:30,usability,document,documentation,30,"Hi @mlin . We'll work on some documentation covering how these are calculated, give us a few weeks to get a first pass on those docs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:270,availability,consist,consistency,270,"@mlin FWIW your interpretation is correct -- it's just taking the PLs from the first position in the combinable block of the reference band, not doing any aggregation across the combinable sites. So it is semantically different from MIN_DP, MED_DP, and GQ. Arguably for consistency with GQ being from the minimum position, this should maybe take that position's PL vector as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:205,interoperability,semant,semantically,205,"@mlin FWIW your interpretation is correct -- it's just taking the PLs from the first position in the combinable block of the reference band, not doing any aggregation across the combinable sites. So it is semantically different from MIN_DP, MED_DP, and GQ. Arguably for consistency with GQ being from the minimum position, this should maybe take that position's PL vector as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:270,usability,consist,consistency,270,"@mlin FWIW your interpretation is correct -- it's just taking the PLs from the first position in the combinable block of the reference band, not doing any aggregation across the combinable sites. So it is semantically different from MIN_DP, MED_DP, and GQ. Arguably for consistency with GQ being from the minimum position, this should maybe take that position's PL vector as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:305,usability,minim,minimum,305,"@mlin FWIW your interpretation is correct -- it's just taking the PLs from the first position in the combinable block of the reference band, not doing any aggregation across the combinable sites. So it is semantically different from MIN_DP, MED_DP, and GQ. Arguably for consistency with GQ being from the minimum position, this should maybe take that position's PL vector as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:279,deployability,releas,release,279,"Hi all,. @akolesnikov has pushed a fix internally on Nov 3rd based on @cmclean 's suggestion in the previous comment. The change also includes a documentation change in deepvariant-gvcf-support.md. For now I'll close this issue. The change should come out officially in the next release. (@mlin if you'd like to see the change earlier, we can also push it to the `dev` branch. Let me know)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:145,usability,document,documentation,145,"Hi all,. @akolesnikov has pushed a fix internally on Nov 3rd based on @cmclean 's suggestion in the previous comment. The change also includes a documentation change in deepvariant-gvcf-support.md. For now I'll close this issue. The change should come out officially in the next release. (@mlin if you'd like to see the change earlier, we can also push it to the `dev` branch. Let me know)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:186,usability,support,support,186,"Hi all,. @akolesnikov has pushed a fix internally on Nov 3rd based on @cmclean 's suggestion in the previous comment. The change also includes a documentation change in deepvariant-gvcf-support.md. For now I'll close this issue. The change should come out officially in the next release. (@mlin if you'd like to see the change earlier, we can also push it to the `dev` branch. Let me know)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:211,usability,close,close,211,"Hi all,. @akolesnikov has pushed a fix internally on Nov 3rd based on @cmclean 's suggestion in the previous comment. The change also includes a documentation change in deepvariant-gvcf-support.md. For now I'll close this issue. The change should come out officially in the next release. (@mlin if you'd like to see the change earlier, we can also push it to the `dev` branch. Let me know)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/715:56,deployability,releas,release,56,"Hi @Tintest,. Our team is currently working on the next release of DeepVariant/DeepTrio which will support ONT R10 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/715:26,energy efficiency,current,currently,26,"Hi @Tintest,. Our team is currently working on the next release of DeepVariant/DeepTrio which will support ONT R10 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/715:18,security,team,team,18,"Hi @Tintest,. Our team is currently working on the next release of DeepVariant/DeepTrio which will support ONT R10 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/715:99,usability,support,support,99,"Hi @Tintest,. Our team is currently working on the next release of DeepVariant/DeepTrio which will support ONT R10 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/716:143,availability,reliab,reliable,143,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:203,availability,reliab,reliable,203,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:510,availability,checkpoint,checkpoint,510,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1233,availability,reliab,reliably,1233,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:799,deployability,stack,stack,799,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:561,energy efficiency,model,model,561,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:581,energy efficiency,GPU,GPU,581,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:590,energy efficiency,GPU,GPU,590,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:687,energy efficiency,current,current,687,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:708,energy efficiency,GPU,GPUs,708,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:728,energy efficiency,GPU,GPU,728,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:818,energy efficiency,current,current,818,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:835,energy efficiency,GPU,GPU,835,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1297,energy efficiency,model,model,1297,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:86,integrability,sub,substantial,86,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:301,integrability,sub,subclonal,301,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:956,modifiability,paramet,parameters,956,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:581,performance,GPU,GPU,581,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:590,performance,GPU,GPU,590,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:708,performance,GPU,GPUs,708,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:728,performance,GPU,GPU,728,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:835,performance,GPU,GPU,835,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:143,reliability,reliab,reliable,143,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:203,reliability,reliab,reliable,203,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:510,reliability,checkpoint,checkpoint,510,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:980,reliability,doe,doesn,980,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1233,reliability,reliab,reliably,1233,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:561,security,model,model,561,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1297,security,model,model,1297,"Hi @Axze-rgb . Conceptually, it is possible, and most of the challenge (which will be substantial) will be in constructing a truth set that is reliable enough, and in things like whether mapping will be reliable enough or the amount of variation is samples. Also, as I really you might be looking for subclonal variants instead of full germline variants. If this last piece is the case, then re-training with DeepVariant is likely not a good fit. In terms of hardware itself, when warmstarting training from a checkpoint, it's generally possible to train a new model with a single GPU. The GPU itself used in the case study is relatively old (a P100), several generations older than the current A100 or H100 GPUs. However, that GPU was designed for industry applications, I am not sure how it would stack up against a current consumer GPU. However, one thing to check - how many of your Illumina reads are MAPQ0 versus MAPQ 1-5 versus MAPQ5+. With default parameters, DeepVariant doesn't see Illumina reads with MAPQ <5. With long reads we were able to lower that threshold to MAPQ1, but if you're not getting many reads with more than MAPQ5 and especially if you're not getting reads more than MAPQ0, then the reads can't be mapped reliably enough for DeepVariant to attempt calling, whether the model is retrained or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:253,modifiability,evolv,evolved,253,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:221,security,sign,signal,221,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:416,testability,simpl,simple,416,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:416,usability,simpl,simple,416,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:96,energy efficiency,GPU,GPU,96,"Hi @Axze-rgb . OK, if that is the case, it's possible that re-training will help. If you have a GPU, hopefully you will be able to re-train. Please let me know if you have questions when you attempt that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:96,performance,GPU,GPU,96,"Hi @Axze-rgb . OK, if that is the case, it's possible that re-training will help. If you have a GPU, hopefully you will be able to re-train. Please let me know if you have questions when you attempt that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:76,usability,help,help,76,"Hi @Axze-rgb . OK, if that is the case, it's possible that re-training will help. If you have a GPU, hopefully you will be able to re-train. Please let me know if you have questions when you attempt that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/717:1885,availability,error,error,1885,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1917,availability,error,error,1917,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:7,deployability,manag,managed,7,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:346,deployability,log,login,346,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:594,deployability,FAIL,FAIL,594,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:774,deployability,modul,module,774,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:788,deployability,modul,module,788,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:810,deployability,modul,module,810,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2078,deployability,contain,containers,2078,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:7,energy efficiency,manag,managed,7,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:212,energy efficiency,adapt,adapt,212,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:737,energy efficiency,cpu,cpu,737,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:795,energy efficiency,load,load,795,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:817,energy efficiency,load,load,817,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:36,integrability,batch,batch,36,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:195,integrability,batch,batch,195,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:212,integrability,adapt,adapt,212,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:568,integrability,event,events,568,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:212,interoperability,adapt,adapt,212,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:212,modifiability,adapt,adapt,212,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:774,modifiability,modul,module,774,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:788,modifiability,modul,module,788,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:810,modifiability,modul,module,810,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1828,modifiability,interm,intermediateresults,1828,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:36,performance,batch,batch,36,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:195,performance,batch,batch,195,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:704,performance,time,time,704,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:737,performance,cpu,cpu,737,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:795,performance,load,load,795,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:800,performance,parallel,parallel,800,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:817,performance,load,load,817,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1885,performance,error,error,1885,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1917,performance,error,error,1917,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2010,performance,parallel,parallel,2010,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:594,reliability,FAIL,FAIL,594,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:7,safety,manag,managed,7,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:346,safety,log,login,346,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:774,safety,modul,module,774,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:788,safety,modul,module,788,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:810,safety,modul,module,810,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1885,safety,error,error,1885,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1917,safety,error,error,1917,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:346,security,log,login,346,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:346,testability,log,login,346,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:234,usability,efficien,efficient,234,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:621,usability,user,user,621,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1885,usability,error,error,1885,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1917,usability,error,error,1917,"n_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. #SBATCH --qos=maxjobs100. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs. HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed. OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:87,availability,error,error,87,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:15,deployability,updat,update,15,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:170,deployability,fail,failed,170,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:381,deployability,updat,update,381,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:55,integrability,messag,message,55,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:55,interoperability,messag,message,55,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:87,performance,error,error,87,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:170,reliability,fail,failed,170,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:15,safety,updat,update,15,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:87,safety,error,error,87,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:381,safety,updat,update,381,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:15,security,updat,update,15,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:381,security,updat,update,381,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:87,usability,error,error,87,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:333,usability,command,command,333,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory. ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:39,usability,person,person,39,"Actually, I realized that the original person who asked was @DineshRavindraRaju , and @amy-houseman was sharing your experience. . I'll reopen this so that @DineshRavindraRaju can either confirm whether you'll able to resolve it, or can ask more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:117,usability,experien,experience,117,"Actually, I realized that the original person who asked was @DineshRavindraRaju , and @amy-houseman was sharing your experience. . I'll reopen this so that @DineshRavindraRaju can either confirm whether you'll able to resolve it, or can ask more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:187,usability,confirm,confirm,187,"Actually, I realized that the original person who asked was @DineshRavindraRaju , and @amy-houseman was sharing your experience. . I'll reopen this so that @DineshRavindraRaju can either confirm whether you'll able to resolve it, or can ask more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/718:174,deployability,updat,update,174,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:201,deployability,version,version,201,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:242,deployability,version,version,242,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:277,deployability,updat,update,277,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:336,deployability,updat,update,336,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:458,deployability,version,version,458,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:201,integrability,version,version,201,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:242,integrability,version,version,242,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:458,integrability,version,version,458,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:194,modifiability,Pac,PacBio,194,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:201,modifiability,version,version,201,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:242,modifiability,version,version,242,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:317,modifiability,Pac,PacBio,317,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:458,modifiability,version,version,458,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:302,performance,time,time,302,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:174,safety,updat,update,174,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:277,safety,updat,update,277,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:336,safety,updat,update,336,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:174,security,updat,update,174,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:277,security,updat,update,277,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:336,security,updat,update,336,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:69,usability,command,command,69,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:108,usability,document,documentation,108,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/719:600,availability,error,error,600,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:479,integrability,filter,filtering,479,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:913,integrability,filter,filtering,913,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:737,interoperability,specif,specific,737,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:600,performance,error,error,600,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:1015,reliability,doe,does,1015,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:56,safety,input,input,56,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:600,safety,error,error,600,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:56,usability,input,input,56,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:132,usability,learn,learn,132,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:600,usability,error,error,600,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/720:300,performance,memor,memory,300,"Hi @crazysummerW ,. Can you tell us how much RAM your machine has? It is possible that after 100min, it has run OOM. (btw, r1.6 was out yesterday. We have introduced the built-in read phasing to DeepTrio, so I'd encourage you to try that! However, that will not solve your problem if it's related to memory.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:300,usability,memor,memory,300,"Hi @crazysummerW ,. Can you tell us how much RAM your machine has? It is possible that after 100min, it has run OOM. (btw, r1.6 was out yesterday. We have introduced the built-in read phasing to DeepTrio, so I'd encourage you to try that! However, that will not solve your problem if it's related to memory.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1027,availability,sli,slightly,1027,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:335,energy efficiency,alloc,allocate,335,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:849,integrability,pub,published,849,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:511,interoperability,share,share,511,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:94,modifiability,Pac,Pacbio-WGS,94,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:271,modifiability,Pac,Pacbio,271,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:540,modifiability,Pac,Pacbio-,540,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:963,modifiability,Pac,Pacbio-WGS,963,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:130,performance,perform,performed,130,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:319,performance,memor,memory,319,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:860,reliability,Doe,Does,860,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1027,reliability,sli,slightly,1027,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:52,safety,compl,completed,52,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:66,safety,test,testing,66,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:264,safety,test,test,264,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:571,safety,test,tested,571,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:52,security,compl,completed,52,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:66,testability,test,testing,66,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:264,testability,test,test,264,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:571,testability,test,tested,571,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:130,usability,perform,performed,130,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:228,usability,confirm,confirm,228,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:319,usability,memor,memory,319,"Hi @pichuan . Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1603,availability,sli,slightly,1603,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2100,availability,error,errors,2100,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2148,availability,error,error,2148,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:343,energy efficiency,alloc,allocate,343,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1871,energy efficiency,model,model-case-study,1871,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2212,energy efficiency,model,models,2212,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1298,integrability,pub,published,1298,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:491,interoperability,standard,standard-,491,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:567,interoperability,specif,specifically,567,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:841,interoperability,share,share,841,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:98,modifiability,Pac,Pacbio-WGS,98,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:279,modifiability,Pac,Pacbio,279,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:989,modifiability,Pac,Pacbio-,989,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1539,modifiability,Pac,Pacbio-WGS,1539,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1864,modifiability,pac,pacbio-model-case-study,1864,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:134,performance,perform,performed,134,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:327,performance,memor,memory,327,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:534,performance,memor,memory,534,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:608,performance,memor,memory,608,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:645,performance,memor,memory,645,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2100,performance,error,errors,2100,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2148,performance,error,error,2148,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1309,reliability,Doe,Does,1309,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1603,reliability,sli,slightly,1603,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:56,safety,compl,completed,56,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:70,safety,test,testing,70,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:272,safety,test,test,272,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1020,safety,test,tested,1020," for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2100,safety,error,errors,2100,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2148,safety,error,error,2148,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:56,security,compl,completed,56,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1871,security,model,model-case-study,1871,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2212,security,model,models,2212,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:70,testability,test,testing,70,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:272,testability,test,test,272,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1020,testability,test,tested,1020," for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:134,usability,perform,performed,134,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:232,usability,confirm,confirm,232,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:327,usability,memor,memory,327,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:442,usability,document,documentation,442,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:534,usability,memor,memory,534,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:608,usability,memor,memory,608,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:645,usability,memor,memory,645,"> Hi @pichuan Thank you for your response. > . > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2100,usability,error,errors,2100,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2148,usability,error,error,2148,"onfirm with you. > . > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering? > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? Please see:. https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6? I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1. The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:168,modifiability,Pac,PacBio,168,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:260,modifiability,Pac,PacBio,260,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:452,modifiability,pac,pacbio-case-study,452,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:490,safety,input,input,490,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:526,safety,input,input,526,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:591,safety,input,input,591,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:56,usability,document,documentation,56,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:380,usability,document,document,380,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:490,usability,input,input,490,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:526,usability,input,input,526,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:591,usability,input,input,591,"hi, @pichuan . Thanks for your reply. I noticed in your documentation(https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md) that the description of PacBio data analysis is as follows: 'In v1.6.0, we introduced read haplotagging in DeepTrio PacBio. You no longer need to run DeepVariant->WhatsHap->DeepTrio, and can just run DeepTrio once.' However, in another document (https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-pacbio-case-study.md), I saw that the input BAM file had phasing applied (input/HG002.pfda_challenge.grch38.phased.chr20.bam). So, for the input BAM file in DeepTrio 1.6, is it sufficient to use only the pbmm2 alignment? Will there be any other effects if a BAM file processed with pbmm2+WhatsHap is used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:145,deployability,log,logic,145,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:504,deployability,updat,update,504,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:117,interoperability,Specif,Specifically,117,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:145,safety,log,logic,145,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:504,safety,updat,update,504,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:145,security,log,logic,145,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:504,security,updat,update,504,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:145,testability,log,logic,145,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing. Specifically, this block of logic deals with that:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/721:285,deployability,releas,release,285,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:39,energy efficiency,model,model,39,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:269,energy efficiency,model,models,269,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:655,integrability,pub,public,655,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:506,interoperability,Standard,Standard,506,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:169,modifiability,Pac,PacBio,169,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:471,modifiability,Extens,Extensive,471,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:39,security,model,model,39,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:269,security,model,models,269,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:787,usability,document,documentation,787,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:812,usability,help,helps,812,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:5,usability,close,close,5,"I'll close this issue now. But if you have more questions, feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/722:18,availability,error,error,18,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:18,performance,error,error,18,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:18,safety,error,error,18,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:56,security,sandbox,sandbox,56,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:18,usability,error,error,18,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:170,energy efficiency,GPU,GPU,170,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:299,energy efficiency,GPU,GPU,299,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:389,energy efficiency,GPU,GPU,389,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:170,performance,GPU,GPU,170,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:299,performance,GPU,GPU,299,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:389,performance,GPU,GPU,389,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:360,usability,person,personally,360,"Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:518,deployability,modul,module,518,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:526,deployability,Version,Version,526,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:544,deployability,releas,released,544,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:178,energy efficiency,GPU,GPU,178,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:307,energy efficiency,GPU,GPU,307,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:397,energy efficiency,GPU,GPU,397,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:526,integrability,Version,Version,526,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:518,modifiability,modul,module,518,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:526,modifiability,Version,Version,526,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:178,performance,GPU,GPU,178,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:307,performance,GPU,GPU,307,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:397,performance,GPU,GPU,397,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:491,safety,test,tested,491,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:518,safety,modul,module,518,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:491,testability,test,tested,491,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:368,usability,person,personally,368,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:527,availability,error,error,527,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:824,availability,error,error,824,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1008,availability,error,error,1008," @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1192,availability,error,error,1192,"t calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1376,availability,error,error,1376,"tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1560,availability,error,error,1560, SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1744,availability,error,error,1744,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1928,availability,error,error,1928,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2112,availability,error,error,2112,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2296,availability,error,error,2296,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2480,availability,error,error,2480,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2664,availability,error,error,2664,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2848,availability,error,error,2848,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3032,availability,error,error,3032,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3216,availability,error,error,3216,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3400,availability,error,error,3400,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3584,availability,error,error,3584,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:507,deployability,version,version,507,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:631,deployability,Version,Version,631,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:178,energy efficiency,GPU,GPU,178,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:307,energy efficiency,GPU,GPU,307,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:397,energy efficiency,GPU,GPU,397,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:503,energy efficiency,GPU,GPU,503,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:507,integrability,version,version,507,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:631,integrability,Version,Version,631,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:507,modifiability,version,version,507,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:631,modifiability,Version,Version,631,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:178,performance,GPU,GPU,178,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:307,performance,GPU,GPU,307,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:397,performance,GPU,GPU,397,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:503,performance,GPU,GPU,503,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:527,performance,error,error,527,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:824,performance,error,error,824,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1008,performance,error,error,1008," @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1192,performance,error,error,1192,"t calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1376,performance,error,error,1376,"tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1560,performance,error,error,1560, SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1744,performance,error,error,1744,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1928,performance,error,error,1928,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2112,performance,error,error,2112,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2296,performance,error,error,2296,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2480,performance,error,error,2480,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2664,performance,error,error,2664,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2848,performance,error,error,2848,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3032,performance,error,error,3032,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3216,performance,error,error,3216,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3400,performance,error,error,3400,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3584,performance,error,error,3584,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:527,safety,error,error,527,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:824,safety,error,error,824,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1008,safety,error,error,1008," @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1192,safety,error,error,1192,"t calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1376,safety,error,error,1376,"tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1560,safety,error,error,1560, SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1744,safety,error,error,1744,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1928,safety,error,error,1928,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2112,safety,error,error,2112,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2296,safety,error,error,2296,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2480,safety,error,error,2480,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2664,safety,error,error,2664,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2848,safety,error,error,2848,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3032,safety,error,error,3032,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3216,safety,error,error,3216,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3400,safety,error,error,3400,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3584,safety,error,error,3584,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:368,usability,person,personally,368,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:527,usability,error,error,527,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:824,usability,error,error,824,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_exec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1008,usability,error,error,1008," @ZuyaoLiu for the question and the solution. We'll give it a try and improve this. > . > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1192,usability,error,error,1192,"t calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1376,usability,error,error,1376,"tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). Hi @pichuan ,. When I run the GPU version, I got this error mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1560,usability,error,error,1560, SNP calling and all the output files seemed fine. == CUDA ==. CUDA Version 11.3.1. 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1744,usability,error,error,1744,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1928,usability,error,error,1928,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2112,usability,error,error,2112,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2296,usability,error,error,2296,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2480,usability,error,error,2480,d not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2664,usability,error,error,2664,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2848,usability,error,error,2848,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3032,usability,error,error,3032,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3216,usability,error,error,3216,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3400,usability,error,error,3400,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3584,usability,error,error,3584,tream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea? Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:283,availability,error,error-while-running-on-gpu,283,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:120,energy efficiency,GPU,GPU,120,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:306,energy efficiency,gpu,gpu,306,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:440,energy efficiency,model,model,440,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:557,energy efficiency,GPU,GPU,557,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:756,energy efficiency,GPU,GPU,756,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:67,integrability,messag,message,67,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:332,integrability,messag,message,332,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:638,integrability,messag,messages,638,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:67,interoperability,messag,message,67,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:332,interoperability,messag,message,332,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:638,interoperability,messag,messages,638,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:120,performance,GPU,GPU,120,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:283,performance,error,error-while-running-on-gpu,283,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:557,performance,GPU,GPU,557,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:756,performance,GPU,GPU,756,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:22,safety,test,tested,22,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:283,safety,error,error-while-running-on-gpu,283,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:431,safety,test,test-the-model,431,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:440,security,model,model,440,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:22,testability,test,tested,22,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:431,testability,test,test-the-model,431,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:283,usability,error,error-while-running-on-gpu,283,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:475,usability,help,help,475,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:24,availability,error,error,24,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:71,energy efficiency,GPU,GPU,71,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:168,energy efficiency,GPU,GPU,168,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:340,energy efficiency,model,models,340,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:30,integrability,messag,messages,30,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:192,integrability,messag,messages,192,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:30,interoperability,messag,messages,30,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:192,interoperability,messag,messages,192,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:319,interoperability,specif,specific,319,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:24,performance,error,error,24,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:71,performance,GPU,GPU,71,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:168,performance,GPU,GPU,168,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:718,reliability,Doe,Does,718,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:24,safety,error,error,24,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:340,security,model,models,340,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:24,usability,error,error,24,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:599,usability,document,documents,599,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. . I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different. However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case? 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you! Zuyao.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:84,deployability,releas,release,84,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:92,energy efficiency,model,models,92,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:92,security,model,models,92,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:283,security,NIST,NIST,283,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:327,security,nist,nist,327,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:628,usability,help,helps,628,"Hi @ZuyaoLiu ,. In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle. We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:296,availability,error,error-while-running-on-gpu,296,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:127,energy efficiency,GPU,GPU,127,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:319,energy efficiency,gpu,gpu,319,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:459,energy efficiency,model,model,459,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:582,energy efficiency,GPU,GPU,582,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:787,energy efficiency,GPU,GPU,787,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:886,energy efficiency,model,model,886,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:965,energy efficiency,model,model,965,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:74,integrability,messag,message,74,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:351,integrability,messag,message,351,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:669,integrability,messag,messages,669,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:74,interoperability,messag,message,74,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:351,interoperability,messag,message,351,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:669,interoperability,messag,messages,669,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:127,performance,GPU,GPU,127,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:296,performance,error,error-while-running-on-gpu,296,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:582,performance,GPU,GPU,582,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:787,performance,GPU,GPU,787,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:29,safety,test,tested,29,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:296,safety,error,error-while-running-on-gpu,296,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:450,safety,test,test-the-model,450,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:855,safety,test,testing,855,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:459,security,model,model,459,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:886,security,model,model,886,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:965,security,model,model,965,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:29,testability,test,tested,29,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:450,testability,test,test-the-model,450,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:855,testability,test,testing,855,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:296,usability,error,error-while-running-on-gpu,296,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:500,usability,help,help,500,"> Hi @ZuyaoLiu. > . > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. > . > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. > . > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. > . > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not? > . > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)? > . > Thank you! Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:146,energy efficiency,GPU,GPU,146,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:404,energy efficiency,current,current,404,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:429,energy efficiency,GPU,GPU,429,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:177,integrability,messag,messages,177,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:177,interoperability,messag,messages,177,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:146,performance,GPU,GPU,146,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:429,performance,GPU,GPU,429,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:116,testability,understand,understanding,116,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:343,usability,close,close,343,"Hi @ZuyaoLiu ,. I was out so I didn't follow up in the past few weeks. Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/723:419,safety,input,input,419,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:567,safety,input,input,567,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:616,safety,input,input,616,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:589,testability,unit,unittest,589,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:419,usability,input,input,419,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:567,usability,input,input,567,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:616,usability,input,input,616,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:931,usability,help,helps,931,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:943,usability,close,close,943,"Hi @stefandiederich ,. You can try the `--only_keep_pass` flag for postprocess_variants. If you're using `run_deepvariant` to do things in one step, you can add:. ```. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. For example, if you're running [Quick Start](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md), you can run:. ```bash. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --postprocess_variants_extra_args=""only_keep_pass=true"". ```. Hope this helps. I'll close this issue, but feel free to reopen if needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:21,safety,test,test,21,Hi @pichuan . I will test that option. But sounds exactly like what I'm looking for. Thanks. Stefan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/723:21,testability,test,test,21,Hi @pichuan . I will test that option. But sounds exactly like what I'm looking for. Thanks. Stefan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/724:173,deployability,releas,release,173,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:23,energy efficiency,Current,Currently,23,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:90,energy efficiency,model,model,90,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:347,interoperability,share,share,347,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:242,performance,memor,memory,242,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:298,performance,memor,memory,298,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:90,security,model,model,90,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:53,usability,support,supported,53,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:110,usability,document,documentation,110,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:242,usability,memor,memory,242,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:274,usability,command,command,274,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:298,usability,memor,memory,298,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:513,availability,fault,fault,513,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:648,availability,down,downloaded,648,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:674,availability,down,downloads,674,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:702,availability,down,downloads,702,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:513,energy efficiency,fault,fault,513,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:554,interoperability,share,share,554,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:306,performance,memor,memory,306,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:362,performance,memor,memory,362,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:513,performance,fault,fault,513,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:513,reliability,fault,fault,513,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:513,safety,fault,fault,513,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:102,usability,support,supported,102,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:229,usability,support,supported,229,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:306,usability,memor,memory,306,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:338,usability,command,command,338,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:362,usability,memor,memory,362,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:383,usability,command,command,383,"hi @kishwarshafin ,. Thank you for your quick reply. I was under the impression that DeepTrio ONT was supported due to the final comment in https://github.com/google/deepvariant/issues/715. Good to know that it is not officially supported yet. > One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? The command is listed above. I've tried running with 6 threads/24GB RAM and 6 threads/128GB RAM. Both result in the same segmentation fault. > Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening. The .bams can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/_dev/github/deepvariant_724/). Thank you for looking into the issue,. Greetings,. @dennishendriksen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:287,availability,down,download,287,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:330,safety,input,input,330,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:403,safety,test,testdata,403,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:467,safety,input,input,467,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:568,safety,input,input,568,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:669,safety,input,input,669,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:770,safety,input,input,770,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:871,safety,input,input,871,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:972,safety,input,input,972,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:49,security,ident,identical,49,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:154,security,ident,identical,154,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:403,testability,test,testdata,403,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:330,usability,input,input,330,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:467,usability,input,input,467,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:568,usability,input,input,568,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:669,usability,input,input,669,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:770,usability,input,input,770,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:871,usability,input,input,871,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:972,usability,input,input,972,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:. ```bash. mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai. ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:102,deployability,version,version,102,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:102,integrability,version,version,102,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:102,modifiability,version,version,102,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:49,performance,memor,memory,49,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:12,usability,confirm,confirm,12,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:49,usability,memor,memory,49,"Also, I can confirm that your data caused out of memory (OOM) issue. Can you tell us which basecaller version you used to basecall your ONT data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:528,availability,avail,available,528,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:332,deployability,version,version,332,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:384,deployability,updat,update,384,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:332,integrability,version,version,332,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:332,modifiability,version,version,332,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:480,performance,time,time,480,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:528,reliability,availab,available,528,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:384,safety,updat,update,384,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:395,safety,test,test,395,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:528,safety,avail,available,528,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:384,security,updat,update,384,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:528,security,availab,available,528,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:395,testability,test,test,395,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:356,usability,minim,minimap,356,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:233,availability,error,error,233,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:170,energy efficiency,model,model,170,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:83,performance,time,time,83,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:233,performance,error,error,233,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:233,safety,error,error,233,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:170,security,model,model,170,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:32,testability,understand,understandable,32,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:196,testability,simpl,simplex,196,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:181,usability,support,supports,181,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:196,usability,simpl,simplex,196,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:233,usability,error,error,233,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. . 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/725:22,availability,error,error,22,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:53,availability,error,error,53,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:75,deployability,build,building,75,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:88,energy efficiency,model,model,88,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:22,performance,error,error,22,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:53,performance,error,error,53,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:22,safety,error,error,22,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:53,safety,error,error,53,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:88,security,model,model,88,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:22,usability,error,error,22,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:53,usability,error,error,53,"This looks like an OS error. Also, it looks like the error is raised while building the model graph `backbone = add_l2_regularizers`. Could it be that you ran out of RAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:241,availability,error,error,241,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:169,energy efficiency,model,model,169,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:205,energy efficiency,model,model,205,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:149,interoperability,specif,specified,149,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:175,modifiability,paramet,parameters,175,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:40,performance,memor,memory,40,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:241,performance,error,error,241,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:241,safety,error,error,241,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:169,security,model,model,169,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:205,security,model,model,205,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:40,usability,memor,memory,40,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:241,usability,error,error,241,"Hi, @akolesnikov . I have about 90GB of memory and I only analyzed chromosome 20. I had no problem running DV1.6 on the same server. However, when I specified to use T7 model parameters(--customized_model model/weights-51-0.995354.ckpt), an error occurred. What should I do about this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:135,deployability,build,build,135,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:27,energy efficiency,model,model,27,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:61,energy efficiency,model,model,61,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:145,energy efficiency,model,model,145,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:27,security,model,model,27,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:61,security,model,model,61,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:145,security,model,model,145,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:110,usability,command,command,110,@crazysummerW how was this model created? Did you follow the model training case study? Could you include the command line you used to build the model?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:302,availability,down,downloaded,302,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:50,deployability,releas,releases,50,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:100,energy efficiency,model,models,100,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:287,energy efficiency,model,model,287,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:120,safety,Compl,Complete,120,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:255,safety,compl,complete-,255,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:100,security,model,models,100,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:120,security,Compl,Complete,120,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:255,security,compl,complete-,255,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:287,security,model,model,287,"hi, @akolesnikov . In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies. I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`. The model file was downloaded from here:. ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:36,availability,error,error,36,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:517,availability,error,error,517,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:547,availability,error,error,547,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1139,availability,error,error,1139,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1243,availability,robust,robust,1243,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:582,deployability,log,logic,582,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:822,energy efficiency,model,model,822,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:298,integrability,wrap,wrapper,298,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:369,integrability,wrap,wrapper,369,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:523,integrability,messag,message,523,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:298,interoperability,wrapper,wrapper,298,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:369,interoperability,wrapper,wrapper,369,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:523,interoperability,messag,message,523,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:123,modifiability,pac,packages,123,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:36,performance,error,error,36,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:458,performance,synch,synchronously,458,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:495,performance,lock,lock,495,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:517,performance,error,error,517,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:547,performance,error,error,547,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1139,performance,error,error,1139,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1243,reliability,robust,robust,1243,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:36,safety,error,error,36,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:517,safety,error,error,517,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:534,safety,Input,Input,534,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:547,safety,error,error,547,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:582,safety,log,logic,582,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1043,safety,compl,complete-,1043,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1139,safety,error,error,1139,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1243,safety,robust,robust,1243,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:495,security,lock,lock,495,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:582,security,log,logic,582,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:822,security,model,model,822,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1043,security,compl,complete-,1043,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:582,testability,log,logic,582,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1182,testability,understand,understand,1182,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:36,usability,error,error,36,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:517,usability,error,error,517,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:534,usability,Input,Input,534,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:547,usability,error,error,547,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1139,usability,error,error,1139,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1164,usability,help,helpful,1164,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). ```. This is because this logic in our code writes a temp file:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```. tmp_weights_dir = tempfile.gettempdir(). tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'). model.save_weights(tmp_weights_path). ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file? I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:52,energy efficiency,CPU,CPU-only,52,"@pichuan . I tested the docker deepvariant:1.6 on a CPU-only machine. And I changed tmp dir:. ```. mkdir -p output/intermediate_results_dir. mkdir -p output/tmp_dir. export TMPDIR=""$PWD/output/tmp_dir"". ```. Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:52,performance,CPU,CPU-only,52,"@pichuan . I tested the docker deepvariant:1.6 on a CPU-only machine. And I changed tmp dir:. ```. mkdir -p output/intermediate_results_dir. mkdir -p output/tmp_dir. export TMPDIR=""$PWD/output/tmp_dir"". ```. Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:208,reliability,Doe,Does,208,"@pichuan . I tested the docker deepvariant:1.6 on a CPU-only machine. And I changed tmp dir:. ```. mkdir -p output/intermediate_results_dir. mkdir -p output/tmp_dir. export TMPDIR=""$PWD/output/tmp_dir"". ```. Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:13,safety,test,tested,13,"@pichuan . I tested the docker deepvariant:1.6 on a CPU-only machine. And I changed tmp dir:. ```. mkdir -p output/intermediate_results_dir. mkdir -p output/tmp_dir. export TMPDIR=""$PWD/output/tmp_dir"". ```. Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:13,testability,test,tested,13,"@pichuan . I tested the docker deepvariant:1.6 on a CPU-only machine. And I changed tmp dir:. ```. mkdir -p output/intermediate_results_dir. mkdir -p output/tmp_dir. export TMPDIR=""$PWD/output/tmp_dir"". ```. Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:159,availability,error,error,159,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:159,performance,error,error,159,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:159,safety,error,error,159,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:183,safety,avoid,avoided,183,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:139,testability,simul,simultaneously,139,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:397,testability,simpl,simple,397,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:159,usability,error,error,159,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:397,usability,simpl,simple,397,"@crazysummerW . I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir. If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:295,energy efficiency,Current,Currently,295,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:712,energy efficiency,CPU,CPUs,712,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:118,performance,time,time,118,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:683,performance,parallel,parallelize,683,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:712,performance,CPU,CPUs,712,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:538,safety,avoid,avoid,538,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:762,safety,sanit,sanity,762,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:762,security,sanit,sanity,762,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:784,usability,confirm,confirm,784,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is:. https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1002,availability,cluster,cluster,1002,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1342,availability,error,errors,1342,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1002,deployability,cluster,cluster,1002,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:307,energy efficiency,Current,Currently,307,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:729,energy efficiency,CPU,CPUs,729,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1032,energy efficiency,charg,charge,1032,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1157,energy efficiency,Current,Currently,1157,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:126,performance,time,time,126,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:700,performance,parallel,parallelize,700,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:729,performance,CPU,CPUs,729,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1342,performance,error,errors,1342,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:549,safety,avoid,avoid,549,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:785,safety,sanit,sanity,785,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1342,safety,error,errors,1342,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:785,security,sanit,sanity,785,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1280,testability,simul,simultaneously,1280,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:807,usability,confirm,confirm,807,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1342,usability,error,errors,1342,"> Hi @ZuyaoLiu ,. > . > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? > . > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue! > . > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. > . > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:128,security,access,access,128,"Hi @crazysummerW , so it seems like you might have a different issue. Is it possible that in your setting, you don't have write access to the directory that `tempfile.gettempdir()` gave you? I'll need more information from you to pinpoint the issue (because I can't reproduce it on my side yet). For example, on my machine:. ```bash. $ python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /tmp. $ export TMPDIR=${HOME}; python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /home/pichuan. ```. @crazysummerW , I wonder if it's possible that you don't have write access to your /tmp? If so, can you try setting `TMPDIR` to a directory that you have write access to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:591,security,access,access,591,"Hi @crazysummerW , so it seems like you might have a different issue. Is it possible that in your setting, you don't have write access to the directory that `tempfile.gettempdir()` gave you? I'll need more information from you to pinpoint the issue (because I can't reproduce it on my side yet). For example, on my machine:. ```bash. $ python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /tmp. $ export TMPDIR=${HOME}; python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /home/pichuan. ```. @crazysummerW , I wonder if it's possible that you don't have write access to your /tmp? If so, can you try setting `TMPDIR` to a directory that you have write access to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:683,security,access,access,683,"Hi @crazysummerW , so it seems like you might have a different issue. Is it possible that in your setting, you don't have write access to the directory that `tempfile.gettempdir()` gave you? I'll need more information from you to pinpoint the issue (because I can't reproduce it on my side yet). For example, on my machine:. ```bash. $ python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /tmp. $ export TMPDIR=${HOME}; python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'. /home/pichuan. ```. @crazysummerW , I wonder if it's possible that you don't have write access to your /tmp? If so, can you try setting `TMPDIR` to a directory that you have write access to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:97,deployability,updat,updates,97,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:226,deployability,updat,updates,226,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:97,safety,updat,updates,97,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:226,safety,updat,updates,226,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:97,security,updat,updates,97,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:226,security,updat,updates,226,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:128,usability,close,close,128,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/726:194,availability,replic,replicate,194,"Hi @ZuyaoLiu . The number of Mendelian violations does seem unusually high. Here, are you using DeepTrio or DeepVariant? Would you be willing/able to share the non-model organism trio for us to replicate the results and take a look? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:164,energy efficiency,model,model,164,"Hi @ZuyaoLiu . The number of Mendelian violations does seem unusually high. Here, are you using DeepTrio or DeepVariant? Would you be willing/able to share the non-model organism trio for us to replicate the results and take a look? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:150,interoperability,share,share,150,"Hi @ZuyaoLiu . The number of Mendelian violations does seem unusually high. Here, are you using DeepTrio or DeepVariant? Would you be willing/able to share the non-model organism trio for us to replicate the results and take a look? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:50,reliability,doe,does,50,"Hi @ZuyaoLiu . The number of Mendelian violations does seem unusually high. Here, are you using DeepTrio or DeepVariant? Would you be willing/able to share the non-model organism trio for us to replicate the results and take a look? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:164,security,model,model,164,"Hi @ZuyaoLiu . The number of Mendelian violations does seem unusually high. Here, are you using DeepTrio or DeepVariant? Would you be willing/able to share the non-model organism trio for us to replicate the results and take a look? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:351,performance,perform,performed,351,"> @ZuyaoLiu ,. > . > Can you please give some insight on how you merged the VCFs for the analysis? Would be great if you have something where we can look how the post-processing was done to generate the stats. Hi @kishwarshafin ,. Sure. I used GLnexus and default DeepvariantWGS settings to merge GVCFs from Deepvariant. And no further processing was performed. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:351,usability,perform,performed,351,"> @ZuyaoLiu ,. > . > Can you please give some insight on how you merged the VCFs for the analysis? Would be great if you have something where we can look how the post-processing was done to generate the stats. Hi @kishwarshafin ,. Sure. I used GLnexus and default DeepvariantWGS settings to merge GVCFs from Deepvariant. And no further processing was performed. Best,. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:294,integrability,filter,filter,294,"@ZuyaoLiu,. We are still evaluating your data. Meanwhile I tried to run the trio with DeepTrio 1.6 (which is an extension of DeepVariant) and as expected the results are much better. I got 0.27% Mendelian violation rate for your data. Another difference is that I used `DeepVariant_unfiltered` filter in GLNexus to merge VCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:112,modifiability,extens,extension,112,"@ZuyaoLiu,. We are still evaluating your data. Meanwhile I tried to run the trio with DeepTrio 1.6 (which is an extension of DeepVariant) and as expected the results are much better. I got 0.27% Mendelian violation rate for your data. Another difference is that I used `DeepVariant_unfiltered` filter in GLNexus to merge VCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:302,integrability,filter,filter,302,"> @ZuyaoLiu,. > . > We are still evaluating your data. Meanwhile I tried to run the trio with DeepTrio 1.6 (which is an extension of DeepVariant) and as expected the results are much better. I got 0.27% Mendelian violation rate for your data. Another difference is that I used `DeepVariant_unfiltered` filter in GLNexus to merge VCFs. @akolesnikov ,. Good to know. I used DV 1.5 to train my data and the Mendelian violation rate is 0.4%.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:120,modifiability,extens,extension,120,"> @ZuyaoLiu,. > . > We are still evaluating your data. Meanwhile I tried to run the trio with DeepTrio 1.6 (which is an extension of DeepVariant) and as expected the results are much better. I got 0.27% Mendelian violation rate for your data. Another difference is that I used `DeepVariant_unfiltered` filter in GLNexus to merge VCFs. @akolesnikov ,. Good to know. I used DV 1.5 to train my data and the Mendelian violation rate is 0.4%.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:562,availability,robust,robust,562,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:80,energy efficiency,model,model,80,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:319,energy efficiency,model,model,319,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:371,energy efficiency,model,model,371,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:541,energy efficiency,model,model,541,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:690,energy efficiency,model,model,690,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:775,interoperability,share,share,775,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:562,reliability,robust,robust,562,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:562,safety,robust,robust,562,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:80,security,model,model,80,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:319,security,model,model,319,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:371,security,model,model,371,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:488,security,team,team,488,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:541,security,model,model,541,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:690,security,model,model,690,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:19,usability,confirm,confirm,19,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:360,usability,custom,customized,360,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:625,usability,feedback,feedback,625,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:729,usability,close,close,729,"Hi @ZuyaoLiu ,. To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!! Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/727:48,deployability,instal,installed,48,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:74,deployability,version,version,74,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:157,deployability,version,version,157,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:230,deployability,instal,install,230,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:272,deployability,build,build,272,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:91,energy efficiency,current,currently,91,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:74,integrability,version,version,74,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:157,integrability,version,version,157,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:74,modifiability,version,version,74,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:157,modifiability,version,version,157,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:142,safety,test,tested,142,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:142,testability,test,tested,142,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:106,usability,support,support,106,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:50,deployability,instal,installed,50,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:76,deployability,version,version,76,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:159,deployability,version,version,159,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:232,deployability,instal,install,232,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:280,deployability,build,build,280,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:93,energy efficiency,current,currently,93,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:76,integrability,version,version,76,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:159,integrability,version,version,159,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:76,modifiability,version,version,76,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:159,modifiability,version,version,159,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:144,safety,test,tested,144,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:144,testability,test,tested,144,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:350,testability,plan,plans,350,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:108,usability,support,support,108,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:359,usability,support,support,359,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. > . > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:16,deployability,version,version,16,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:32,deployability,updat,updated,32,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:52,deployability,releas,release,52,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:99,deployability,releas,release,99,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:108,deployability,version,version,108,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:16,integrability,version,version,16,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:108,integrability,version,version,108,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:16,modifiability,version,version,16,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:108,modifiability,version,version,108,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:32,safety,updat,updated,32,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:32,security,updat,updated,32,@liambai Python version will be updated in the next release. . You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:57,deployability,releas,release,57,@pichuan Thanks so much! When do you plan to do the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:37,testability,plan,plan,37,@pichuan Thanks so much! When do you plan to do the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/728:336,deployability,observ,observe,336,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:207,energy efficiency,current,currently,207,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:336,testability,observ,observe,336,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:217,usability,support,support,217,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:389,usability,feedback,feedback,389,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:49,usability,command,command,49,"HI @danielecook , I took a look at DeepSomatic's command and it requires 2 BAM files: one from normal sample file and one from tumor sample file. I don't think I can run this program because I only have one BAM file for this type of sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:31,deployability,updat,update,31,"@sophienguyen01 thanks for the update. For now, I would recommend an alternative tumor-only caller. We may explore a tumor-only approach in the future. I will close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:31,safety,updat,update,31,"@sophienguyen01 thanks for the update. For now, I would recommend an alternative tumor-only caller. We may explore a tumor-only approach in the future. I will close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:31,security,updat,update,31,"@sophienguyen01 thanks for the update. For now, I would recommend an alternative tumor-only caller. We may explore a tumor-only approach in the future. I will close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:159,usability,close,close,159,"@sophienguyen01 thanks for the update. For now, I would recommend an alternative tumor-only caller. We may explore a tumor-only approach in the future. I will close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/729:45,energy efficiency,current,currently,45,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:86,energy efficiency,model,model,86,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:142,energy efficiency,model,model,142,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:182,energy efficiency,model,model,182,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:79,modifiability,Pac,PacBio,79,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:86,security,model,model,86,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:142,security,model,model,142,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:182,security,model,model,182,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:39,testability,plan,plans,39,"@tbenavi1 I do not believe we have any plans currently to develop a hybrid ONT/PacBio model. However, if there is a lot of interest in such a model we could consider training such a model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:197,deployability,pipelin,pipeline,197,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:443,energy efficiency,model,model,443,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:197,integrability,pipelin,pipeline,197,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:71,modifiability,Pac,PacBio,71,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:443,security,model,model,443,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/730:225,availability,error,error,225,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:111,deployability,build,build-test,111,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:225,performance,error,error,225,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:117,safety,test,test,117,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:225,safety,error,error,225,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:117,testability,test,test,117,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:225,usability,error,error,225,"Hello,. Did you follow the instructions here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md? It seems like you are missing distutils, please follow the instruction in the readme to see if the error persists.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/731:172,safety,input,input,172,"Hello,. You can use force calling for to achieve this. You can set the argument like:. ```bash. MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". ```. And provide it as:. ```bash. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"". ```. There's a full example [here](https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38) for v1.1.0, which you can similarly use for v1.6.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/731:172,usability,input,input,172,"Hello,. You can use force calling for to achieve this. You can set the argument like:. ```bash. MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". ```. And provide it as:. ```bash. --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"". ```. There's a full example [here](https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38) for v1.1.0, which you can similarly use for v1.6.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/731:56,safety,test,tested,56,My apologies. Thank you for the comment. I have not yet tested but it's looking like what I was after. I very much appreciate it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/731:56,testability,test,tested,56,My apologies. Thank you for the comment. I have not yet tested but it's looking like what I was after. I very much appreciate it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/732:392,integrability,filter,filter,392,"It looks like there is an insertion at the same position in the 5th read from the top, and most likely a soft clip in the 3d read from the top. Total Depth is Ref depth + all allele depths. In this case there 7 reads supporting ref, and 2 reads supporting the SNP. There are two more reads that support different alleles. Those different alleles are not reported because they didn't pass the filter. The allele is considered if the base quality is higher than a threshold and number of supporting reads is greater than 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:217,usability,support,supporting,217,"It looks like there is an insertion at the same position in the 5th read from the top, and most likely a soft clip in the 3d read from the top. Total Depth is Ref depth + all allele depths. In this case there 7 reads supporting ref, and 2 reads supporting the SNP. There are two more reads that support different alleles. Those different alleles are not reported because they didn't pass the filter. The allele is considered if the base quality is higher than a threshold and number of supporting reads is greater than 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:245,usability,support,supporting,245,"It looks like there is an insertion at the same position in the 5th read from the top, and most likely a soft clip in the 3d read from the top. Total Depth is Ref depth + all allele depths. In this case there 7 reads supporting ref, and 2 reads supporting the SNP. There are two more reads that support different alleles. Those different alleles are not reported because they didn't pass the filter. The allele is considered if the base quality is higher than a threshold and number of supporting reads is greater than 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:295,usability,support,support,295,"It looks like there is an insertion at the same position in the 5th read from the top, and most likely a soft clip in the 3d read from the top. Total Depth is Ref depth + all allele depths. In this case there 7 reads supporting ref, and 2 reads supporting the SNP. There are two more reads that support different alleles. Those different alleles are not reported because they didn't pass the filter. The allele is considered if the base quality is higher than a threshold and number of supporting reads is greater than 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:486,usability,support,supporting,486,"It looks like there is an insertion at the same position in the 5th read from the top, and most likely a soft clip in the 3d read from the top. Total Depth is Ref depth + all allele depths. In this case there 7 reads supporting ref, and 2 reads supporting the SNP. There are two more reads that support different alleles. Those different alleles are not reported because they didn't pass the filter. The allele is considered if the base quality is higher than a threshold and number of supporting reads is greater than 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:24,usability,close,close,24,You are welcome! I will close the issue. Feel free to reopen if needed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/733:4,availability,error,error,4,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:121,energy efficiency,model,model,121,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:62,integrability,Queue,Queue,62,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:262,integrability,Queue,Queue,262,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:4,performance,error,error,4,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:62,performance,Queue,Queue,62,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:262,performance,Queue,Queue,262,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:4,safety,error,error,4,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:95,safety,test,test,95,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:121,security,model,model,121,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:88,testability,simpl,simple,88,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:95,testability,test,test,95,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:4,usability,error,error,4,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:88,usability,simpl,simple,88,The error comes from the line `output_queue = multiprocessing.Queue()`. Could you try a simple test? . Run docker in CLI model: `docker run -it <DeepVariant image> bash`. Inside docker start Python3 and execute:. ```. import multiprocessing. q = multiprocessing.Queue(). ```. Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:322,safety,input,input,322,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:444,safety,input,input,444,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:495,safety,input,input,495,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:467,testability,unit,unittest,467,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:69,usability,support,supported,69,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:261,usability,command,command,261,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:322,usability,input,input,322,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:444,usability,input,input,444,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:495,usability,input,input,495,"Adding to @akolesnikov's comment. I believe DeepVariant is still not supported in `udocker`. I am unsure if `udocker` is causing the multiprocessing issue. Can you also try with:. ```bash. --postprocess_cpus 0 \. ```. to see if that resolves the issue. So your command would be:. ```bash. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --postprocess_cpus 0 \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:60,availability,error,error,60,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:421,availability,error,error,421,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1625,availability,down,downgrade,1625,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:788,deployability,modul,module,788,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1659,deployability,version,version,1659,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1981,deployability,releas,release,1981,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:174,energy efficiency,model,model,174,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:118,integrability,Queue,Queue,118,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:324,integrability,Queue,Queue,324,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:715,integrability,Queue,Queue,715,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:864,integrability,Queue,Queue,864,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:878,integrability,Queue,Queue,878,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:959,integrability,queue,queues,959,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1659,integrability,version,version,1659,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:788,modifiability,modul,module,788,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1659,modifiability,version,version,1659,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:60,performance,error,error,60,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:118,performance,Queue,Queue,118,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:324,performance,Queue,Queue,324,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:421,performance,error,error,421,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:432,performance,parallel,parallels,432,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:715,performance,Queue,Queue,715,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:864,performance,Queue,Queue,864,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:878,performance,Queue,Queue,878,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:959,performance,queue,queues,959,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1011,performance,Lock,Lock,1011,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1085,performance,Lock,Lock,1085,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1098,performance,Lock,Lock,1098,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1169,performance,synch,synchronize,1169,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1300,performance,synch,synchronize,1300,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:387,reliability,doe,doesn,387,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:60,safety,error,error,60,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:150,safety,test,test,150,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:421,safety,error,error,421,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:788,safety,modul,module,788,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:174,security,model,model,174,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:462,security,disclosur,disclosure,462,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1011,security,Lock,Lock,1011,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1085,security,Lock,Lock,1085,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1098,security,Lock,Lock,1098,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:143,testability,simpl,simple,143,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:150,testability,test,test,150,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:724,testability,Trace,Traceback,724,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:838,testability,context,context,838,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1060,testability,context,context,1060,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:60,usability,error,error,60,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:143,usability,simpl,simple,143,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:421,usability,error,error,421,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:598,usability,help,help,598,"Hi, . thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:. > . > ```. > import multiprocessing. > q = multiprocessing.Queue(). > ```. > . > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):. ```. Python 3.8.10 (default, May 26 2023, 14:05:08). [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import multiprocessing. >>> q = multiprocessing.Queue(). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! . Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:52,deployability,modul,module,52,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:77,deployability,depend,depends,77,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:77,integrability,depend,depends,77,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:36,interoperability,standard,standard,36,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:52,modifiability,modul,module,52,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:77,modifiability,depend,depends,77,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:52,safety,modul,module,52,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:77,safety,depend,depends,77,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:77,testability,depend,depends,77,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/734:363,modifiability,paramet,parameters,363,"Hi @JosephLalli . I notice one thing that may be of relevance, the flag should be `--keep_duplicates` with an underscore. I am not sure if that could explain your results. The information regarding duplicates should be stored in the FLAG field of the BAM, which does not need `parse-sam-aux-fields`. DeepVariant excludes duplicate reads by default, when no other parameters are supplied. Can you confirm that there are duplicate reads present in our BAM with the command `samtools view -f 1024 ${BAM}`, which should only produce reads marked as duplicates?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/734:262,reliability,doe,does,262,"Hi @JosephLalli . I notice one thing that may be of relevance, the flag should be `--keep_duplicates` with an underscore. I am not sure if that could explain your results. The information regarding duplicates should be stored in the FLAG field of the BAM, which does not need `parse-sam-aux-fields`. DeepVariant excludes duplicate reads by default, when no other parameters are supplied. Can you confirm that there are duplicate reads present in our BAM with the command `samtools view -f 1024 ${BAM}`, which should only produce reads marked as duplicates?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/734:396,usability,confirm,confirm,396,"Hi @JosephLalli . I notice one thing that may be of relevance, the flag should be `--keep_duplicates` with an underscore. I am not sure if that could explain your results. The information regarding duplicates should be stored in the FLAG field of the BAM, which does not need `parse-sam-aux-fields`. DeepVariant excludes duplicate reads by default, when no other parameters are supplied. Can you confirm that there are duplicate reads present in our BAM with the command `samtools view -f 1024 ${BAM}`, which should only produce reads marked as duplicates?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/734:463,usability,command,command,463,"Hi @JosephLalli . I notice one thing that may be of relevance, the flag should be `--keep_duplicates` with an underscore. I am not sure if that could explain your results. The information regarding duplicates should be stored in the FLAG field of the BAM, which does not need `parse-sam-aux-fields`. DeepVariant excludes duplicate reads by default, when no other parameters are supplied. Can you confirm that there are duplicate reads present in our BAM with the command `samtools view -f 1024 ${BAM}`, which should only produce reads marked as duplicates?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/735:194,availability,down,download,194,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:316,safety,test,testdata,316,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:376,safety,input,input,376,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:440,safety,input,input,440,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:316,testability,test,testdata,316,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:69,usability,command,command,69,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:233,usability,command,command,233,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:376,usability,input,input,376,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:440,usability,input,input,440,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:486,usability,command,command,486,"Hi @mallikag9 . I don't see a BED file for the exome regions in your command. For exome, we typically restrict to the capture regions of the exome (sometimes with a 100bp pad region). . You can download the capture regions with this command:. ```. HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed. ```. and add `--regions /input/idt_capture_novogene.grch38.bed` to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:47,interoperability,specif,specify,47,"@mallikag9 ,. As @AndrewCarroll , if you don't specify the bed file then the DV runs on the entire genome which is the reason you are seeing the runtime being so high. If you see different behavior after applying the parameter, please reopen the issue with the command you used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:217,modifiability,paramet,parameter,217,"@mallikag9 ,. As @AndrewCarroll , if you don't specify the bed file then the DV runs on the entire genome which is the reason you are seeing the runtime being so high. If you see different behavior after applying the parameter, please reopen the issue with the command you used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:189,usability,behavi,behavior,189,"@mallikag9 ,. As @AndrewCarroll , if you don't specify the bed file then the DV runs on the entire genome which is the reason you are seeing the runtime being so high. If you see different behavior after applying the parameter, please reopen the issue with the command you used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:261,usability,command,command,261,"@mallikag9 ,. As @AndrewCarroll , if you don't specify the bed file then the DV runs on the entire genome which is the reason you are seeing the runtime being so high. If you see different behavior after applying the parameter, please reopen the issue with the command you used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/736:132,availability,error,error,132,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:785,availability,error,error,785,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:101,deployability,log,log,101,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:110,deployability,observ,observation,110,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1413,deployability,releas,release,1413,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1460,deployability,releas,releases,1460,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1833,deployability,instal,install,1833,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1892,deployability,instal,install,1892,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1915,deployability,updat,update,1915,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:921,energy efficiency,cpu,cpu,921,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1774,energy efficiency,model,model-case-study,1774,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:87,interoperability,share,share,87,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:967,interoperability,share,share,967,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1390,modifiability,pac,packaged,1390,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1767,modifiability,pac,pacbio-model-case-study,1767,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:132,performance,error,error,132,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:785,performance,error,error,785,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:921,performance,cpu,cpu,921,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:101,safety,log,log,101,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:132,safety,error,error,132,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:785,safety,error,error,785,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1817,safety,test,test,1817,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1915,safety,updat,update,1915,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:101,security,log,log,101,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1593,security,team,team,1593,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1774,security,model,model-case-study,1774,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1915,security,updat,update,1915,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:101,testability,log,log,101,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:110,testability,observ,observation,110,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:526,testability,hook,hook,526,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1817,testability,test,test,1817,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:132,usability,error,error,132,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:785,usability,error,error,785,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1576,usability,support,supported,1576,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1977,usability,support,support,1977,"elafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:. It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8? ---. Here is what I tried. On a GCE instance, I ran:. ```bash. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. Then, I ran:. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```. (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/. call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip. call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh. deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip. freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip. ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:113,usability,close,close,113,"Hi @tgelafr-btx ,. If you have more follow-up after my last comment, please feel free to comment or reopen. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/737:400,availability,avail,available,400,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:164,deployability,build,build-test,164,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:233,deployability,build,build-prereq,233,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:292,deployability,build,build-prereq,292,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:333,deployability,log,log,333,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:369,deployability,log,log,369,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:393,deployability,log,log,393,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:400,reliability,availab,available,400,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:170,safety,test,test,170,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:333,safety,log,log,333,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:369,safety,log,log,369,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:393,safety,log,log,393,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:400,safety,avail,available,400,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:333,security,log,log,333,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:369,security,log,log,369,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:393,security,log,log,393,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:400,security,availab,available,400,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:170,testability,test,test,170,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:333,testability,log,log,333,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:369,testability,log,log,369,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:393,testability,log,log,393,"Hi @pioneer-pi ,. Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. ```bash. ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. ```. Then upload the `dv_build.log` here so a detailed log is available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:425,availability,avail,available,425,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1206,availability,state,state,1206,"run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1328,availability,state,state,1328," tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1584,availability,operat,operation,1584,", I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2286,availability,echo,echo,2286,"n discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2461,availability,echo,echo,2461,"u may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3449,availability,Error,Error,3449,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:172,deployability,build,build-test,172,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:241,deployability,build,build-prereq,241,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:309,deployability,build,build-prereq,309,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:350,deployability,log,log,350,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:394,deployability,log,log,394,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:418,deployability,log,log,418,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:501,deployability,contain,container,501,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:630,deployability,build,build,630,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:724,deployability,build,build-prereq,724,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1842,deployability,INSTAL,INSTALL,1842,"/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1868,deployability,INSTAL,INSTALL,1868,"lif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1945,deployability,build,build,1945,"/proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2324,deployability,build,build,2324," state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2365,deployability,build,build,2365,"switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2609,deployability,build,build,2609,"-. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2632,deployability,build,build,2632," by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgCo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3357,deployability,version,version,3357,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3392,deployability,modul,module,3392,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3480,deployability,Modul,Modules,3480,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3563,deployability,Stack,Stack,3563,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3618,deployability,Modul,Modules,3618,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3692,deployability,modul,modules,3692,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2108,energy efficiency,cpu,cpuinfo,2108,"d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2133,energy efficiency,cpu,cpuinfo,2133,"ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1206,integrability,state,state,1206,"run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1328,integrability,state,state,1328," tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1806,integrability,repositor,repository,1806,"f /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3357,integrability,version,version,3357,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3513,integrability,messag,message,3513,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:925,interoperability,Prox,ProxyChains-,925,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:949,interoperability,prox,proxychains,949,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1806,interoperability,repositor,repository,1806,"f /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3463,interoperability,share,share,3463,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3513,interoperability,messag,message,3513,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3601,interoperability,share,share,3601,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1654,modifiability,variab,variable,1654,"ng to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EX",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3357,modifiability,version,version,3357,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3392,modifiability,modul,module,3392,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3417,modifiability,pac,package,3417,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3480,modifiability,Modul,Modules,3480,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3535,modifiability,pac,package,3535,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3618,modifiability,Modul,Modules,3618,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3692,modifiability,modul,modules,3692,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2108,performance,cpu,cpuinfo,2108,"d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2133,performance,cpu,cpuinfo,2133,"ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3449,performance,Error,Error,3449,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:425,reliability,availab,available,425,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:178,safety,test,test,178,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:350,safety,log,log,350,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:394,safety,log,log,394,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:418,safety,log,log,418,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:425,safety,avail,available,425,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2902,safety,Detect,Detecting,2902,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2936,safety,Detect,Detecting,2936,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2977,safety,Detect,Detecting,2977,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3010,safety,Detect,Detecting,3010,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3157,safety,Detect,Detecting,3157,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3193,safety,Detect,Detecting,3193,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3236,safety,Detect,Detecting,3236,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3271,safety,Detect,Detecting,3271,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3392,safety,modul,module,3392,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3449,safety,Error,Error,3449,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3480,safety,Modul,Modules,3480,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3618,safety,Modul,Modules,3618,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3692,safety,modul,modules,3692,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:350,security,log,log,350,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:394,security,log,log,394,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:418,security,log,log,418,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:425,security,availab,available,425,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2720,security,ident,identification,2720," `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2769,security,ident,identification,2769,"t. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2902,security,Detect,Detecting,2902,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2936,security,Detect,Detecting,2936,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:2977,security,Detect,Detecting,2977,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3010,security,Detect,Detecting,3010,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3157,security,Detect,Detecting,3157,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3193,security,Detect,Detecting,3193,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3236,security,Detect,Detecting,3236,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3271,security,Detect,Detecting,3271,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:178,testability,test,test,178,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:350,testability,log,log,350,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:394,testability,log,log,394,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:418,testability,log,log,418,"> Hi @pioneer-pi ,. > . > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:. > . > ```shell. > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log. > ```. > . > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1519,usability,command,command,1519,"ker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1574,usability,undo,undo,1574,"nt(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + git init. Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ ''",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:3449,usability,Error,Error,3449,". Reinitialized existing Git repository in /root/clif/.git/. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:26,deployability,instal,installation,26,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:60,deployability,instal,installing,60,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:136,deployability,instal,install,136,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:179,deployability,contain,container,179,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:242,deployability,instal,installed,242,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:438,deployability,contain,container,438,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:. ```bash. /opt/deepvariant/bin/. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:34,deployability,instal,installation,34,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:68,deployability,instal,installing,68,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:144,deployability,instal,install,144,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:187,deployability,contain,container,187,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:250,deployability,instal,installed,250,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:452,deployability,contain,container,452,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:778,deployability,contain,container,778,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:834,deployability,build,build,834,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:869,deployability,build,build,869,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:888,deployability,contain,container,888,"> @pioneer-pi ,. > . > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. > . > If you are starting a container then DeepVariant binaries should be in:. > . > ```shell. > /opt/deepvariant/bin/. > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:180,deployability,build,build,180,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:261,deployability,contain,container,261,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:189,safety,test,test,189,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:170,security,modif,modify,170,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:189,testability,test,test,189,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/738:30,usability,close,close,30,Thanks @kishwarshafin. I will close the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/738
https://github.com/google/deepvariant/issues/739:64,deployability,log,log,64,"Hi @pioneer-pi ,. I'll try to reproduce on my side and paste my log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:64,safety,log,log,64,"Hi @pioneer-pi ,. I'll try to reproduce on my side and paste my log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:64,security,log,log,64,"Hi @pioneer-pi ,. I'll try to reproduce on my side and paste my log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:64,testability,log,log,64,"Hi @pioneer-pi ,. I'll try to reproduce on my side and paste my log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1660,availability,error,error,1660,"age-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3445,availability,error,error,3445,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:65,deployability,build,build,65,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:183,deployability,build,build-test,183,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:221,deployability,build,build,221,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:391,deployability,build,build-test,391,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1162,deployability,version,version,1162,"docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1216,deployability,log,log,1216,"to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1353,deployability,Updat,Update,1353,"variant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1516,deployability,build,build-test,1516,"d#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1562,deployability,build,build-prereq,1562,"d-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1617,deployability,log,log,1617,"e ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1715,deployability,build,build,1715,"-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1738,deployability,build,build,1738,"t ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2463,deployability,version,version,2463,"om/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2499,deployability,modul,module,2499,"s/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2537,deployability,version,version,2537," ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2569,deployability,modul,module,2569,"eq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2605,deployability,version,version,2605,"d at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2769,deployability,Fail,Failed,2769,"=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to unders",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3060,deployability,version,version,3060,"BI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3133,deployability,version,version,3133,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3223,deployability,version,version,3223,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3287,deployability,Build,Build,3287,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3332,deployability,build,build,3332,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3509,deployability,fail,failed,3509,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3551,deployability,fail,failed,3551,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3645,deployability,build,build,3645,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3676,deployability,build,build,3676,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3788,deployability,fail,failed,3788,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3980,deployability,instal,install,3980,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4047,deployability,build,build,4047,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:537,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,537,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:630,energy efficiency,cpu,cpu,630,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:672,energy efficiency,cloud,cloud-platform,672,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:754,energy efficiency,cloud,cloud,754,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:854,energy efficiency,cpu,cpu-platform,854,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:951,energy efficiency,cpu,cpu,951,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1193,energy efficiency,cpu,cpu,1193,"d. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3600,energy efficiency,cpu,cpu,3600,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3665,energy efficiency,cpu,cpu,3665,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1162,integrability,version,version,1162,"docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2463,integrability,version,version,2463,"om/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2537,integrability,version,version,2537," ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2605,integrability,version,version,2605,"d at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3060,integrability,version,version,3060,"BI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3133,integrability,version,version,3133,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3223,integrability,version,version,3223,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3246,integrability,Configur,Configuring,3246,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:570,interoperability,platform,platform,570,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:678,interoperability,platform,platform,678,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:783,interoperability,standard,standard-,783,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:858,interoperability,platform,platform,858,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1162,modifiability,version,version,1162,"docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2463,modifiability,version,version,2463,"om/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2499,modifiability,modul,module,2499,"s/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2537,modifiability,version,version,2537," ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2569,modifiability,modul,module,2569,"eq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2605,modifiability,version,version,2605,"d at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3060,modifiability,version,version,3060,"BI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3133,modifiability,version,version,3133,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3223,modifiability,version,version,3223,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3246,modifiability,Configur,Configuring,3246,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:537,performance,cpu,cpu-only-machine-on-google-cloud-platform,537,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:630,performance,cpu,cpu,630,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:806,performance,disk,disk-size,806,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:854,performance,cpu,cpu-platform,854,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:951,performance,cpu,cpu,951,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1193,performance,cpu,cpu,1193,"d. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1660,performance,error,error,1660,"age-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2683,performance,Performing Test,Performing Test,2683,". + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2727,performance,Performing Test,Performing Test,2727,"if/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3445,performance,error,error,3445,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3600,performance,cpu,cpu,3600,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3665,performance,cpu,cpu,3665,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2769,reliability,Fail,Failed,2769,"=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to unders",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3509,reliability,fail,failed,3509,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3551,reliability,fail,failed,3551,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3788,reliability,fail,failed,3788,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:189,safety,test,test,189,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:295,safety,reme,remember,295,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:397,safety,test,test,397,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1216,safety,log,log,1216,"to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1353,safety,Updat,Update,1353,"variant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1522,safety,test,test,1522,"ommand-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Fou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1617,safety,log,log,1617,"e ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1660,safety,error,error,1660,"age-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2008,safety,Detect,Detecting,2008,"riant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2042,safety,Detect,Detecting,2042," git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2083,safety,Detect,Detecting,2083,"variant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2116,safety,Detect,Detecting,2116," checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2263,safety,Detect,Detecting,2263,"9462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2299,safety,Detect,Detecting,2299,"ogle.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2342,safety,Detect,Detecting,2342,"0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2377,safety,Detect,Detecting,2377,"in-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2499,safety,modul,module,2499,"s/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2569,safety,modul,module,2569,"eq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2694,safety,Test,Test,2694,"kdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2738,safety,Test,Test,2738,"ild. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3445,safety,error,error,3445,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4029,safety,permiss,permission,4029,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4094,safety,permiss,permission,4094,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:891,security,ssh,sshed,891,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:939,security,ssh,ssh,939,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1216,security,log,log,1216,"to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1277,security,Auth,Author,1277,"ame , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1353,security,Updat,Update,1353,"variant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1617,security,log,log,1617,"e ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1826,security,ident,identification,1826,"us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1875,security,ident,identification,1875,". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2008,security,Detect,Detecting,2008,"riant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2042,security,Detect,Detecting,2042," git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2083,security,Detect,Detecting,2083,"variant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2116,security,Detect,Detecting,2116," checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2263,security,Detect,Detecting,2263,"9462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2299,security,Detect,Detecting,2299,"ogle.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2342,security,Detect,Detecting,2342,"0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2377,security,Detect,Detecting,2377,"in-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3246,security,Configur,Configuring,3246,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:189,testability,test,test,189,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:397,testability,test,test,397,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1216,testability,log,log,1216,"to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1522,testability,test,test,1522,"ommand-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Fou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1617,testability,log,log,1617,"e ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2694,testability,Test,Test,2694,"kdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2738,testability,Test,Test,2738,"ild. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3491,testability,understand,understand,3491,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3766,testability,understand,understand,3766,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:88,usability,document,documentation,88,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:433,usability,command,command,433,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:523,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,523,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:624,usability,USER,USER,624,"Hi @pioneer-pi , . Here is my attempt to reproduce the issue. To build from source, the documentation to use is this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- wo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1148,usability,confirm,confirmed,1148,"nt/blob/r1.6/docs/deepvariant-build-test.md. Given that you want to build 1.5, let me use the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1640,usability,close,close,1640,"pes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1660,usability,error,error,1660,"age-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into the machine. ```bash. gcloud compute ssh pichuan-cpu --zone us-west1-b. ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant/. git checkout r1.5. ```. And I confirmed the version:. ```. pichuan@pichuan-cpu:~/deepvariant$ git log | head. commit ab068c4588a02e2167051bd9e74c0c9579462b51. Author: pichuan <pichuan@google.com>. Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md. . PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. So I ran:. ```bash. sudo su. ./build-prereq.sh. ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2683,usability,Perform,Performing,2683,"```. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2727,usability,Perform,Performing,2727,"/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3445,usability,error,error,3445,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3879,usability,help,help,3879,"tures. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") . -- Checking for module 'protobuf'. -- Found protobuf, version 3.13.0. -- Checking for module 'libglog'. -- Found libglog, version 0.4.0. -- Looking for pthread.h. -- Looking for pthread.h - found. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD. -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed. -- Looking for pthread_create in pthreads. -- Looking for pthread_create in pthreads - not found. -- Looking for pthread_create in pthread. -- Looking for pthread_create in pthread - found. -- Found Threads: TRUE . -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") . -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") . -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") . -- Configuring done. -- Generating done. -- Build files have been written to: /root/clif/build. ```. which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash. root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build. root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:534,availability,Error,Error,534,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:102,deployability,contain,container,102,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:125,deployability,contain,container,125,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:169,deployability,build,build,169,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:216,deployability,build,build,216,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:357,deployability,build,build,357,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:477,deployability,modul,module,477,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:565,deployability,Modul,Modules,565,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:648,deployability,Stack,Stack,648,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:703,deployability,Modul,Modules,703,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:777,deployability,modul,modules,777,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:935,deployability,build,build-prereq,935,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:956,deployability,instal,installed,956,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:598,integrability,messag,message,598,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:548,interoperability,share,share,548,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:598,interoperability,messag,message,598,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:686,interoperability,share,share,686,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:477,modifiability,modul,module,477,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:502,modifiability,pac,package,502,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:565,modifiability,Modul,Modules,565,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:620,modifiability,pac,package,620,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:703,modifiability,Modul,Modules,703,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:777,modifiability,modul,modules,777,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:534,performance,Error,Error,534,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:477,safety,modul,module,477,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:534,safety,Error,Error,534,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:565,safety,Modul,Modules,565,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:703,safety,Modul,Modules,703,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:777,safety,modul,modules,777,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:534,usability,Error,Error,534,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). . I run the code:. ```. cd /root/clif/build. cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. ```. and the problem is also:. ```. -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```. I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:60,deployability,build,build,60,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:256,deployability,build,build,256,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:343,deployability,build,build,343,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:375,deployability,build,build,375,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:612,deployability,updat,update,612,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:602,interoperability,share,share,602,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:316,safety,permiss,permissions,316,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:428,safety,permiss,permissions,428,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:612,safety,updat,update,612,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:502,security,team,team,502,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:612,security,updat,update,612,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:159,usability,interact,interaction,159,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:223,usability,support,support,223,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:466,usability,help,help,466,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:511,usability,support,support,511,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:532,usability,close,close,532,"Hi @pioneer-pi ,. Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue. If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/740:14,deployability,Build,Building,14,"Hi @spz1st ,. Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:290,energy efficiency,model,model-case-study,290,"Hi @spz1st ,. Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:283,modifiability,pac,pacbio-model-case-study,283,"Hi @spz1st ,. Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:150,safety,permiss,permission,150,"Hi @spz1st ,. Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:290,security,model,model-case-study,290,"Hi @spz1st ,. Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:203,availability,avail,available,203,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:233,availability,avail,available,233,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:522,availability,operat,operations,522,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:589,availability,operat,operations,589,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:128,deployability,version,version,128,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:194,deployability,version,versions,194,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:243,deployability,version,versions,243,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:674,deployability,Build,Building,674,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:333,energy efficiency,core,core,333,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:400,energy efficiency,optim,optimized,400,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:481,energy efficiency,CPU,CPU,481,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:950,energy efficiency,model,model-case-study,950,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:97,integrability,messag,message,97,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:128,integrability,version,version,128,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:194,integrability,version,versions,194,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:243,integrability,version,versions,243,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:97,interoperability,messag,message,97,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:338,interoperability,platform,platform,338,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:128,modifiability,version,version,128,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:194,modifiability,version,versions,194,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:243,modifiability,version,versions,243,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:943,modifiability,pac,pacbio-model-case-study,943,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:400,performance,optimiz,optimized,400,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:434,performance,Network,Network,434,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:481,performance,CPU,CPU,481,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:501,performance,perform,performance-critical,501,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:203,reliability,availab,available,203,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:233,reliability,availab,available,233,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:203,safety,avail,available,203,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:233,safety,avail,available,233,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:810,safety,permiss,permission,810,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:203,security,availab,available,203,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:233,security,availab,available,233,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:434,security,Network,Network,434,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:950,security,model,model-case-study,950,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:286,usability,help,help,286,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:501,usability,perform,performance-critical,501,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]. This TensorFlow binary is optimized with oneAPI Deep Neural Network Library. (oneDNN) to use the following CPU instructions in performance-critical operations:. AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate comp. iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:324,availability,error,error,324,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:201,energy efficiency,optim,optimized,201,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:141,integrability,messag,message,141,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:330,integrability,messag,messages,330,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:141,interoperability,messag,message,141,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:330,interoperability,messag,messages,330,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:201,performance,optimiz,optimized,201,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:324,performance,error,error,324,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:83,reliability,doe,does,83,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:324,safety,error,error,324,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:324,usability,error,error,324,"Hi @spz1st ,. Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that? I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:114,availability,avail,available,114,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:105,deployability,version,versions,105,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:170,deployability,version,version,170,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:105,integrability,version,versions,105,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:170,integrability,version,version,170,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:153,interoperability,specif,specified,153,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:105,modifiability,version,versions,105,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:170,modifiability,version,version,170,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:114,reliability,availab,available,114,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:114,safety,avail,available,114,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:114,security,availab,available,114,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:220,usability,help,help,220,"HI @pichuan, the program worked fine and finished with results. I was just wondering if there were other versions available because the docker image was specified with a version number (1.6.0). Thanks again for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:94,deployability,version,version,94,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:129,deployability,version,versions,129,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:193,deployability,releas,releases,193,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:240,deployability,releas,releases,240,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:94,integrability,version,version,94,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:129,integrability,version,versions,129,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:94,modifiability,version,version,94,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:129,modifiability,version,versions,129,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:314,usability,close,close,314,"Hi @spz1st ,. Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use. There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:47,usability,close,close,47,"Hi @pichuan, thanks for the info. Yes, you can close the issue and I'll surely let you know if I have new questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/741:21,energy efficiency,optim,optimistic,21,"Might have been over optimistic. Some samples also appear to have issues with cram in v1.5, but these cram files use the cram3.1 format which was officially supported in samtools v1.18. Converting to default cram (`samtools view -o <sample>.v3.cram <sample>.v3.1.cram`) then seems to work, so may be an issue with deepvariant bundling with samtools v1.10 and having to convert the cram to bam internally. Trying to be a bit more systematic to get to the bottom of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:129,interoperability,format,format,129,"Might have been over optimistic. Some samples also appear to have issues with cram in v1.5, but these cram files use the cram3.1 format which was officially supported in samtools v1.18. Converting to default cram (`samtools view -o <sample>.v3.cram <sample>.v3.1.cram`) then seems to work, so may be an issue with deepvariant bundling with samtools v1.10 and having to convert the cram to bam internally. Trying to be a bit more systematic to get to the bottom of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:157,usability,support,supported,157,"Might have been over optimistic. Some samples also appear to have issues with cram in v1.5, but these cram files use the cram3.1 format which was officially supported in samtools v1.18. Converting to default cram (`samtools view -o <sample>.v3.cram <sample>.v3.1.cram`) then seems to work, so may be an issue with deepvariant bundling with samtools v1.10 and having to convert the cram to bam internally. Trying to be a bit more systematic to get to the bottom of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:101,deployability,contain,container,101,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,deployability,version,version,184,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:258,deployability,version,version,258,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:455,deployability,contain,container,455,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:594,deployability,contain,container,594,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,integrability,version,version,184,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:258,integrability,version,version,258,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,modifiability,version,version,184,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:258,modifiability,version,version,258,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:275,modifiability,deco,decoding,275,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:623,modifiability,deco,decode,623,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:35,reliability,doe,does,35,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:284,safety,except,except,284,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:166,usability,support,supported,166,"So converting the files to cram v3 does work with DV v1.6. It is strange though, because the default container uses samtools v1.10 but htslib v1.18, and cram v3.1 is supported in that version of htslib. Samtools itself seems fairly agnostic about the htslib version for cram decoding except for this bug maybe https://github.com/samtools/samtools/issues/1866. I was able to convert a cram v3.1 file that crashes instantly at _make\_examples_ *within* the container using samtools v1.10 to v3, and then can successfully run with that file. Maybe there is some issue in nucleus after all, as the container has the ability to decode cram v3.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:82,availability,error,error,82,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:123,deployability,updat,update,123,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:146,deployability,version,versions,146,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:146,integrability,version,versions,146,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:146,modifiability,version,versions,146,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:82,performance,error,error,82,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:47,safety,input,input,47,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:82,safety,error,error,82,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:123,safety,updat,update,123,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:277,safety,test,tests,277,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:123,security,updat,update,123,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:277,testability,test,tests,277,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:47,usability,input,input,47,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:82,usability,error,error,82,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:189,usability,behavi,behavior,189,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:213,usability,help,helpful,213,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:14,availability,sli,slice,14,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:263,availability,sli,slicing,263,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:330,availability,sli,slicing,330,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:523,availability,sli,slice,523,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:670,availability,slo,slow,670,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:737,availability,error,error,737,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:471,deployability,contain,container,471,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:481,deployability,version,version,481,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
