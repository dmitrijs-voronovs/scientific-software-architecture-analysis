id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/29:642,availability,error,error,642,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2330,availability,error,error,2330,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:266,deployability,build,build-prereq,266,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:555,deployability,BUILD,BUILD,555,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:613,deployability,fail,failed,613,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:634,deployability,fail,failed,634,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2614,deployability,FAIL,FAILED,2614,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2622,deployability,Build,Build,2622,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1675,integrability,pub,public,1675,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1732,integrability,pub,public,1732,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1977,integrability,pub,public,1977,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1873,interoperability,platform,platform,1873,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1908,interoperability,platform,platform,1908,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1941,interoperability,platform,platform,1941,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2033,interoperability,platform,platform,2033,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:997,modifiability,pac,packages,997,"K. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsyn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1146,modifiability,paramet,parameter,1146," sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAM",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:377,performance,error,error,377,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:458,performance,ERROR,ERROR,458,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:480,performance,cach,cache,480,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:642,performance,error,error,642,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:686,performance,cach,cache,686,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2330,performance,error,error,2330,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2567,performance,time,time,2567,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:613,reliability,fail,failed,613,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:634,reliability,fail,failed,634,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2614,reliability,FAIL,FAILED,2614,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:377,safety,error,error,377,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:431,safety,test,test,431,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:458,safety,ERROR,ERROR,458,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:642,safety,error,error,642,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2330,safety,error,error,2330,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2636,safety,compl,complete,2636,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2636,security,compl,complete,2636,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:431,testability,test,test,431,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:377,usability,error,error,377,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:458,usability,ERROR,ERROR,458,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:642,usability,error,error,642,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:658,usability,command,command,658,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:. ```. sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto. ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:. ```. (06:15:00) INFO: Found 80 targets and 33 test targets... (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1769,usability,tool,tools,1769,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2330,usability,error,error,2330,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2350,usability,command,command,2350,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2415,usability,command,command,2415,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:2494,usability,command,command,2494,"and. (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \. exec env - \. PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \. PWD=/proc/self/cwd \. PYTHON_BIN_PATH=/usr/local/bin/python2.7 \. PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \. TF_NEED_CUDA=0 \. TF_NEED_OPENCL_SYCL=0 \. /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o). cc1plus: error: unrecognized command line option ""-std=c++11"". cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized"". cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object"". (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s. (06:15:00) FAILED: Build did NOT complete successfully. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:105,availability,error,error,105,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:644,availability,down,down,644,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:16,deployability,instal,install,16,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:44,deployability,fail,failed,44,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:54,deployability,instal,install,54,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:166,deployability,version,version,166,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:359,deployability,version,version,359,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:558,deployability,instal,install,558,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:166,integrability,version,version,166,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:359,integrability,version,version,359,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,modifiability,concern,concerns,116,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:166,modifiability,version,version,166,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:240,modifiability,pac,packages,240,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:359,modifiability,version,version,359,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:105,performance,error,error,105,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:44,reliability,fail,failed,44,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:105,safety,error,error,105,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,testability,concern,concerns,116,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:105,usability,error,error,105,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:. ```. ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6. I did some search and found some old thread that could be relevant:. https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,deployability,depend,dependency,564,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,deployability,depend,dependency,711,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:832,deployability,instal,installing,832,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:863,deployability,build,build,863,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,integrability,depend,dependency,564,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,integrability,depend,dependency,711,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:582,interoperability,compatib,compatible,582,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,modifiability,depend,dependency,564,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,modifiability,depend,dependency,711,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:783,modifiability,pac,package,783,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,safety,depend,dependency,564,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,safety,depend,dependency,711,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:214,testability,understand,understand,214,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:564,testability,depend,dependency,564,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:711,testability,depend,dependency,711,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:897,usability,help,helping,897,"Pi-Chuan;. Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know? For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:669,availability,error,error,669,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:382,deployability,BUILD,BUILD,382,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:746,deployability,instal,installing,746,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:780,deployability,instal,install,780,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1086,energy efficiency,current,currently,1086,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:669,performance,error,error,669,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:625,safety,input,input,625,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:669,safety,error,error,669,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:625,usability,input,input,625,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:669,usability,error,error,669,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead). In the `WORKSPACE` file of DeepVariant, you can see this at the bottom. I tried changing the path:. ```. new_local_repository(. name = ""clif"",. build_file = ""third_party/clif.BUILD"",. path = ""/home/pichuan"",. ). ```. And I make sure the two files are there:. ```. $ ls /home/pichuan/clif/bin/. pyclif pyclif_proto. ```. After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:. ```. # Import tensorflow. Note path. local_repository(. name = ""org_tensorflow"",. path = ""../tensorflow"",. ). ```. So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:229,availability,avail,available,229,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:383,availability,error,errors,383,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:573,availability,error,error,573,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:589,availability,error,error,589,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:757,availability,ERROR,ERROR,757,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:168,deployability,build,build,168,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:644,deployability,BUILD,BUILD,644,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:696,deployability,continu,continue,696,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:851,deployability,BUILD,BUILD,851,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1337,deployability,build,build,1337,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1389,deployability,build,builds,1389,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1469,deployability,build,build,1469,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1522,deployability,depend,dependencies,1522,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1542,deployability,instal,installing,1542,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1522,integrability,depend,dependencies,1522,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:242,interoperability,standard,standard,242,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1522,modifiability,depend,dependencies,1522,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:383,performance,error,errors,383,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:573,performance,error,error,573,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:589,performance,error,error,589,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:757,performance,ERROR,ERROR,757,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:777,performance,cach,cache,777,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:229,reliability,availab,available,229,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:229,safety,avail,available,229,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:383,safety,error,errors,383,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:573,safety,error,error,573,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:589,safety,error,error,589,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:757,safety,ERROR,ERROR,757,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1522,safety,depend,dependencies,1522,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:229,security,availab,available,229,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:492,security,hack,hacking,492,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1558,security,apt,apt,1558,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1522,testability,depend,dependencies,1522,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:58,usability,tip,tip,58,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:383,usability,error,errors,383,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:573,usability,error,error,573,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:589,usability,error,error,589,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:757,usability,ERROR,ERROR,757,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1726,usability,tip,tips,1726,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:1778,usability,help,helping,1778,"Pi-Chuan;. Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:. ```. sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD. ```. but bazel is too smart and won't let us continue with non-bazel defined references:. ```. (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root. ```. So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:191,availability,error,errors,191,"@chapmanb . I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself. Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:50,deployability,build,build,50,"@chapmanb . I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself. Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:191,performance,error,errors,191,"@chapmanb . I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself. Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:191,safety,error,errors,191,"@chapmanb . I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself. Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:191,usability,error,errors,191,"@chapmanb . I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself. Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:252,deployability,depend,dependencies,252,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:333,deployability,build,build,333,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:354,deployability,build,build,354,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:592,deployability,build,build,592,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:718,deployability,build,build,718,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:509,energy efficiency,current,current,509,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:252,integrability,depend,dependencies,252,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:314,integrability,inject,inject,314,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:499,interoperability,share,share,499,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:252,modifiability,depend,dependencies,252,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:252,safety,depend,dependencies,252,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:140,security,hack,hacking,140,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:208,security,attack,attack,208,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:314,security,inject,inject,314,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:179,testability,understand,understanding,179,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:252,testability,depend,dependencies,252,"Pi-Chuan;. Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:40,deployability,build,building,40,"@chapmanb In terms of finding zlib when building with bazel, I wonder if things like these are useful:. https://github.com/tensorflow/tensorflow/issues/2536. https://github.com/bazelbuild/bazel/issues/1353. I wonder if directly asking on the bazel GitHub issues is the best way:. https://github.com/bazelbuild/bazel/issues. Since I'm not familiar with bazel, the only way I would be able to help is to probably repeat what you did and see if I can get unstuck. Do you mind asking on Bazel issues first?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:391,usability,help,help,391,"@chapmanb In terms of finding zlib when building with bazel, I wonder if things like these are useful:. https://github.com/tensorflow/tensorflow/issues/2536. https://github.com/bazelbuild/bazel/issues/1353. I wonder if directly asking on the bazel GitHub issues is the best way:. https://github.com/bazelbuild/bazel/issues. Since I'm not familiar with bazel, the only way I would be able to help is to probably repeat what you did and see if I can get unstuck. Do you mind asking on Bazel issues first?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:341,availability,cluster,cluster,341,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:96,deployability,version,version,96,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:312,deployability,releas,release,312,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:341,deployability,cluster,cluster,341,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:330,energy efficiency,Core,Core,330,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:96,integrability,version,version,96,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:96,modifiability,version,version,96,"+1 for work being done. Thanks! I cannot use the binaries:. ```. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so). ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:124,modifiability,pac,package,124,"@bgruening , yes I mean the singularity stores. There is no deep variant in there. ;-(. I almost have clif built as a conda package, but its kind of hacky.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:149,security,hack,hacky,149,"@bgruening , yes I mean the singularity stores. There is no deep variant in there. ;-(. I almost have clif built as a conda package, but its kind of hacky.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:84,deployability,build,building,84,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,deployability,depend,dependencies,116,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:139,deployability,build,build,139,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:230,deployability,build,build,230,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:236,deployability,infrastructur,infrastructure,236,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:347,deployability,fail,fails,347,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:503,deployability,build,build,503,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:560,deployability,build,build,560,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,integrability,depend,dependencies,116,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,modifiability,depend,dependencies,116,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:347,reliability,fail,fails,347,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,safety,depend,dependencies,116,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:201,security,modif,modify,201,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:416,security,ident,identify,416,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:116,testability,depend,dependencies,116,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:33,usability,help,help,33,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:452,usability,help,help,452,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:512,usability,support,support,512,"Jillian;. Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:51,deployability,build,builds,51,"@chapmanb , I might be able to help with the bazel builds, and if not I have some other talented folks around who could possibly be bribed. ;-) . Do you have a start on it somewhere?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:31,usability,help,help,31,"@chapmanb , I might be able to help with the bazel builds, and if not I have some other talented folks around who could possibly be bribed. ;-) . Do you have a start on it somewhere?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:290,safety,detect,detecting,290,"Jillian;. Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:183,security,hack,hacking,183,"Jillian;. Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:290,security,detect,detecting,290,"Jillian;. Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:391,usability,help,helping,391,"Jillian;. Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:193,deployability,continu,continue,193,"Hi all,. I know that this was not fully resolved, but by having it open forever also doesn't seen very effective. I'm going to close it for now. But please do feel free to comment here. I will continue to read and reply anything here. If there are suggestions on how to re-engage this effort, also feel to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:85,reliability,doe,doesn,85,"Hi all,. I know that this was not fully resolved, but by having it open forever also doesn't seen very effective. I'm going to close it for now. But please do feel free to comment here. I will continue to read and reply anything here. If there are suggestions on how to re-engage this effort, also feel to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:103,usability,effectiv,effective,103,"Hi all,. I know that this was not fully resolved, but by having it open forever also doesn't seen very effective. I'm going to close it for now. But please do feel free to comment here. I will continue to read and reply anything here. If there are suggestions on how to re-engage this effort, also feel to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/29:127,usability,close,close,127,"Hi all,. I know that this was not fully resolved, but by having it open forever also doesn't seen very effective. I'm going to close it for now. But please do feel free to comment here. I will continue to read and reply anything here. If there are suggestions on how to re-engage this effort, also feel to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29
https://github.com/google/deepvariant/issues/30:119,deployability,build,build-prereqs,119,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:146,deployability,instal,installing,146,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:376,deployability,instal,install,376,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:433,energy efficiency,cloud,cloud,433,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:162,modifiability,pac,packages,162,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:320,safety,test,tested,320,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:320,testability,test,tested,320,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:58,usability,clear,clear,58,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:231,usability,clear,clear,231,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:355,usability,confirm,confirm,355,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:197,deployability,instal,installs,197,"@depristo This might be happening because I'd aliased the ""python3"" command as ""python"", which is supposed to be for python 2.7 by default. Any idea as to how I could reset everything such that it installs only for python 2.7 (I've already removed the alias)? . EDIT: The closing and reopening of the issue was a mistake, please ignore.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/30:68,usability,command,command,68,"@depristo This might be happening because I'd aliased the ""python3"" command as ""python"", which is supposed to be for python 2.7 by default. Any idea as to how I could reset everything such that it installs only for python 2.7 (I've already removed the alias)? . EDIT: The closing and reopening of the issue was a mistake, please ignore.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30
https://github.com/google/deepvariant/issues/31:145,performance,time,time,145,Unfortunately DeepVariant only works with python 2.7 right now. We can look into making this work with Python 3 but it's likely to take a bit of time. How critical is it for you to use python 3? More information will help us prioritize python 3 support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/31
https://github.com/google/deepvariant/issues/31:217,usability,help,help,217,Unfortunately DeepVariant only works with python 2.7 right now. We can look into making this work with Python 3 but it's likely to take a bit of time. How critical is it for you to use python 3? More information will help us prioritize python 3 support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/31
https://github.com/google/deepvariant/issues/31:245,usability,support,support,245,Unfortunately DeepVariant only works with python 2.7 right now. We can look into making this work with Python 3 but it's likely to take a bit of time. How critical is it for you to use python 3? More information will help us prioritize python 3 support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/31
https://github.com/google/deepvariant/issues/32:78,deployability,fail,failed,78,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:88,deployability,build,build,88,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:78,reliability,fail,failed,78,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:11,usability,clear,clear,11,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:126,usability,custom,customized,126,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:37,deployability,instal,installed,37,It is customised. And Python 2.7 was installed using Anaconda.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:6,usability,custom,customised,6,It is customised. And Python 2.7 was installed using Anaconda.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:572,availability,error,error,572,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:668,availability,error,error,668,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:524,deployability,modul,module,524,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:765,deployability,modul,module,765,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:846,deployability,modul,module,846,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:946,deployability,modul,module,946,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1045,deployability,modul,module,1045,"e. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1163,deployability,modul,module,1163,"epvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""x",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1189,deployability,log,log,1189,"ls."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1282,deployability,modul,module,1282,"__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1408,deployability,modul,module,1408,"imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1540,deployability,modul,module,1540,"standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1679,deployability,modul,module,1679,">> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1814,deployability,modul,module,1814,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1955,deployability,modul,module,1955,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2117,deployability,modul,module,2117,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2262,deployability,modul,module,2262,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2412,deployability,modul,module,2412,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2599,deployability,modul,module,2599,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:69,energy efficiency,core,core,69,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:177,energy efficiency,core,core,177,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1836,energy efficiency,core,core,1836,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1923,energy efficiency,core,core,1923,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2543,energy efficiency,load,load,2543,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:543,interoperability,standard,standard,543,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2583,interoperability,standard,standard,2583,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:524,modifiability,modul,module,524,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:765,modifiability,modul,module,765,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:846,modifiability,modul,module,846,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:946,modifiability,modul,module,946,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1045,modifiability,modul,module,1045,"e. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1163,modifiability,modul,module,1163,"epvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""x",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1282,modifiability,modul,module,1282,"__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1365,modifiability,pac,packages,1365,"metools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1408,modifiability,modul,module,1408,"imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1495,modifiability,pac,packages,1495,"h, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1540,modifiability,modul,module,1540,"standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1634,modifiability,pac,packages,1634,"ame path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1679,modifiability,modul,module,1679,">> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1766,modifiability,pac,packages,1766,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1814,modifiability,modul,module,1814,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1908,modifiability,pac,packages,1908,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1955,modifiability,modul,module,1955,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2067,modifiability,pac,packages,2067,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2117,modifiability,modul,module,2117,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2140,modifiability,deco,decorators,2140,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2210,modifiability,pac,packages,2210,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2233,modifiability,deco,decorators,2233,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2262,modifiability,modul,module,2262,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2365,modifiability,pac,packages,2365,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2412,modifiability,modul,module,2412,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2599,modifiability,modul,module,2599,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:572,performance,error,error,572,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:668,performance,error,error,668,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2543,performance,load,load,2543,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2666,reliability,doe,does,2666,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:154,safety,Test,Tests,154,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:524,safety,modul,module,524,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:572,safety,error,error,572,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:582,safety,test,test,582,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:668,safety,error,error,668,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:765,safety,modul,module,765,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:846,safety,modul,module,846,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:946,safety,modul,module,946,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1045,safety,modul,module,1045,"e. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1163,safety,modul,module,1163,"epvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""x",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1189,safety,log,log,1189,"ls."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1282,safety,modul,module,1282,"__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1408,safety,modul,module,1408,"imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1540,safety,modul,module,1540,"standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1679,safety,modul,module,1679,">> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1814,safety,modul,module,1814,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1955,safety,modul,module,1955,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1975,safety,test,testing,1975,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2082,safety,test,testing,2082,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2117,safety,modul,module,2117,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2225,safety,test,testing,2225,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2262,safety,modul,module,2262,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2380,safety,test,testing,2380,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2412,safety,modul,module,2412,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2599,safety,modul,module,2599,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1189,security,log,log,1189,"ls."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:154,testability,Test,Tests,154,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:582,testability,test,test,582,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:603,testability,simpl,simply,603,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:701,testability,Trace,Traceback,701,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1189,testability,log,log,1189,"ls."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:1975,testability,test,testing,1975,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2082,testability,test,testing,2082,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2225,testability,test,testing,2225,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:2380,testability,test,testing,2380,"httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import _numpy_tester. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>. from . import decorators as dec. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>. from .utils import SkipTest, assert_warns. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>. from tempfile import mkdtemp, mkstemp. ImportError: cannot import name mkdtemp. >>> . ```. As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module. On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:11,usability,clear,clearer,11,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:572,usability,error,error,572,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:603,usability,simpl,simply,603,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:668,usability,error,error,668,"To make it clearer, I put the path structure here. ```. /deepvariant/core/. cloud_utils_test.py. math.py. ... ```. And in `cloud_utils_test.py`:. ```. """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import. from __future__ import division. from __future__ import print_function. import httplib. ... ```. Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. . But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:. ```. >>> import httplib. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>. import mimetools. File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>. import tempfile. File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>. from random import Random as _Random. File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>. from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil. ***File ""math.py"", line 79, in <module>***. import numpy as np. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>. from . import add_newdocs. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>. from numpy.lib import add_newdoc. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>. from .type_check import *. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>. import numpy.core.numeric as _nx. File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>. from numpy.testing.nosetester import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:156,deployability,updat,update,156,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:107,energy efficiency,core,core,107,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:156,safety,updat,update,156,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:156,security,updat,update,156,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:166,deployability,updat,update,166,"A fix is in in google, renaming math.py to genomics_math.py, which should fix the problem. The next major push of functional changes to DeepVariant will include this update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:166,safety,updat,update,166,"A fix is in in google, renaming math.py to genomics_math.py, which should fix the problem. The next major push of functional changes to DeepVariant will include this update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/32:166,security,updat,update,166,"A fix is in in google, renaming math.py to genomics_math.py, which should fix the problem. The next major push of functional changes to DeepVariant will include this update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32
https://github.com/google/deepvariant/issues/33:197,deployability,pipelin,pipelines,197,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:406,deployability,API,API,406,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:277,energy efficiency,CPU,CPU,277,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:282,energy efficiency,GPU,GPU,282,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:429,energy efficiency,cloud,cloud,429,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:197,integrability,pipelin,pipelines,197,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:331,integrability,sub,substantial,331,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:406,integrability,API,API,406,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:406,interoperability,API,API,406,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:219,performance,time,time,219,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:277,performance,CPU,CPU,277,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:282,performance,GPU,GPU,282,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:303,performance,disk,disk,303,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:13,energy efficiency,charg,charge,13,So it should charge same amount of money right (compare running one by one with in parallel)? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:83,performance,parallel,parallel,83,So it should charge same amount of money right (compare running one by one with in parallel)? .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/issues/33:38,usability,help,help,38,Thank you for all the information and help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33
https://github.com/google/deepvariant/pull/34:2,security,sign,signed,2,I signed it!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/34
https://github.com/google/deepvariant/issues/35:2175,availability,consist,consistent,2175," basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considered false positives). These should be bgzipped and tabix indexed and be on a reference consistent with. the one provided with the `--ref` argument.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:122,deployability,releas,release,122,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:131,deployability,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:506,deployability,version,version,506,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:1939,deployability,contain,containing,1939," basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considered false positives). These should be bgzipped and tabix indexed and be on a reference consistent with. the one provided with the `--ref` argument.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:1138,energy efficiency,current,currently,1138," timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considered false positives). These should be bgzipped and tabi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:131,integrability,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:506,integrability,version,version,506,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:493,interoperability,compatib,compatible,493,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:566,interoperability,compatib,compatible,566,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:609,interoperability,share,share,609,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:702,interoperability,share,shared,702,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:923,interoperability,share,shared,923,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:131,modifiability,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:506,modifiability,version,version,506,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:800,modifiability,deco,decoy,800,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:1088,performance,disk,disk,1088,"or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considere",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:131,safety,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:207,safety,input,input,207,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:1549,safety,valid,valid,1549," basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considered false positives). These should be bgzipped and tabix indexed and be on a reference consistent with. the one provided with the `--ref` argument.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:131,testability,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:207,usability,input,input,207,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:939,usability,effectiv,effectively,939,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:2175,usability,consist,consistent,2175," basic requirements to. be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be. indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible"". version of the genome reference provided as the `--ref`. By compatible here we. mean the BAM and FASTA share at least a reasonable set of common contigs, as. DeepVariant will only process contigs shared by both the BAM and reference. As. an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you. provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only. process variants on the shared contigs, effectively excluding the hs37d5 contig. present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you. cannot pipe it into DeepVariant. We currently recommend that the BAM be. duplicate marked, but it's unclear if this is even necessary. Finally, it's not. necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should. refer to contigs present in the reference genome. These arguments accept. space-separated lists, so all of the follow examples are valid arguments for. `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20. * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20. * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and. `confident_regions` arguments should point to VCF and BED files containing the. true variants and regions where we are confident in our calls (i.e., calls. within these regions and not in the truth_vcf are considered false positives). These should be bgzipped and tabix indexed and be on a reference consistent with. the one provided with the `--ref` argument.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:44,usability,help,helpful,44,"@depristo Thank you very much Mark, this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:129,modifiability,concern,concerning,129,"@depristo Following up. Not sure if you will have answers to these questions, but I'll ask anyway. What are your recommendations concerning which reads should be included in BAM files? As I'm sure you know, there is a trade-off between false positives and false negatives, particularly in regions where coverage is low and we need evidence. What is the minimum recommended read mapping quality? Suppose a read has high mapping quality, but is not properly paired, meaning the distance between the mates is greater than expected. Should the read be included in the BAM file? Should the BAM file include unpaired reads with high mapping quality, meaning only one of the mates was mapped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:129,testability,concern,concerning,129,"@depristo Following up. Not sure if you will have answers to these questions, but I'll ask anyway. What are your recommendations concerning which reads should be included in BAM files? As I'm sure you know, there is a trade-off between false positives and false negatives, particularly in regions where coverage is low and we need evidence. What is the minimum recommended read mapping quality? Suppose a read has high mapping quality, but is not properly paired, meaning the distance between the mates is greater than expected. Should the read be included in the BAM file? Should the BAM file include unpaired reads with high mapping quality, meaning only one of the mates was mapped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:303,testability,coverag,coverage,303,"@depristo Following up. Not sure if you will have answers to these questions, but I'll ask anyway. What are your recommendations concerning which reads should be included in BAM files? As I'm sure you know, there is a trade-off between false positives and false negatives, particularly in regions where coverage is low and we need evidence. What is the minimum recommended read mapping quality? Suppose a read has high mapping quality, but is not properly paired, meaning the distance between the mates is greater than expected. Should the read be included in the BAM file? Should the BAM file include unpaired reads with high mapping quality, meaning only one of the mates was mapped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/35:353,usability,minim,minimum,353,"@depristo Following up. Not sure if you will have answers to these questions, but I'll ask anyway. What are your recommendations concerning which reads should be included in BAM files? As I'm sure you know, there is a trade-off between false positives and false negatives, particularly in regions where coverage is low and we need evidence. What is the minimum recommended read mapping quality? Suppose a read has high mapping quality, but is not properly paired, meaning the distance between the mates is greater than expected. Should the read be included in the BAM file? Should the BAM file include unpaired reads with high mapping quality, meaning only one of the mates was mapped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35
https://github.com/google/deepvariant/issues/36:198,deployability,contain,contain,198,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:243,deployability,releas,releasing,243,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:299,energy efficiency,core,core,299,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:228,interoperability,share,share,228,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:536,modifiability,evolv,evolving,536,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:155,performance,content,content,155,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:432,reliability,doe,does,432,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:489,reliability,Doe,Does,489,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:716,safety,compl,complete,716,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:716,security,compl,complete,716,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:518,testability,understand,understanding,518,"> Why did you decide to not keep the software history? DeepVariant was originally developed within Google, using our internal systems. The original paths, content, etc from the earliest commits may contain information we cannot share, so upon releasing DeepVariant we squashed the history. > Do the core developers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? > How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software? > Do the newcomers faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems? No, because our history is complete within Google.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/36:173,integrability,sub,submitted,173,"Hi @depristo, . thanks once again for answering our research inquiries. We were able to collect 35 responses and we drafted a research paper with the results. The paper was submitted and accepted for the 14th International Conference on Open Source Systems (http://oss2018.org/). You can find the paper [here](http://gustavopinto.org/lost+found/oss2018.pdf). Hope you enjoy reading the paper! Thanks again,. Gustavo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/36
https://github.com/google/deepvariant/issues/37:354,usability,document,documentation,354,"Thanks! Attached is a list from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa. This list has all the GRCh38 contigs after dropping chr1-chr22, chrX, chrY and chrM. . GRCh38 has lots of additional contigs which are described here:. https://software.broadinstitute.org/gatk/documentation/article.php?id=7857. https://software.broadinstitute.org/gatk/blog?id=8180. [GRCh38.exclude.contigs.txt](https://github.com/google/deepvariant/files/1624030/GRCh38.exclude.contigs.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:63,deployability,releas,release,63,Thanks @jjfarrell. We've added a bug and will fix for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:54,deployability,releas,release,54,Nice! This is fixed now and will go out with the next release of DeepVariant. Thanks for the great suggestion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:361,availability,state,stated,361,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:375,deployability,releas,release,375,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:361,integrability,state,stated,361,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:30,interoperability,specif,specific,30,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:164,modifiability,deco,decoys,164,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1,reliability,rpo,rpoplin,1,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:217,reliability,doe,doesn,217,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:286,reliability,doe,doesn,286,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:401,reliability,Doe,Does,401,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:702,safety,test,testing,702,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:702,testability,test,testing,702,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:475,usability,user,user,475,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:694,deployability,releas,release,694,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:122,energy efficiency,current,current,122,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:618,energy efficiency,model,model,618,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:87,modifiability,deco,decoy,87,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:263,modifiability,deco,decoy,263,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:161,security,modif,modifiable,161,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:221,security,assess,assess,221,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:618,security,model,model,618,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:364,testability,coverag,coverage,364,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:37,usability,user,users,37,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:324,deployability,version,version,324,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:324,integrability,version,version,324,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:312,interoperability,compatib,compatible,312,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:382,interoperability,compatib,compatible,382,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:424,interoperability,share,share,424,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:516,interoperability,share,shared,516,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:324,modifiability,version,version,324,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:150,security,team,team,150,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:125,testability,understand,understand,125,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:46,usability,close,closed,46,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:221,usability,document,documentation,221,"Dear @AndrewCarroll , . I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. Thanks in advance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1115,availability,state,statement,1115,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:297,integrability,coupl,couple,297,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1115,integrability,state,statement,1115,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:297,modifiability,coupl,couple,297,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:317,performance,perform,performed,317,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:936,performance,perform,performed,936,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:297,testability,coupl,couple,297,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:317,usability,perform,performed,317,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:936,usability,perform,performed,936,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1144,usability,document,documentation,1144,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1696,usability,help,helps,1696,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:331,deployability,version,version,331,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:331,integrability,version,version,331,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:319,interoperability,compatib,compatible,319,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:389,interoperability,compatib,compatible,389,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:431,interoperability,share,share,431,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:523,interoperability,share,shared,523,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:331,modifiability,version,version,331,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:157,security,team,team,157,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:132,testability,understand,understand,132,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:53,usability,close,closed,53,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:228,usability,document,documentation,228,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:701,usability,document,documentation,701,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:1246,usability,help,helps,1246,"> Dear @AndrewCarroll ,. > . > I know this issue was closed, but I did not want to open a new one for a question this trivial. Do I understand correctly the team has now implemented some form of alt contig consideration, as the documentation now says: ""_Second, the BAM file provided to --reads should be aligned to a ""compatible"" version of the genome reference provided as the --ref. By compatible here we mean the BAM and FASTA share at least a reasonable set of common contigs, as DeepVariant will only process contigs shared by both the BAM and reference._ """" [https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md]. > . > Thanks in advance. Hi @ESDeutekom ,. Note that the documentation you quoted actually has been there for a while now, since 2018: https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md. If this wording is confusing, we're happy to improve it. Please let us know which part is particularly confusing. If your question is whether DeepVariant takes alt contigs into account, the answer also hasn't changed - not really. In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:282,availability,state,stated,282,"Dear @pgrosu and @pichuan, . Thank you for your answers. This does help me further along. With regards to the following: . > In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Is this stated somwhere, or published with comparative results? I am curious to get some insight and elaboration on why this would happen. Kind regards, . Eva.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:282,integrability,state,stated,282,"Dear @pgrosu and @pichuan, . Thank you for your answers. This does help me further along. With regards to the following: . > In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Is this stated somwhere, or published with comparative results? I am curious to get some insight and elaboration on why this would happen. Kind regards, . Eva.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:302,integrability,pub,published,302,"Dear @pgrosu and @pichuan, . Thank you for your answers. This does help me further along. With regards to the following: . > In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Is this stated somwhere, or published with comparative results? I am curious to get some insight and elaboration on why this would happen. Kind regards, . Eva.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:62,reliability,doe,does,62,"Dear @pgrosu and @pichuan, . Thank you for your answers. This does help me further along. With regards to the following: . > In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Is this stated somwhere, or published with comparative results? I am curious to get some insight and elaboration on why this would happen. Kind regards, . Eva.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:67,usability,help,help,67,"Dear @pgrosu and @pichuan, . Thank you for your answers. This does help me further along. With regards to the following: . > In general, when choosing between mapping to GRCh38 with alt contigs or without alt contigs, we empirically find that it's better to use the latter. Is this stated somwhere, or published with comparative results? I am curious to get some insight and elaboration on why this would happen. Kind regards, . Eva.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:223,usability,experien,experience,223,"@ESDeutekom , this article https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use from Heng describes different caveats of using different reference genomes. As suggested in that article and from our previous experience we found `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` to be the most suitable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:275,availability,Mask,Masked-reference-genomes,275,"Hi Eva, . Usually such comparisons are rare, as Kishwar suggested through Heng's article. If you are curious about such a comparison, below is a table generated on GIAB samples through [GATK's HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/17295731870235-Masked-reference-genomes) on GRCh38 and GRCh38 with masked alt regions:. ![image](https://github.com/google/deepvariant/assets/6555937/18be1f0e-6c1f-4c65-ae76-51661af20282). Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:327,availability,mask,masked,327,"Hi Eva, . Usually such comparisons are rare, as Kishwar suggested through Heng's article. If you are curious about such a comparison, below is a table generated on GIAB samples through [GATK's HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/17295731870235-Masked-reference-genomes) on GRCh38 and GRCh38 with masked alt regions:. ![image](https://github.com/google/deepvariant/assets/6555937/18be1f0e-6c1f-4c65-ae76-51661af20282). Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:457,usability,help,helps,457,"Hi Eva, . Usually such comparisons are rare, as Kishwar suggested through Heng's article. If you are curious about such a comparison, below is a table generated on GIAB samples through [GATK's HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/17295731870235-Masked-reference-genomes) on GRCh38 and GRCh38 with masked alt regions:. ![image](https://github.com/google/deepvariant/assets/6555937/18be1f0e-6c1f-4c65-ae76-51661af20282). Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/37:96,usability,help,helps,96,"Dear @pgrosu and @kishwarshafin,. Thank you very much for the added information. This certainly helps me along. . Kind regards, .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37
https://github.com/google/deepvariant/issues/38:149,modifiability,exten,extend,149,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:299,modifiability,exten,extend,299,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:36,usability,support,support,36,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:119,usability,support,supports,119,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:819,availability,down,downloading,819,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1629,availability,sla,slash,1629," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1660,availability,sla,slash,1660," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1836,availability,down,downloaded,1836," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1939,availability,down,downloads,1939," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1928,integrability,sub,subsequent,1928," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:301,modifiability,deco,decode,301,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:582,modifiability,variab,variable,582,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1798,modifiability,variab,variable,1798," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:894,performance,cach,cache,894,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1125,performance,cach,cache,1125," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1170,performance,cach,cache,1170," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1252,performance,cach,cache,1252," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1497,performance,cach,cached,1497," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1629,reliability,sla,slash,1629," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1660,reliability,sla,slash,1660," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1715,safety,avoid,avoid,1715," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1922,safety,avoid,avoid,1922," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1706,usability,help,helps,1706," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/38:1818,usability,indicat,indicate,1818," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**. One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta. export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s. export REF_CACHE=/some_dir/cache/%2s/%2s/%s. REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38
https://github.com/google/deepvariant/issues/39:48,interoperability,compatib,compatibility,48,This looks similar to #16 and may be a hardware compatibility issue. Could you please clarify if you're able to run the [quickstart](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) binaries without using the docker image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:135,availability,error,error,135,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:266,availability,slo,slower,266,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:204,deployability,contain,container,204,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:49,energy efficiency,CPU,CPU,49,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:108,energy efficiency,cpu,cpuinfo,108,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:141,integrability,messag,message,141,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:141,interoperability,messag,message,141,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:49,performance,CPU,CPU,49,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:108,performance,cpu,cpuinfo,108,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:135,performance,error,error,135,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:67,reliability,doe,does,67,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:266,reliability,slo,slower,266,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:135,safety,error,error,135,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:76,usability,support,support,76,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:135,usability,error,error,135,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:307,usability,tool,tool,307,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:34,availability,error,error,34,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:141,deployability,build,build,141,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:301,deployability,Contain,Container-optimized,301,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:350,deployability,contain,container-optimized-os,350,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:249,energy efficiency,optim,optimization,249,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:311,energy efficiency,optim,optimized,311,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:333,energy efficiency,cloud,cloud,333,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:360,energy efficiency,optim,optimized-os,360,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:390,energy efficiency,Cloud,Cloud,390,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:40,integrability,messag,message,40,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:40,interoperability,messag,message,40,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:34,performance,error,error,34,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:249,performance,optimiz,optimization,249,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:311,performance,optimiz,optimized,311,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:360,performance,optimiz,optimized-os,360,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:34,safety,error,error,34,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:34,usability,error,error,34,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:91,usability,user,users,91,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:207,usability,prefer,prefer,207,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/39:418,usability,tool,tool,418,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39
https://github.com/google/deepvariant/issues/40:219,availability,avail,available,219,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:191,deployability,version,version,191,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:191,integrability,version,version,191,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:379,integrability,discover,discovery,379,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:379,interoperability,discover,discovery,379,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:191,modifiability,version,version,191,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:219,reliability,availab,available,219,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:219,safety,avail,available,219,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:219,security,availab,available,219,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:98,testability,coverag,coverage,98,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:465,testability,coverag,coverage,465,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:545,testability,coverag,coverage,545,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:967,testability,coverag,coverage,967,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:379,usability,discov,discovery,379,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:703,usability,command,command,703,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:941,usability,experien,experiences,941,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:1065,usability,user,user-images,1065,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:489,availability,avail,available,489,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:461,deployability,version,version,461,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:461,integrability,version,version,461,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:658,integrability,discover,discovery,658,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:658,interoperability,discover,discovery,658,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:461,modifiability,version,version,461,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:489,reliability,availab,available,489,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:489,safety,avail,available,489,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:489,security,availab,available,489,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:1742,security,auth,auth,1742,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:84,testability,coverag,coverage,84,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:365,testability,coverag,coverage,365,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:750,testability,coverag,coverage,750,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:833,testability,coverag,coverage,833,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:1273,testability,coverag,coverage,1273,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:658,usability,discov,discovery,658,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:997,usability,command,command,997,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:1247,usability,experien,experiences,1247,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:1389,usability,user,user-images,1389,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200). of low coverage WGS? Would that make a difference to DeepVariant? On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>. wrote:. > Hi @avilella <https://github.com/avilella>,. >. > That's a great question. We done some limited experiments with DeepVariant. > on lower coverage samples, but not at the 2.5x-5x range directly (see. > attached image using an earlier version of DeepVariant than available on. > GitHub). >. > Typically at such a low depth you need to follow a joint calling strategy. > like the 1000 Genomes project in order to get accurate allele discovery. > across many samples. If you are trying to do single sample calling from low. > coverage, despite the relatively low quality of calls you'll get due to the. > low coverage, you can certainly use DeepVariant. You really don't need to. > do anything different than for a deep WGS sample, so just follow the case. > study example command lines. >. > There are some options in make_examples.py to manipulate the thresholds. > for generating candidate variant calls, but I'm not sure tweaking those. > will materially change the results. We'd be interested in hearing about. > your experiences with such low coverage samples if you do decide to try it. > out. >. > [image: screen shot 2018-01-17 at 11 14 01 am]. > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:153,deployability,contain,containing,153,"I'm not sure what you mean. DeepVariant only does single sample calling. I'm really not sure what would happen if you tried to send in a merged BAM file containing hundreds of samples, but it could be an interesting experiment...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/40:45,reliability,doe,does,45,"I'm not sure what you mean. DeepVariant only does single sample calling. I'm really not sure what would happen if you tried to send in a merged BAM file containing hundreds of samples, but it could be an interesting experiment...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40
https://github.com/google/deepvariant/issues/41:25,availability,operat,operating,25,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:104,availability,error,error,104,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:110,integrability,messag,messages,110,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:110,interoperability,messag,messages,110,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:104,performance,error,error,104,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:104,safety,error,error,104,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:104,usability,error,error,104,"Hi,. can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:24,usability,confirm,confirmed,24,OS: Debian 9 stretch. I confirmed the 0 exit status of run-prereq.sh.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:45,usability,statu,status,45,OS: Debian 9 stretch. I confirmed the 0 exit status of run-prereq.sh.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:140,safety,test,test,140,I confirmed again that `cd bin; sudo ./run-prereq.sh` exits with `$?` set to 0. Then I did another attempt with `make-examples.zip` and the test data but the result is still the same ImportError.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:140,testability,test,test,140,I confirmed again that `cd bin; sudo ./run-prereq.sh` exits with `$?` set to 0. Then I did another attempt with `make-examples.zip` and the test data but the result is still the same ImportError.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:2,usability,confirm,confirmed,2,I confirmed again that `cd bin; sudo ./run-prereq.sh` exits with `$?` set to 0. Then I did another attempt with `make-examples.zip` and the test data but the result is still the same ImportError.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:86,availability,error,error,86,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:379,availability,Operat,Operation,379,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:141,deployability,Stage,Stage,141,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:148,deployability,Instal,Install,148,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:191,deployability,Instal,Installing,191,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:209,energy efficiency,Cloud,Cloud,209,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:224,energy efficiency,optim,optimized,224,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:234,energy efficiency,CPU,CPU-only,234,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:215,interoperability,Platform,Platform,215,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:171,modifiability,pac,package,171,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:287,modifiability,pac,packages,287,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:86,performance,error,error,86,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:224,performance,optimiz,optimized,224,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:234,performance,CPU,CPU-only,234,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:75,safety,permiss,permission,75,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:86,safety,error,error,86,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:86,usability,error,error,86,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:. ```. ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting . Installing Google Cloud Platform optimized CPU-only TensorFlow wheel . Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl... OSError: Operation not permitted. . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:0,energy efficiency,Current,Currently,0,"Currently our case studies are all done on Ubuntu 16.04. The easiest way will be to try it on that. We don't currently have plans to support other OS. If you do get it to work, please let us know!! I added an internal bug to track this, but currently the priority is not high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:109,energy efficiency,current,currently,109,"Currently our case studies are all done on Ubuntu 16.04. The easiest way will be to try it on that. We don't currently have plans to support other OS. If you do get it to work, please let us know!! I added an internal bug to track this, but currently the priority is not high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:241,energy efficiency,current,currently,241,"Currently our case studies are all done on Ubuntu 16.04. The easiest way will be to try it on that. We don't currently have plans to support other OS. If you do get it to work, please let us know!! I added an internal bug to track this, but currently the priority is not high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:124,testability,plan,plans,124,"Currently our case studies are all done on Ubuntu 16.04. The easiest way will be to try it on that. We don't currently have plans to support other OS. If you do get it to work, please let us know!! I added an internal bug to track this, but currently the priority is not high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:133,usability,support,support,133,"Currently our case studies are all done on Ubuntu 16.04. The easiest way will be to try it on that. We don't currently have plans to support other OS. If you do get it to work, please let us know!! I added an internal bug to track this, but currently the priority is not high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:273,availability,error,error,273,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:128,deployability,contain,contains,128,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:273,performance,error,error,273,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:273,safety,error,error,273,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:608,safety,valid,validated,608,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:48,security,privil,privileges,48,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:440,security,control,control-flow,440,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:608,security,validat,validated,608,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:306,testability,trace,trace,306,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:440,testability,control,control-flow,440,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:273,usability,error,error,273,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:643,usability,help,helps,643,". Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:336,deployability,build,build,336,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:395,deployability,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:455,deployability,instal,installation,455,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:395,integrability,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:395,modifiability,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:395,safety,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:408,safety,prevent,prevented,408,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:249,security,hack,hack,249,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:408,security,preven,prevented,408,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:395,testability,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:68,deployability,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:68,integrability,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:172,integrability,discover,discover,172,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:172,interoperability,discover,discover,172,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:68,modifiability,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:129,performance,time,time,129,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:68,safety,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:68,testability,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:291,testability,simpl,simplify,291,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:172,usability,discov,discover,172,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:229,usability,tool,tools,229,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:291,usability,simpl,simplify,291,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:93,deployability,manag,managing,93,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:113,deployability,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:189,deployability,build,build,189,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:93,energy efficiency,manag,managing,93,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:320,energy efficiency,optim,optimizations,320,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:113,integrability,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:113,modifiability,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:287,modifiability,extens,extensions,287,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:320,performance,optimiz,optimizations,320,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:93,safety,manag,managing,93,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:113,safety,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:177,safety,compl,complex,177,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:177,security,compl,complex,177,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:113,testability,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:299,testability,simpl,simplifications,299,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:39,usability,close,close,39,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:299,usability,simpl,simplifications,299,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:60,energy efficiency,current,current,60,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:158,reliability,growth,growth,158,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:333,safety,reme,remedies,333,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:326,testability,simpl,simple,326,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:150,usability,support,support,150,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:326,usability,simpl,simple,326,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:0,availability,down,downloading,0,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:60,deployability,updat,updates,60,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:152,deployability,instal,install,152,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:60,safety,updat,updates,60,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:19,security,secur,security,19,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:46,security,secur,security,46,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/41:60,security,updat,updates,60,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by. ```. sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb. ```. seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41
https://github.com/google/deepvariant/issues/42:183,deployability,configurat,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:55,energy efficiency,cpu,cpuset-cpus,55,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:416,energy efficiency,CPU,CPU,416,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:183,integrability,configur,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:183,modifiability,configur,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:55,performance,cpu,cpuset-cpus,55,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:155,performance,perform,performed,155,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:416,performance,CPU,CPU,416,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:174,security,Session,Session,174,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:183,security,configur,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:427,security,session,session,427,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:440,security,Session,Session,440,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:155,usability,perform,performed,155,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:575,usability,prefer,preference,575,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:679,usability,help,helps,679,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```. config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \. allow_soft_placement=True, device_count = {'CPU': 1}). session = tf.Session(config=config). ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:41,deployability,continu,continue,41,"Thank you! Although while testing I will continue to use Docker, changing the code might become important to me 👍",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:26,safety,test,testing,26,"Thank you! Although while testing I will continue to use Docker, changing the code might become important to me 👍",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/42:26,testability,test,testing,26,"Thank you! Although while testing I will continue to use Docker, changing the code might become important to me 👍",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42
https://github.com/google/deepvariant/issues/43:30,deployability,build,build-prereq,30,"bazel seems to be at 0.8.1 in build-prereq.sh (this was one of the problems in the earlier versions, apparently).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:91,deployability,version,versions,91,"bazel seems to be at 0.8.1 in build-prereq.sh (this was one of the problems in the earlier versions, apparently).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:91,integrability,version,versions,91,"bazel seems to be at 0.8.1 in build-prereq.sh (this was one of the problems in the earlier versions, apparently).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:91,modifiability,version,versions,91,"bazel seems to be at 0.8.1 in build-prereq.sh (this was one of the problems in the earlier versions, apparently).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:16,availability,error,error,16,"The command and error attached here in full, just in case that is needed to debug the problem. [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:16,performance,error,error,16,"The command and error attached here in full, just in case that is needed to debug the problem. [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:16,safety,error,error,16,"The command and error attached here in full, just in case that is needed to debug the problem. [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:4,usability,command,command,4,"The command and error attached here in full, just in case that is needed to debug the problem. [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:16,usability,error,error,16,"The command and error attached here in full, just in case that is needed to debug the problem. [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:46,availability,error,errors,46,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:125,integrability,messag,message,125,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:125,interoperability,messag,message,125,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:46,performance,error,errors,46,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:46,safety,error,errors,46,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:46,usability,error,errors,46,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:133,usability,help,help,133,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/43:74,interoperability,incompatib,incompatible,74,"solved the problem, had to edit the build_and_test.sh file to include the incompatible label incantation. attached here in case it is useful for someone. [build_and_test.sh.txt](https://github.com/google/deepvariant/files/1670623/build_and_test.sh.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43
https://github.com/google/deepvariant/issues/45:1,reliability,rpo,rpoplin,1,@rpoplin Do you have any suggestions? Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:283,energy efficiency,frequenc,frequencies,283,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:28,interoperability,specif,specific,28,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:489,interoperability,syntact,syntactically,489,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:507,interoperability,semant,semantically,507,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:362,performance,perform,performance,362,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:431,reliability,pra,practices,431,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:198,testability,simpl,simply,198,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:198,usability,simpl,simply,198,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:362,usability,perform,performance,362,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:558,usability,tool,tools,558,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:623,usability,tool,tools,623,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:87,availability,error,error,87,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:206,availability,error,error,206,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:243,availability,ERROR,ERROR,243,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:294,deployability,contain,contain,294,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:87,performance,error,error,87,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:206,performance,error,error,206,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:243,performance,ERROR,ERROR,243,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:87,safety,error,error,87,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:168,safety,test,tested,168,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:206,safety,error,error,206,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:243,safety,ERROR,ERROR,243,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:275,safety,input,input,275,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:168,testability,test,tested,168,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:87,usability,error,error,87,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:206,usability,error,error,206,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:238,usability,USER,USER,238,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:243,usability,ERROR,ERROR,243,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:275,usability,input,input,275,"Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:680,deployability,updat,update,680,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:57,energy efficiency,current,currently,57,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:490,energy efficiency,model,models,490,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:531,energy efficiency,current,currently,531,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:585,energy efficiency,optim,optimal,585,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:911,energy efficiency,current,currently,911,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:234,integrability,filter,filtering,234,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:469,interoperability,specif,specific-deepvariant-models,469,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:848,interoperability,format,format,848,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:213,safety,valid,valid,213,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:680,safety,updat,update,680,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:490,security,model,models,490,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:680,security,updat,update,680,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:27,deployability,updat,update,27,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:116,deployability,releas,release,116,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:144,reliability,pra,practices,144,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:27,safety,updat,update,27,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:27,security,updat,update,27,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/45:311,usability,user,users,311,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45
https://github.com/google/deepvariant/issues/46:492,availability,mainten,maintenance-policy,492,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:534,availability,failur,failure,534,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:551,availability,down,downloaded,551,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:810,availability,down,downloaded,810,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:534,deployability,fail,failure,534,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:616,deployability,build,build,616,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:752,deployability,build,build-prereq,752,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1363,deployability,log,log,1363,"u-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsol",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1669,deployability,log,log,1669," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:297,energy efficiency,cloud,cloud-platform,297,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:370,energy efficiency,cloud,cloud,370,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:622,energy efficiency,optim,optimization,622,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1009,energy efficiency,cpu,cpus,1009," I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1147,energy efficiency,adapt,adapted,1147,"ommand-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1955,energy efficiency,model,model-training,1955," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1147,integrability,adapt,adapted,1147,"ommand-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:303,interoperability,platform,platform,303,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:394,interoperability,standard,standard-,394,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1147,interoperability,adapt,adapted,1147,"ommand-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:844,modifiability,variab,variables,844,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1147,modifiability,adapt,adapted,1147,"ommand-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:412,performance,disk,disk-size,412,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:534,performance,failur,failure,534,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:622,performance,optimiz,optimization,622,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1009,performance,cpu,cpus,1009," I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1292,performance,time,time,1292,"ll,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1324,performance,parallel,parallel,1324,"ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:492,reliability,mainten,maintenance-policy,492,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:534,reliability,fail,failure,534,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2073,reliability,rpo,rpoplin,2073," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1363,safety,log,log,1363,"u-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsol",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1669,safety,log,log,1669," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:150,security,command-lin,command-line,150,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:504,security,polic,policy,504,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1363,security,log,log,1363,"u-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsol",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1669,security,log,log,1669," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1955,security,model,model-training,1955," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1363,testability,log,log,1363,"u-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsol",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1669,testability,log,log,1669," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. ```. Then I launched a training job with model_train.zip:. ```. python ""${BIN_DIR}""/model_train.zip \. --dataset_config_pbtxt output/data.pbtxt \. --start_from_checkpoint """" \. --batch_size 16 \. --alsologtostderr. ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:102,usability,help,helpful,102,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:150,usability,command,command-line,150,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:233,usability,USER,USER,233,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1159,usability,command,command,1159,"from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_release_binaries.sh. ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. python ""${BIN_DIR}""/make_examples.zip \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --confident_regions ""${TRUTH_BED}"" \. --truth_variants ""${TRUTH_VCF}"" \. --examples ""${EXAMPLES}"" \. --regions ""20:10,000,000-12,000,000"" \. --task {}. ) >""${LOG_DIR}/make_examples.log"" 2>&1. ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:. ```. > cat output/data.pbtxt . name: ""my-training-dataset"". tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz"". num_examples: 150. `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:414,availability,Sli,Slim,414,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:628,availability,Sli,Slim,628,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:21,energy efficiency,model,model,21,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:318,energy efficiency,model,model,318,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:352,energy efficiency,model,model,352,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:421,energy efficiency,model,model,421,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:530,energy efficiency,model,modeling,530,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:171,integrability,batch,batches,171,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:435,modifiability,paramet,parameter,435,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:171,performance,batch,batches,171,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:414,reliability,Sli,Slim,414,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:599,reliability,doe,doesn,599,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:628,reliability,Sli,Slim,628,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:21,security,model,model,21,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:318,security,model,model,318,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:352,security,model,model,352,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:421,security,model,model,421,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:530,security,model,modeling,530,"we found the default model saving path is ""/tmp/deepvariant"" https://github.com/google/deepvariant/blob/r0.5/deepvariant/model_train.py#L67 and default numbers of trainig batches is unlimited https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L85 . However, we would like to try to train the model with a different pretrained model, and we found that for Inception V3, it uses TensorFlow Slim's model, with a parameter of pretrained_model_path https://github.com/google/deepvariant/blob/r0.5/deepvariant/modeling.py#L353 . We don't quite get where this path comes from. It doesn't seem to exist in the Slim repo. Can you please point us to where to find this set of pretrained weights?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:63,availability,checkpoint,checkpoint,63,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1118,availability,restor,restore,1118,"tion below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1126,availability,operat,operator,1126,"ow. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2197,availability,Checkpoint,CheckpointReader,2197," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2648,availability,restor,restore,2648," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2656,availability,operat,operator,2656," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:100,deployability,fail,failed,100,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:343,deployability,log,logs,343,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:367,deployability,log,log,367,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:381,deployability,Log,Logging,381,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1268,deployability,modul,module,1268,"uce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:258,energy efficiency,model,model,258,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:609,energy efficiency,core,core,609,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:654,energy efficiency,CPU,CPU,654,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:816,energy efficiency,model,model,816,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:851,energy efficiency,model,model,851,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:904,energy efficiency,core,core,904,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:980,energy efficiency,model,model,980,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1682,energy efficiency,model,model,1682,"this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1812,energy efficiency,model,modeling,1812," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2510,energy efficiency,model,model,2510," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:537,integrability,pub,public,537,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:614,interoperability,platform,platform,614,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1079,interoperability,format,format,1079,"found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1354,interoperability,platform,platform,1354,"variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2609,interoperability,format,format,2609," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1268,modifiability,modul,module,1268,"uce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1327,modifiability,pac,packages,1327,"tudy/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2097,modifiability,pac,packages,2097," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2289,modifiability,pac,packages,2289," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:654,performance,CPU,CPU,654,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:63,reliability,checkpoint,checkpoint,63,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:100,reliability,fail,failed,100,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1118,reliability,restor,restore,1118,"tion below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2197,reliability,Checkpoint,CheckpointReader,2197," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2648,reliability,restor,restore,2648," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:117,safety,except,exception,117,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:343,safety,log,logs,343,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:367,safety,log,log,367,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:381,safety,Log,Logging,381,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1268,safety,modul,module,1268,"uce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:258,security,model,model,258,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:343,security,log,logs,343,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:367,security,log,log,367,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:381,security,Log,Logging,381,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:526,security,access,access,526,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:816,security,model,model,816,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:851,security,model,model,851,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:980,security,model,model,980,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:997,security,loss,loss,997,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1682,security,model,model,1682,"this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1812,security,model,modeling,1812," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2510,security,model,model,2510," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2527,security,loss,loss,2527," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:343,testability,log,logs,343,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:367,testability,log,log,367,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:381,testability,Log,Logging,381,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:1136,testability,Trace,Traceback,1136,"not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:196,usability,command,command,196,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:221,usability,command,command,221,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:658,usability,support,supports,658,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants. ```. root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log. WARNING: Logging before flag parsing goes to stderr. I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib. 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2244,usability,statu,status,2244," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2388,usability,statu,status,2388," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2395,usability,statu,status,2395," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:2683,usability,user,user,2683," compiled to use: AVX2 FMA. I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt. 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants. model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint. [self.n_classes_model_variable]). File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes. reader = tf.train.NewCheckpointReader(checkpoint_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader. return CheckpointReader(compat.as_bytes(filepattern), status). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__. c_api.TF_GetCode(self.status.status)). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? . real 0m5.561s. user 0m6.116s. sys 0m0.810s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:262,availability,sli,slim,262,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:282,availability,down,down,282,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:345,availability,down,download,345,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:369,availability,checkpoint,checkpoint,369,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:394,availability,down,download,394,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:673,availability,checkpoint,checkpoint,673,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:723,availability,Checkpoint,Checkpoint,723,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:895,availability,checkpoint,checkpoint,895,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:35,energy efficiency,model,model,35,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:234,energy efficiency,model,models,234,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:312,energy efficiency,Model,Models,312,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:328,energy efficiency,Cloud,Cloud,328,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:418,energy efficiency,model,models,418,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:662,energy efficiency,model,model,662,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:706,energy efficiency,model,modeling,706,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:843,energy efficiency,load,loaded,843,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:906,energy efficiency,load,loading,906,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:608,integrability,messag,messages,608,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:608,interoperability,messag,messages,608,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:843,performance,load,loaded,843,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:866,performance,network,network,866,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:906,performance,load,loading,906,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:262,reliability,sli,slim,262,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:369,reliability,checkpoint,checkpoint,369,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:673,reliability,checkpoint,checkpoint,673,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:723,reliability,Checkpoint,Checkpoint,723,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:895,reliability,checkpoint,checkpoint,895,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:35,security,model,model,35,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:234,security,model,models,234,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:312,security,Model,Models,312,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:418,security,model,models,418,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:662,security,model,model,662,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:706,security,model,modeling,706,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:866,security,network,network,866,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:520,usability,command,command,520,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:. wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz. tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add. "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt. modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:55,availability,checkpoint,checkpoints,55,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:46,deployability,releas,released,46,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:168,deployability,releas,released,168,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:177,energy efficiency,model,model,177,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:55,reliability,checkpoint,checkpoints,55,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:177,security,model,model,177,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:22,availability,checkpoint,checkpoint,22,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:123,availability,checkpoint,checkpoint,123,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:410,availability,checkpoint,checkpoint,410,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:499,deployability,log,log,499,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:10,energy efficiency,load,loading,10,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:191,energy efficiency,model,model,191,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:438,energy efficiency,model,model,438,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:645,interoperability,specif,specifying,645,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:10,performance,load,loading,10,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:265,performance,time,time,265,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:22,reliability,checkpoint,checkpoint,22,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:123,reliability,checkpoint,checkpoint,123,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:410,reliability,checkpoint,checkpoint,410,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:499,safety,log,log,499,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:191,security,model,model,191,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:438,security,model,model,438,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:499,security,log,log,499,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/46:499,testability,log,log,499,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46
https://github.com/google/deepvariant/issues/47:55,deployability,build,building,55,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:114,deployability,build,build,114,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:337,deployability,build,build-prereq,337,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:80,energy efficiency,gpu,gpu,80,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:166,energy efficiency,gpu,gpu,166,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:80,performance,gpu,gpu,80,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:166,performance,gpu,gpu,166,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:394,safety,test,test,394,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:427,safety,test,testdata,427,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:394,testability,test,test,394,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:427,testability,test,testdata,427,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:90,usability,command,commands,90,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:399,usability,command,command,399,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:141,availability,avail,available,141,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1995,availability,error,error-free,1995,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2173,availability,error,errors,2173,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:65,deployability,version,version,65,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:91,deployability,instal,installed,91,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:135,deployability,build,build,135,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:162,deployability,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:272,deployability,instal,install,272,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:601,deployability,Build,Build,601,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:701,deployability,build,building,701,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:760,deployability,build,build,760,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:983,deployability,build,build-prereq,983,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1629,deployability,contain,contains,1629,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2101,deployability,contain,contain,2101,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2329,deployability,version,version,2329,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:87,energy efficiency,gpu,gpu,87,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:116,energy efficiency,GPU,GPU,116,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:726,energy efficiency,gpu,gpu,726,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:812,energy efficiency,gpu,gpu,812,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:65,integrability,version,version,65,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:162,integrability,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:557,integrability,Sub,Subject,557,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1621,integrability,messag,message,1621,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2217,integrability,messag,message,2217,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2329,integrability,version,version,2329,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1621,interoperability,messag,message,1621,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1773,interoperability,distribut,distribute,1773,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2217,interoperability,messag,message,2217,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:65,modifiability,version,version,65,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:162,modifiability,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2329,modifiability,version,version,2329,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:87,performance,gpu,gpu,87,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:116,performance,GPU,GPU,116,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:726,performance,gpu,gpu,726,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:812,performance,gpu,gpu,812,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1995,performance,error,error-free,1995,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2173,performance,error,errors,2173,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2200,performance,content,contents,2200,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:141,reliability,availab,available,141,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2139,reliability,doe,does,2139,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:141,safety,avail,available,141,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:162,safety,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:246,safety,test,tests,246,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:611,safety,test,test,611,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1040,safety,test,test,1040,"m with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1073,safety,test,testdata,1073,"ensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1995,safety,error,error-free,1995,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2173,safety,error,errors,2173,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:141,security,availab,available,141,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:521,security,Auth,Author,521,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:529,security,auth,author,529,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1350,security,auth,authored,1350,"fications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Labo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1556,security,auth,auth,1556,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1638,security,confidential,confidential,1638,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1984,security,secur,secured,1984,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2030,security,intercept,intercepted,2030,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:162,testability,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:246,testability,test,tests,246,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:611,testability,test,test,611,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1040,testability,test,test,1040,"m with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1073,testability,test,testdata,1073,"ensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2277,testability,verif,verification,2277,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:736,usability,command,commands,736,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1045,usability,command,command,1045,"CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 3:08 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrup",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1995,usability,error,error-free,1995,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2173,usability,error,errors,2173,"@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh. ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip. --mode calling. --ref ""${REF}"". --reads ""${BAM}"". --regions ""chr20:10,000,000-10,010,000"". --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:63,deployability,version,version,63,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:101,deployability,instal,install,101,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:124,deployability,version,version,124,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:161,deployability,build,build,161,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:204,deployability,instal,install,204,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:214,deployability,upgrad,upgrade,214,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:120,energy efficiency,gpu,gpu,120,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:234,energy efficiency,gpu,gpu,234,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:63,integrability,version,version,63,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:124,integrability,version,version,124,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:36,interoperability,compatib,compatible,36,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:63,modifiability,version,version,63,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:124,modifiability,version,version,124,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:214,modifiability,upgrad,upgrade,214,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:120,performance,gpu,gpu,120,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:234,performance,gpu,gpu,234,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with . sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1594,availability,error,error-free,1594," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1772,availability,error,errors,1772," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:27,deployability,build,build,27,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:216,deployability,version,version,216,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:622,deployability,Build,Build,622,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:730,deployability,version,version,730,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:768,deployability,instal,install,768,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:791,deployability,version,version,791,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:828,deployability,build,build,828,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:870,deployability,instal,install,870,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:880,deployability,upgrad,upgrade,880,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1228,deployability,contain,contains,1228," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1700,deployability,contain,contain,1700," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1928,deployability,version,version,1928," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:86,energy efficiency,gpu,gpu,86,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:172,energy efficiency,frequenc,frequency,172,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:787,energy efficiency,gpu,gpu,787,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:900,energy efficiency,gpu,gpu,900,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:216,integrability,version,version,216,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:578,integrability,Sub,Subject,578,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:730,integrability,version,version,730,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:791,integrability,version,version,791,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1220,integrability,messag,message,1220," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1816,integrability,messag,message,1816," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1928,integrability,version,version,1928," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:703,interoperability,compatib,compatible,703,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1220,interoperability,messag,message,1220," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1372,interoperability,distribut,distribute,1372," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1816,interoperability,messag,message,1816," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:216,modifiability,version,version,216,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:730,modifiability,version,version,730,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:791,modifiability,version,version,791,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:880,modifiability,upgrad,upgrade,880,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1928,modifiability,version,version,1928," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:86,performance,gpu,gpu,86,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:787,performance,gpu,gpu,787,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:900,performance,gpu,gpu,900,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1594,performance,error,error-free,1594," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1772,performance,error,errors,1772," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1799,performance,content,contents,1799," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1738,reliability,doe,does,1738," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:632,safety,test,test,632,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1594,safety,error,error-free,1594," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1772,safety,error,errors,1772," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:542,security,Auth,Author,542,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:550,security,auth,author,550,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:949,security,auth,authored,949,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1155,security,auth,auth,1155," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1237,security,confidential,confidential,1237," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1583,security,secur,secured,1583," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1629,security,intercept,intercepted,1629," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:632,testability,test,test,632,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1876,testability,verif,verification,1876," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:157,usability,minim,minimum,157,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1594,usability,error,error-free,1594," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1772,usability,error,errors,1772," CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)? Thanks,. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Monday, February 5, 2018 5:10 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with. sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:385,availability,state,states,385,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:537,deployability,version,version,537,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:257,energy efficiency,model,model,257,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:335,energy efficiency,model,model,335,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:356,energy efficiency,predict,predict,356,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:504,energy efficiency,model,model,504,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:520,energy efficiency,current,currently,520,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:385,integrability,state,states,385,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:537,integrability,version,version,537,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:579,interoperability,specif,specifically,579,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:537,modifiability,version,version,537,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:550,reliability,doe,does,550,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:356,safety,predict,predict,356,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:257,security,model,model,257,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:335,security,model,model,335,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:504,security,model,model,504,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:25,usability,minim,minimum,25,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:874,availability,state,states,874,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1752,availability,error,error-free,1752," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1930,availability,error,errors,1930," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:444,deployability,Build,Build,444,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1026,deployability,version,version,1026,"formation. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1386,deployability,contain,contains,1386," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1858,deployability,contain,contain,1858," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2086,deployability,version,version,2086," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:123,energy efficiency,model,model,123,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:746,energy efficiency,model,model,746,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:824,energy efficiency,model,model,824,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:845,energy efficiency,predict,predict,845,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:993,energy efficiency,model,model,993,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1009,energy efficiency,current,currently,1009,"hanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:400,integrability,Sub,Subject,400,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:874,integrability,state,states,874,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1026,integrability,version,version,1026,"formation. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1378,integrability,messag,message,1378," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1974,integrability,messag,message,1974," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2086,integrability,version,version,2086," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1378,interoperability,messag,message,1378," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1530,interoperability,distribut,distribute,1530," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1974,interoperability,messag,message,1974," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1026,modifiability,version,version,1026,"formation. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2086,modifiability,version,version,2086," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1752,performance,error,error-free,1752," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1930,performance,error,errors,1930," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1957,performance,content,contents,1957," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1039,reliability,doe,does,1039,"We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verific",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1896,reliability,doe,does,1896," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:454,safety,test,test,454,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:845,safety,predict,predict,845,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1752,safety,error,error-free,1752," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1930,safety,error,errors,1930," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:123,security,model,model,123,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:392,security,Auth,Author,392,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:746,security,model,model,746,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:824,security,model,model,824,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:993,security,model,model,993,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1107,security,auth,authored,1107,"to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Labo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1313,security,auth,auth,1313," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1395,security,confidential,confidential,1395," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1741,security,secur,secured,1741," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1787,security,intercept,intercepted,1787," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:454,testability,test,test,454,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:2034,testability,verif,verification,2034," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:514,usability,minim,minimum,514,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar? Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1752,usability,error,error-free,1752," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1930,usability,error,errors,1930," Best,. Brad Thomas. ________________________________. From: Ryan Poplin <notifications@github.com>. Sent: Friday, February 9, 2018 12:54 PM. To: google/deepvariant. Cc: Brad Thomas; Author. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:148,testability,coverag,coverage,148,"Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1539,availability,error,error-free,1539,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1717,availability,error,errors,1717,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:422,deployability,Build,Build,422,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1173,deployability,contain,contains,1173,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1645,deployability,contain,contain,1645,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1873,deployability,version,version,1873,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:378,integrability,Sub,Subject,378,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1165,integrability,messag,message,1165,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1761,integrability,messag,message,1761,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1873,integrability,version,version,1873,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1165,interoperability,messag,message,1165,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1317,interoperability,distribut,distribute,1317,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1761,interoperability,messag,message,1761,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1873,modifiability,version,version,1873,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1539,performance,error,error-free,1539,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1717,performance,error,errors,1717,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1744,performance,content,contents,1744,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1683,reliability,doe,does,1683,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:432,safety,test,test,432,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1539,safety,error,error-free,1539,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1717,safety,error,errors,1717,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:342,security,Auth,Author,342,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:350,security,auth,author,350,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:894,security,auth,authored,894,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1100,security,auth,auth,1100,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1182,security,confidential,confidential,1182,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1528,security,secur,secured,1528,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1574,security,intercept,intercepted,1574,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:432,testability,test,test,432,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:615,testability,coverag,coverage,615,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1821,testability,verif,verification,1821,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1539,usability,error,error-free,1539,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:1717,usability,error,errors,1717,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]. Sent: Tuesday, February 13, 2018 10:28 AM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:178,energy efficiency,model,model,178,"Yeah, the input to the CNN is a pileup tensor. Check out here for some more info: https://github.com/google/deepvariant#about-deepvariant. It would be interesting to train a new model for your particular data type.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:10,safety,input,input,10,"Yeah, the input to the CNN is a pileup tensor. Check out here for some more info: https://github.com/google/deepvariant#about-deepvariant. It would be interesting to train a new model for your particular data type.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:178,security,model,model,178,"Yeah, the input to the CNN is a pileup tensor. Check out here for some more info: https://github.com/google/deepvariant#about-deepvariant. It would be interesting to train a new model for your particular data type.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/47:10,usability,input,input,10,"Yeah, the input to the CNN is a pileup tensor. Check out here for some more info: https://github.com/google/deepvariant#about-deepvariant. It would be interesting to train a new model for your particular data type.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47
https://github.com/google/deepvariant/issues/48:407,energy efficiency,current,current,407,"Process each sample individually, and then merge the results. When you process each sample, be sure to create gVCF outputs (see https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md for details). The single-sample gVCF files can then be merged into a single cohort-level VCF using existing tools (see https://github.com/google/deepvariant/issues/45#issuecomment-363913008 for our current thoughts on merging tools).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/48
https://github.com/google/deepvariant/issues/48:198,usability,support,support,198,"Process each sample individually, and then merge the results. When you process each sample, be sure to create gVCF outputs (see https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md for details). The single-sample gVCF files can then be merged into a single cohort-level VCF using existing tools (see https://github.com/google/deepvariant/issues/45#issuecomment-363913008 for our current thoughts on merging tools).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/48
https://github.com/google/deepvariant/issues/48:317,usability,tool,tools,317,"Process each sample individually, and then merge the results. When you process each sample, be sure to create gVCF outputs (see https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md for details). The single-sample gVCF files can then be merged into a single cohort-level VCF using existing tools (see https://github.com/google/deepvariant/issues/45#issuecomment-363913008 for our current thoughts on merging tools).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/48
https://github.com/google/deepvariant/issues/48:435,usability,tool,tools,435,"Process each sample individually, and then merge the results. When you process each sample, be sure to create gVCF outputs (see https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md for details). The single-sample gVCF files can then be merged into a single cohort-level VCF using existing tools (see https://github.com/google/deepvariant/issues/45#issuecomment-363913008 for our current thoughts on merging tools).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/48
https://github.com/google/deepvariant/issues/49:158,deployability,configurat,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:391,deployability,contain,containers,391,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:122,energy efficiency,cpu,cpuset-cpus,122,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:326,energy efficiency,cpu,cpuset-cpus,326,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:450,energy efficiency,schedul,scheduler,450,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:158,integrability,configur,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:424,integrability,configur,configure-the-default-cfs-scheduler,424,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:97,interoperability,specif,specific,97,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:158,modifiability,configur,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:424,modifiability,configur,configure-the-default-cfs-scheduler,424,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:122,performance,cpu,cpuset-cpus,122,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:326,performance,cpu,cpuset-cpus,326,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:450,performance,schedul,scheduler,450,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:150,security,Session,Session,150,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:158,security,configur,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:424,security,configur,configure-the-default-cfs-scheduler,424,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:56,testability,simpl,simplest,56,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:26,usability,workflow,workflow,26,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:56,usability,simpl,simplest,56,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:597,usability,help,helps,597,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:293,deployability,configurat,configuration,293,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:558,deployability,contain,containers,558,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:255,energy efficiency,cpu,cpuset-cpus,255,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:489,energy efficiency,cpu,cpuset-cpus,489,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:621,energy efficiency,schedul,scheduler,621,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:293,integrability,configur,configuration,293,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:595,integrability,configur,configure-the-default-cfs-scheduler,595,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:231,interoperability,specif,specific,231,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:293,modifiability,configur,configuration,293,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:595,modifiability,configur,configure-the-default-cfs-scheduler,595,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:255,performance,cpu,cpuset-cpus,255,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:489,performance,cpu,cpuset-cpus,489,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:621,performance,schedul,scheduler,621,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:285,security,Session,Session,285,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:293,security,configur,configuration,293,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:595,security,configur,configure-the-default-cfs-scheduler,595,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:842,security,auth,authored,842,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:1061,security,auth,auth,1061,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:187,testability,simpl,simplest,187,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:157,usability,workflow,workflow,157,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:187,usability,simpl,simplest,187,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:781,usability,help,helps,781,"Is it possible to merge the tfrecords files though? On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,. >. > Since your WDL workflow is using Docker, the simplest approach is to. > include a Docker-specific argument for --cpuset-cpus, or change the. > Session configuration which I've detailed at, the following location:. >. > #42 (comment). > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>. >. > For information regarding the --cpuset-cpus here's a reference:. >. > https://docs.docker.com/config/containers/resource_. > constraints/#configure-the-default-cfs-scheduler. >. > There are many ways to change DeepVariant, but I think this will will get. > you the quickest results for the issue you're facing. >. > Hope it helps,. > Paul. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:202,interoperability,semant,semantics,202,"Well, what do you think might be happening at this line in the code :). https://github.com/google/deepvariant/blob/r0.5/deepvariant/call_variants.py#L170. Here's a link describing the `TFRecordDataset` semantics:. https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:22,testability,simpl,simply,22,"If you're looking for simply merging the tfrecord files (without having to touch Python code) to one, you can actually just concatenate tfrecord files together. Something like:. `cat shard.*.tfrecord > merged.tfrecord`. or you can also concatenate zipped tfrecord files:. `cat shard.*.tfrecord.gz > merged.tfrecord.gz`. will work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:22,usability,simpl,simply,22,"If you're looking for simply merging the tfrecord files (without having to touch Python code) to one, you can actually just concatenate tfrecord files together. Something like:. `cat shard.*.tfrecord > merged.tfrecord`. or you can also concatenate zipped tfrecord files:. `cat shard.*.tfrecord.gz > merged.tfrecord.gz`. will work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/49:96,usability,help,help,96,Great! Looks like this issue might be resolved but let us know if there is anything else we can help with.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49
https://github.com/google/deepvariant/issues/51:60,energy efficiency,model,model,60,The issue was that I was appending the prefix .index to the model's name. Sorry for the trouble.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/51
https://github.com/google/deepvariant/issues/51:60,security,model,model,60,The issue was that I was appending the prefix .index to the model's name. Sorry for the trouble.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/51
https://github.com/google/deepvariant/issues/52:110,interoperability,share,share,110,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:179,testability,simpl,simple,179,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:67,usability,command,command,67,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:179,usability,simpl,simple,179,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:17,availability,down,downloaded,17,"The Bam file was downloaded from. ```. https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam. ```. Then, since it was missing the @RG line, I added it manually just to test using picard:. ```. java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20. ```. The output of runninng. ```. samtools view -H ENCFF528VXT.bam. ```. is the following :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:89,availability,down,download,89,"The Bam file was downloaded from. ```. https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam. ```. Then, since it was missing the @RG line, I added it manually just to test using picard:. ```. java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20. ```. The output of runninng. ```. samtools view -H ENCFF528VXT.bam. ```. is the following :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2986,availability,checkpoint,checkpoint,2986,"_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3038,availability,error,error,3038,"D5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2348,deployability,contain,container,2348,"kDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/geno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2559,deployability,contain,container,2559,"MY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3067,deployability,Log,Logging,3067," the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Lui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3402,deployability,modul,module,3402,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3001,energy efficiency,model,models,3001,"AM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3008,energy efficiency,model,model,3008,"00 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2098,integrability,pub,public,2098,"Q SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3221,integrability,pub,public,3221,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:458,interoperability,coordinat,coordinate,458,"The Bam file was downloaded from. ```. https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam. ```. Then, since it was missing the @RG line, I added it manually just to test using picard:. ```. java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20. ```. The output of runninng. ```. samtools view -H ENCFF528VXT.bam. ```. is the following :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3487,interoperability,platform,platform,3487,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3932,interoperability,format,format,3932,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3402,modifiability,modul,module,3402,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3460,modifiability,pac,packages,3460,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2612,performance,time,time,2612,"ORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2627,performance,parallel,parallel,2627,"TION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3038,performance,error,error,3038,"D5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2986,reliability,checkpoint,checkpoint,2986,"_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:189,safety,test,test,189,"The Bam file was downloaded from. ```. https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam. ```. Then, since it was missing the @RG line, I added it manually just to test using picard:. ```. java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20. ```. The output of runninng. ```. samtools view -H ENCFF528VXT.bam. ```. is the following :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1429,safety,INPUT,INPUT,1429,ing :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF52,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2175,safety,test,testfiles,2175,"bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2202,safety,test,testfiles,2202,"local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3038,safety,error,error,3038,"D5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3067,safety,Log,Logging,3067," the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Lui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3402,safety,modul,module,3402,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2076,security,modif,modified,2076,":chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging befor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3001,security,model,models,3001,"AM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3008,security,model,model,3008,"00 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3067,security,Log,Logging,3067," the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Lui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3247,security,access,accessible,3247,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:189,testability,test,test,189,"The Bam file was downloaded from. ```. https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam. ```. Then, since it was missing the @RG line, I added it manually just to test using picard:. ```. java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20. ```. The output of runninng. ```. samtools view -H ENCFF528VXT.bam. ```. is the following :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2175,testability,test,testfiles,2175,"bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2202,testability,test,testfiles,2202,"local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3067,testability,Log,Logging,3067," the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Lui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3271,testability,Trace,Traceback,3271,"he modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again. ```. Thanks a lot, . Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1429,usability,INPUT,INPUT,1429,ing :. ```. @HD VN:1.5 SO:coordinate. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16571. @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1. @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz. @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF52,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3038,usability,error,error,3038,"D5_FILE=false. ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : . s3://dv-testfiles/hg19.fa. s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :. ```. samtools index ENCFF528VXT.bam. samtools faidx hg19.fa. bgzip -c -i hg19.fa > hg19.fa.gz. samtools faidx ""hg19.fa.gz"". ```. Then I ran in the docker container you provide :. ```. mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```. /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt. ```. and here the error output:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants. examples_filename, example_format)). ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:464,availability,ERROR,ERROR,464,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:45,integrability,pub,public,45,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:464,performance,ERROR,ERROR,464,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:69,safety,test,testfiles,69,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:147,safety,test,testfiles,147,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:204,safety,test,testfiles,204,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:235,safety,test,testfiles,235,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:298,safety,test,testfiles,298,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:329,safety,test,testfiles,329,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:464,safety,ERROR,ERROR,464,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:69,testability,test,testfiles,69,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:147,testability,test,testfiles,147,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:204,testability,test,testfiles,204,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:235,testability,test,testfiles,235,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:298,testability,test,testfiles,298,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:329,testability,test,testfiles,329,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:464,usability,ERROR,ERROR,464,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 403 Forbidden. 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:623,availability,ERROR,ERROR,623,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:41,integrability,pub,public,41,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:182,integrability,pub,public,182,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:623,performance,ERROR,ERROR,623,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:211,safety,test,testfiles,211,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:291,safety,test,testfiles,291,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:353,safety,test,testfiles,353,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:384,safety,test,testfiles,384,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:451,safety,test,testfiles,451,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:482,safety,test,testfiles,482,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:623,safety,ERROR,ERROR,623,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:690,security,auth,authored,690,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:909,security,auth,auth,909,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:211,testability,test,testfiles,211,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:291,testability,test,testfiles,291,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:353,testability,test,testfiles,353,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:384,testability,test,testfiles,384,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:451,testability,test,testfiles,451,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:482,testability,test,testfiles,482,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:623,usability,ERROR,ERROR,623,"Thank a lot for your answer. Now the are public for real! Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:. >. > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 07:28:01-- http://dv-testfiles.s3. > amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... > 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... > connected. > HTTP request sent, awaiting response... 403 Forbidden. > 2018-03-07 07:28:01 ERROR 403: Forbidden. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:238,deployability,upgrad,upgrade,238,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:258,deployability,releas,release,258,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:266,deployability,version,version,266,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:266,integrability,version,version,266,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:238,modifiability,upgrad,upgrade,238,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:266,modifiability,version,version,266,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:274,reliability,doe,does,274,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:153,availability,error,error,153,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1614,availability,sli,slim,1614," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2073,availability,error,error,2073," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:47,deployability,version,version,47,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:183,deployability,Log,Logging,183,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:532,deployability,contain,contain,532,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:686,deployability,modul,module,686,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1100,energy efficiency,predict,predictions,1100," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1114,energy efficiency,model,model,1114," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1158,energy efficiency,Predict,Predictions,1158," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1236,energy efficiency,model,modeling,1236," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:47,integrability,version,version,47,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:337,integrability,pub,public,337,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:772,interoperability,platform,platform,772,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:47,modifiability,version,version,47,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:686,modifiability,modul,module,686,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:745,modifiability,pac,packages,745,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1717,modifiability,pac,packages,1717," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1882,modifiability,pac,packages,1882," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1910,modifiability,layer,layers,1910," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1924,modifiability,layer,layers,1924," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1931,modifiability,layer,layers,1931," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:153,performance,error,error,153,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2073,performance,error,error,2073," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1614,reliability,sli,slim,1614," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:153,safety,error,error,153,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:183,safety,Log,Logging,183,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:686,safety,modul,module,686,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1100,safety,predict,predictions,1100," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1158,safety,Predict,Predictions,1158," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1626,safety,input,inputs,1626," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2027,safety,input,input,2027," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2073,safety,error,error,2073," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:183,security,Log,Logging,183,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:363,security,access,accessible,363,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1114,security,model,model,1114," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1236,security,model,modeling,1236," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1653,security,stride,stride,1653," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:183,testability,Log,Logging,183,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:554,testability,Trace,Traceback,554,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:153,usability,error,error,153,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:. ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1626,usability,input,inputs,1626," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2013,usability,support,supported,2013," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2027,usability,input,input,2027," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2073,usability,error,error,2073," ```. gcr.io/deepvariant-docker/deepvariant:0.5.1. ```. I get this error instead:. ```. WARNING: Logging before flag parsing goes to stderr. W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main. batch_size=FLAGS.batch_size). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants. predictions = model.create(images, 3, is_training=False)['Predictions']. File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create. images, num_classes, create_aux_logits=False, is_training=is_training). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3. depth_multiplier=depth_multiplier). File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base. net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args. return func(*args, **current_args). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution. input_rank). ValueError: ('Convolution not supported for input with rank', 1). ```. Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1105,availability,operat,operation,1105,"isa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1127,availability,error,error,1127,"noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1791,availability,servic,service,1791,"========================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1115,deployability,fail,failed,1115,"M file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1176,deployability,fail,failed,1176,"h would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1652,deployability,Log,Logging,1652,"ng response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1791,deployability,servic,service,1791,"========================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2273,deployability,modul,module,2273,". paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3170,energy efficiency,core,core,3170,"YTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@gubuntu:~/deepvariant/bazel-bin$. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1791,integrability,servic,service,1791,"========================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1910,integrability,pub,public,1910,"bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2359,interoperability,platform,platform,2359,"el-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@gubuntu:~/deepvariant/bazel-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1791,modifiability,servic,service,1791,"========================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2273,modifiability,modul,module,2273,". paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2332,modifiability,pac,packages,2332,"l@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1127,performance,error,error,1127,"noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1750,performance,Time,Timeout,1750,"â. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2087,performance,cach,cache,2087,"ad] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2464,performance,cach,cache,2464,"YTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@gubuntu:~/deepvariant/bazel-bin$. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2733,performance,cach,cache,2733,"YTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@gubuntu:~/deepvariant/bazel-bin$. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3018,performance,cach,cache,3018,"YTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. paul@gubuntu:~/deepvariant/bazel-bin$. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1115,reliability,fail,failed,1115,"M file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1176,reliability,fail,failed,1176,"h would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:70,safety,test,test,70,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:244,safety,compl,complete,244,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:311,safety,test,testfiles,311,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:389,safety,test,testfiles,389,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:446,safety,test,testfiles,446,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:477,safety,test,testfiles,477,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:540,safety,test,testfiles,540,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:571,safety,test,testfiles,571,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1050,safety,input,input,1050,"o I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most rece",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1127,safety,error,error,1127,"noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1652,safety,Log,Logging,1652,"ng response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1750,safety,Timeout,Timeout,1750,"â. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2004,safety,input,input,2004,"W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2273,safety,modul,module,2273,". paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. random_seed=random_seed)). ValueError: Not found: No i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:244,security,compl,complete,244,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1652,security,Log,Logging,1652,"ng response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1936,security,access,accessible,1936,"653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:63,testability,simpl,simple,63,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:70,testability,test,test,70,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:311,testability,test,testfiles,311,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:389,testability,test,testfiles,389,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:446,testability,test,testfiles,446,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:477,testability,test,testfiles,477,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:540,testability,test,testfiles,540,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:571,testability,test,testfiles,571,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1652,testability,Log,Logging,1652,"ng response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2033,testability,Trace,Traceback,2033,"absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/home/paul/.cache/bazel/_bazel_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:63,usability,simpl,simple,63,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1050,usability,input,input,1050,"o I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most rece",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1127,usability,error,error,1127,"noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```. paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. HTTP request sent, awaiting response... 200 OK. Length: 357342653 (341M) [binary/octet-stream]. Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2004,usability,input,input,2004,"W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. samtools index: failed to create index for ""ENCFF528VXT.bam"". paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$. paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. WARNING: Logging before flag parsing goes to stderr. I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. Traceback (most recent call last):. File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. File ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:274,availability,error,error,274,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1566,availability,operat,operation,1566,"e. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1588,availability,error,error,1588,"ke_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2266,availability,servic,service,2266,"============>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1576,deployability,fail,failed,1576,"requisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1639,deployability,fail,failed,1639,"tu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2125,deployability,Log,Logging,2125,"7342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2266,deployability,servic,service,2266,"============>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2756,deployability,modul,module,2756,":~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3669,energy efficiency,core,core,3669,"4592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:209,integrability,pub,public,209,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2266,integrability,servic,service,2266,"============>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2387,integrability,pub,public,2387,"342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2846,interoperability,platform,platform,2846,"TH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2266,modifiability,servic,service,2266,"============>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2756,modifiability,modul,module,2756,":~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2819,modifiability,pac,packages,2819,"variant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF52",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:274,performance,error,error,274,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1588,performance,error,error,1588,"ke_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2225,performance,Time,Timeout,2225,"=====================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2570,performance,cach,cache,2570,"on failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2955,performance,cach,cache,2955,"ome/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this em",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3228,performance,cach,cache,3228,"4592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3517,performance,cach,cache,3517,"4592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1576,reliability,fail,failed,1576,"requisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1639,reliability,fail,failed,1639,"tu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:60,safety,test,testing,60,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:132,safety,test,testfiles,132,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:274,safety,error,error,274,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:426,safety,test,test,426,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:607,safety,compl,complete,607,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:674,safety,test,testfiles,674,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:744,safety,test,testfiles,744,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:821,safety,test,testfiles,821,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:880,safety,test,testfiles,880,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:911,safety,test,testfiles,911,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:976,safety,test,testfiles,976,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1007,safety,test,testfiles,1007,"ile was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1509,safety,input,input,1509,"t even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1588,safety,error,error,1588,"ke_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2125,safety,Log,Logging,2125,"7342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2225,safety,Timeout,Timeout,2225,"=====================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2483,safety,input,input,2483," marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2756,safety,modul,module,2756,":~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:607,security,compl,complete,607,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2125,security,Log,Logging,2125,"7342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2413,security,access,accessible,2413,"/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:3919,security,auth,authored,3919,"4592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:4138,security,auth,auth,4138,"4592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader. > random_seed=random_seed)). > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam. > paul@gubuntu:~/deepvariant/bazel-bin$. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:60,testability,test,testing,60,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:132,testability,test,testfiles,132,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:419,testability,simpl,simple,419,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:426,testability,test,test,426,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:674,testability,test,testfiles,674,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:744,testability,test,testfiles,744,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:821,testability,test,testfiles,821,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:880,testability,test,testfiles,880,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:911,testability,test,testfiles,911,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:976,testability,test,testfiles,976,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1007,testability,test,testfiles,1007,"ile was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2125,testability,Log,Logging,2125,"7342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2514,testability,Trace,Traceback,2514,"robably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:. > File ""/home/paul/.ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:274,usability,error,error,274,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:419,usability,simpl,simple,419,"Indeed the file was truncated, sorry about that. I am still testing locally. with other even smaller files ( like : wget. http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam. which is public and smaller and not truncated ) and I get the same exact. error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test. > to see if there is an issue with Luisa's BAM file, and noticed that I. > cannot even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1509,usability,input,input,1509,"t even create an index - which would naturally make even the. > prerequisite make_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:1588,usability,error,error,1588,"ke_examples not complete properly:. >. > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam. > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113. > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected. > HTTP request sent, awaiting response... 200 OK. > Length: 357342653 (341M) [binary/octet-stream]. > Saving to: âENCFF528VXT.bamâ. >. > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. >. > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. >. > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:2483,usability,input,input,2483," marker is absent. The input is probably truncated. > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes. > samtools index: failed to create index for ""ENCFF528VXT.bam"". > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$. > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin. > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0. > WARNING: Logging before flag parsing goes to stderr. > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service. > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. > Traceback (most recent call last):. > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>. > tf.app.run(). > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. > _sys.exit(main(_sys.argv[:1] + flags_passthrough)). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options. > with genomics_io.make_sam_reader(flags_obj.reads) as sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:162,energy efficiency,model,models,162,Believe the issue was coming from limiting the previous step to certain regions. When running it now on full genome without restriction with 0.5.1 and the latest models it works for me. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/52:162,security,model,models,162,Believe the issue was coming from limiting the previous step to certain regions. When running it now on full genome without restriction with 0.5.1 and the latest models it works for me. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52
https://github.com/google/deepvariant/issues/53:250,availability,consist,consistent,250,Are you saying you've swapped hg19.fa.gz for our hs37d5.fa.gz fasta? That won't work because the chromosome names are different - 20 in hs37d5 and it sounds like chr20 in hg19.fa.gz. DeepVariant requires the BAM file and the reference genome to have consistent contigs.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:250,usability,consist,consistent,250,Are you saying you've swapped hg19.fa.gz for our hs37d5.fa.gz fasta? That won't work because the chromosome names are different - 20 in hs37d5 and it sounds like chr20 in hg19.fa.gz. DeepVariant requires the BAM file and the reference genome to have consistent contigs.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:87,deployability,contain,contains,87,"I do not think this is the case. The example BAM file, NA12878_S1.chr20.10_10p1mb.bam, contains reads mapped to chromosome 20 which show chr20 in RNAME for all the reads. This means the fasta file should contain chromosomes named as 'chr'. Morever, the example fasta, ucsc.hg19.chr20.unittest.fasta, comes from hg19 and it works. Any idea why using the entire genome would make the run fail?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:204,deployability,contain,contain,204,"I do not think this is the case. The example BAM file, NA12878_S1.chr20.10_10p1mb.bam, contains reads mapped to chromosome 20 which show chr20 in RNAME for all the reads. This means the fasta file should contain chromosomes named as 'chr'. Morever, the example fasta, ucsc.hg19.chr20.unittest.fasta, comes from hg19 and it works. Any idea why using the entire genome would make the run fail?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:386,deployability,fail,fail,386,"I do not think this is the case. The example BAM file, NA12878_S1.chr20.10_10p1mb.bam, contains reads mapped to chromosome 20 which show chr20 in RNAME for all the reads. This means the fasta file should contain chromosomes named as 'chr'. Morever, the example fasta, ucsc.hg19.chr20.unittest.fasta, comes from hg19 and it works. Any idea why using the entire genome would make the run fail?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:386,reliability,fail,fail,386,"I do not think this is the case. The example BAM file, NA12878_S1.chr20.10_10p1mb.bam, contains reads mapped to chromosome 20 which show chr20 in RNAME for all the reads. This means the fasta file should contain chromosomes named as 'chr'. Morever, the example fasta, ucsc.hg19.chr20.unittest.fasta, comes from hg19 and it works. Any idea why using the entire genome would make the run fail?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:284,testability,unit,unittest,284,"I do not think this is the case. The example BAM file, NA12878_S1.chr20.10_10p1mb.bam, contains reads mapped to chromosome 20 which show chr20 in RNAME for all the reads. This means the fasta file should contain chromosomes named as 'chr'. Morever, the example fasta, ucsc.hg19.chr20.unittest.fasta, comes from hg19 and it works. Any idea why using the entire genome would make the run fail?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:87,availability,consist,consistent,87,"The only real requirement is that the fasta and the BAM need to have a large number of consistent contigs (meaning same length in bp and with the same name) and DeepVariant will process the dataset. If you do samtools view -H on the BAM and cat the hg19.fa.gz.fai file, do these have the same set of contigs with the same lengths?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:87,usability,consist,consistent,87,"The only real requirement is that the fasta and the BAM need to have a large number of consistent contigs (meaning same length in bp and with the same name) and DeepVariant will process the dataset. If you do samtools view -H on the BAM and cat the hg19.fa.gz.fai file, do these have the same set of contigs with the same lengths?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:82,deployability,version,version,82,"I already have a copy of hs37d5.fa on my machine, so to create a gzipped, indexed version I do:. ```. cp hs37d5.fa /tmp. bgzip /tmp/hs37d5.fa. samtools faidx /tmp/hs37d5.fa.gz. head /tmp/hs37d5.fa.gz.fai. 1 249250621 52 60 61. 2 243199373 253404903 60 61. 3 198022430 500657651 60 61. 4 191154276 701980507 60 61. 5 180915260 896320740 60 61. 6 171115067 1080251307 60 61. 7 159138663 1254218344 60 61. 8 146364022 1416009371 60 61. 9 141213431 1564812846 60 61. 10 135534747 1708379889 60 61. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:82,integrability,version,version,82,"I already have a copy of hs37d5.fa on my machine, so to create a gzipped, indexed version I do:. ```. cp hs37d5.fa /tmp. bgzip /tmp/hs37d5.fa. samtools faidx /tmp/hs37d5.fa.gz. head /tmp/hs37d5.fa.gz.fai. 1 249250621 52 60 61. 2 243199373 253404903 60 61. 3 198022430 500657651 60 61. 4 191154276 701980507 60 61. 5 180915260 896320740 60 61. 6 171115067 1080251307 60 61. 7 159138663 1254218344 60 61. 8 146364022 1416009371 60 61. 9 141213431 1564812846 60 61. 10 135534747 1708379889 60 61. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/53:82,modifiability,version,version,82,"I already have a copy of hs37d5.fa on my machine, so to create a gzipped, indexed version I do:. ```. cp hs37d5.fa /tmp. bgzip /tmp/hs37d5.fa. samtools faidx /tmp/hs37d5.fa.gz. head /tmp/hs37d5.fa.gz.fai. 1 249250621 52 60 61. 2 243199373 253404903 60 61. 3 198022430 500657651 60 61. 4 191154276 701980507 60 61. 5 180915260 896320740 60 61. 6 171115067 1080251307 60 61. 7 159138663 1254218344 60 61. 8 146364022 1416009371 60 61. 9 141213431 1564812846 60 61. 10 135534747 1708379889 60 61. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/53
https://github.com/google/deepvariant/issues/54:98,deployability,version,version,98,"This is the name problem as in https://github.com/google/deepvariant/issues/53. You need to use a version of the reference compatible with the reference that BAM file is mapped to. hg19.fa.gz uses chr1, chr2, etc while hs37d5 uses 1, 2, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/54
https://github.com/google/deepvariant/issues/54:98,integrability,version,version,98,"This is the name problem as in https://github.com/google/deepvariant/issues/53. You need to use a version of the reference compatible with the reference that BAM file is mapped to. hg19.fa.gz uses chr1, chr2, etc while hs37d5 uses 1, 2, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/54
https://github.com/google/deepvariant/issues/54:123,interoperability,compatib,compatible,123,"This is the name problem as in https://github.com/google/deepvariant/issues/53. You need to use a version of the reference compatible with the reference that BAM file is mapped to. hg19.fa.gz uses chr1, chr2, etc while hs37d5 uses 1, 2, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/54
https://github.com/google/deepvariant/issues/54:98,modifiability,version,version,98,"This is the name problem as in https://github.com/google/deepvariant/issues/53. You need to use a version of the reference compatible with the reference that BAM file is mapped to. hg19.fa.gz uses chr1, chr2, etc while hs37d5 uses 1, 2, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/54
https://github.com/google/deepvariant/pull/57:154,availability,avail,available,154,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:39,deployability,updat,updated,39,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:47,deployability,version,version,47,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:99,deployability,updat,update,99,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:142,deployability,version,version,142,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:47,integrability,version,version,47,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:142,integrability,version,version,142,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:47,modifiability,version,version,47,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:142,modifiability,version,version,142,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:154,reliability,availab,available,154,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:39,safety,updat,updated,39,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:99,safety,updat,update,99,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:154,safety,avail,available,154,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:39,security,updat,updated,39,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:99,security,updat,update,99,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:154,security,availab,available,154,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash. $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/. gs://deepvariant/binaries/DeepVariant/0.4.0/. gs://deepvariant/binaries/DeepVariant/0.4.1/. gs://deepvariant/binaries/DeepVariant/0.5.0/. gs://deepvariant/binaries/DeepVariant/0.5.1/. $. ```. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:40,availability,avail,available,40,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:139,deployability,releas,release,139,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:178,energy efficiency,model,models,178,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:40,reliability,availab,available,40,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:40,safety,avail,available,40,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:40,security,availab,available,40,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:178,security,model,models,178,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:447,availability,down,download-the-deepvariant-binaries-and-install-prerequisites,447,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:93,deployability,version,version,93,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:485,deployability,instal,install-prerequisites,485,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:93,integrability,version,version,93,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:93,modifiability,version,version,93,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/pull/57:527,usability,user,users,527,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57
https://github.com/google/deepvariant/issues/59:11,deployability,build,build-prereq,11,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:313,deployability,build,build-prereq,313,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:80,energy efficiency,Cloud,Cloud,80,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:426,energy efficiency,Cloud,Cloud,426,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:202,security,ssh,ssh,202,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:517,usability,help,help,517,"Did your ./build-prereq.sh run finish without problems? I just got a machine on Cloud (using the example from https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md). And then I ssh'ed in, and did the following:. ```. git clone https://github.com/google/deepvariant.git. cd deepvariant. ./build-prereq.sh. ./build_and_test.sh. ```. Everything worked for me in that setting. You can try with the Google Cloud instance, or you might need to provide more information about your setting so we can help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/59:64,usability,help,help,64,I ran build_release_binaries.sh and itgot fixed. Thanks for the help :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/59
https://github.com/google/deepvariant/issues/60:4,availability,error,error,4,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:46,energy efficiency,CPU,CPU,46,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:103,energy efficiency,CPU,CPUs,103,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:151,energy efficiency,cloud,cloud,151,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:4,performance,error,error,4,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:46,performance,CPU,CPU,46,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:103,performance,CPU,CPUs,103,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:4,safety,error,error,4,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/60:4,usability,error,error,4,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60
https://github.com/google/deepvariant/issues/61:158,safety,test,testdata,158,"Hi @aderzelle ,. Since you're using `zsh` as a shell just include single-quotes like this when you use wildcards:. `gsutil -m cp 'gs://deepvariant/quickstart-testdata/*' input/`. Or you could try `bash` as a shell :). Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/61
https://github.com/google/deepvariant/issues/61:170,safety,input,input,170,"Hi @aderzelle ,. Since you're using `zsh` as a shell just include single-quotes like this when you use wildcards:. `gsutil -m cp 'gs://deepvariant/quickstart-testdata/*' input/`. Or you could try `bash` as a shell :). Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/61
https://github.com/google/deepvariant/issues/61:158,testability,test,testdata,158,"Hi @aderzelle ,. Since you're using `zsh` as a shell just include single-quotes like this when you use wildcards:. `gsutil -m cp 'gs://deepvariant/quickstart-testdata/*' input/`. Or you could try `bash` as a shell :). Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/61
https://github.com/google/deepvariant/issues/61:170,usability,input,input,170,"Hi @aderzelle ,. Since you're using `zsh` as a shell just include single-quotes like this when you use wildcards:. `gsutil -m cp 'gs://deepvariant/quickstart-testdata/*' input/`. Or you could try `bash` as a shell :). Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/61
https://github.com/google/deepvariant/issues/62:23,deployability,releas,released,23,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:32,energy efficiency,model,models,32,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:194,energy efficiency,Current,Currently,194,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:292,energy efficiency,frequenc,frequency,292,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:151,performance,time,time,151,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:32,security,model,models,32,"Hi KBT59,. because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:85,energy efficiency,model,model,85,"Hi again,. I didn't read carefully so I missed that you said you want to __train__ a model. If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:85,security,model,model,85,"Hi again,. I didn't read carefully so I missed that you said you want to __train__ a model. If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:370,testability,simpl,simply,370,"Hi again,. I didn't read carefully so I missed that you said you want to __train__ a model. If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:370,usability,simpl,simply,370,"Hi again,. I didn't read carefully so I missed that you said you want to __train__ a model. If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1954,availability,error,error-free,1954,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2132,availability,error,errors,2132,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1588,deployability,contain,contains,1588,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2060,deployability,contain,contain,2060,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2288,deployability,version,version,2288,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:787,energy efficiency,model,model,787,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:476,integrability,Sub,Subject,476,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1580,integrability,messag,message,1580,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2176,integrability,messag,message,2176,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2288,integrability,version,version,2288,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1580,interoperability,messag,message,1580,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1732,interoperability,distribut,distribute,1732,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2176,interoperability,messag,message,2176,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2288,modifiability,version,version,2288,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:689,performance,content,content,689,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1954,performance,error,error-free,1954,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2132,performance,error,errors,2132,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2159,performance,content,contents,2159,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2098,reliability,doe,does,2098,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:700,safety,safe,safe,700,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1954,safety,error,error-free,1954,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2132,safety,error,errors,2132,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:440,security,Auth,Author,440,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:448,security,auth,author,448,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:787,security,model,model,787,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1309,security,auth,authored,1309,"April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Labo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1515,security,auth,auth,1515,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1597,security,confidential,confidential,1597,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1943,security,secur,secured,1943,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1989,security,intercept,intercepted,1989,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1062,testability,simpl,simply,1062,"count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2236,testability,verif,verification,2236,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1062,usability,simpl,simply,1062,"count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1954,usability,error,error-free,1954,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2132,usability,error,errors,2132,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:371,energy efficiency,current,current,371,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:379,energy efficiency,model,model,379,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:472,energy efficiency,model,model,472,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:214,reliability,doe,does,214,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:379,security,model,model,379,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:472,security,model,model,472,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:184,usability,confirm,confirm,184,"Lowering the fractions makes sense. Since you're doing something very experimental, you'll need to look into your own metrics to see what threshold makes sense. I think you'll want to confirm that your new setting does give you enough sensitivity. Because if something is not picked up by the Very Sensitive Caller, it won't be called later on. There's a chance that the current model won't work well on your use case at all (and you might need to use a different kind of model), but it's worth a try.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:358,availability,error,error,358,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2286,availability,error,error-free,2286,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2464,availability,error,errors,2464,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:184,deployability,contain,contains,184,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1920,deployability,contain,contains,1920,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2392,deployability,contain,contain,2392,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2620,deployability,version,version,2620,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1119,energy efficiency,model,model,1119,"-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:808,integrability,Sub,Subject,808,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1912,integrability,messag,message,1912,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2508,integrability,messag,message,2508,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2620,integrability,version,version,2620,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:497,interoperability,specif,specify,497,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1912,interoperability,messag,message,1912,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2064,interoperability,distribut,distribute,2064,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2508,interoperability,messag,message,2508,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2620,modifiability,version,version,2620,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:358,performance,error,error,358,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1021,performance,content,content,1021,"s I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2286,performance,error,error-free,2286,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2464,performance,error,errors,2464,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2491,performance,content,contents,2491,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2430,reliability,doe,does,2430,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:358,safety,error,error,358,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1032,safety,safe,safe,1032,"ve I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addresse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2286,safety,error,error-free,2286,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2464,safety,error,errors,2464,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:772,security,Auth,Author,772,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:780,security,auth,author,780,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1119,security,model,model,1119,"-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1641,security,auth,authored,1641,"April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Labo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1847,security,auth,auth,1847,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1929,security,confidential,confidential,1929,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2275,security,secur,secured,2275,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2321,security,intercept,intercepted,2321,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1394,testability,simpl,simply,1394,"g files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2568,testability,verif,verification,2568,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:358,usability,error,error,358,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset"". tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1394,usability,simpl,simply,1394,"g files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files? Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Thursday, April 5, 2018 6:56 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2286,usability,error,error-free,2286,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:2464,usability,error,errors,2464,"rad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,. I didn't read carefully so I missed that you said you want to train a model. If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:46,availability,error,error,46,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1601,availability,error,error-free,1601,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1779,availability,error,errors,1779,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:143,deployability,Log,Logits,143,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1235,deployability,contain,contains,1235,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1707,deployability,contain,contain,1707,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1935,deployability,version,version,1935,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:574,integrability,Sub,Subject,574,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1227,integrability,messag,message,1227,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1823,integrability,messag,message,1823,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1935,integrability,version,version,1935,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1227,interoperability,messag,message,1227,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1379,interoperability,distribut,distribute,1379,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1823,interoperability,messag,message,1823,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1935,modifiability,version,version,1935,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:46,performance,error,error,46,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:787,performance,content,content,787,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1601,performance,error,error-free,1601,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1779,performance,error,errors,1779,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1806,performance,content,contents,1806,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1745,reliability,doe,does,1745,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:46,safety,error,error,46,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:143,safety,Log,Logits,143,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:187,safety,input,input,187,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:798,safety,safe,safe,798,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1601,safety,error,error-free,1601,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1779,safety,error,errors,1779,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:143,security,Log,Logits,143,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:538,security,Auth,Author,538,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:546,security,auth,author,546,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:956,security,auth,authored,956,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1162,security,auth,auth,1162,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1244,security,confidential,confidential,1244,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1590,security,secur,secured,1590,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1636,security,intercept,intercepted,1636,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:143,testability,Log,Logits,143,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1883,testability,verif,verification,1883,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:46,usability,error,error,46,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:187,usability,input,input,187,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:267,usability,document,documentation,267,"OK – that proceeded further, I think. Now the error is. ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1601,usability,error,error-free,1601,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
https://github.com/google/deepvariant/issues/62:1779,usability,error,errors,1779,"lueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to? Thanks,. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]. Sent: Tuesday, April 10, 2018 1:04 PM. To: google/deepvariant <deepvariant@noreply.github.com>. Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>. Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:. tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62
